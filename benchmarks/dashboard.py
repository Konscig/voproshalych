"""–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—à–±–æ—Ä–¥ –¥–ª—è –º–µ—Ç—Ä–∏–∫ RAG-–±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""

from __future__ import annotations

from datetime import datetime
import json
import logging
import os
from pathlib import Path
from typing import Dict, List, Optional

try:
    import gradio as gr
    import pandas as pd

    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False

from sqlalchemy import select
from sqlalchemy.orm import Session

from qa.config import Config
from qa.database import BenchmarkRun, create_engine, ensure_benchmark_runs_schema

logger = logging.getLogger(__name__)

APP_TITLE = "RAG Quality Assurance System"

METRICS_BY_TIER = {
    "tier_0": ["avg_intra_cluster_sim", "avg_inter_cluster_dist", "silhouette_score"],
    "tier_1": ["mrr", "hit_rate@1", "hit_rate@5", "hit_rate@10"],
    "tier_2": [
        "avg_faithfulness",
        "avg_answer_relevance",
        "avg_rouge1_f",
        "avg_rougeL_f",
        "avg_bleu",
    ],
    "tier_3": [
        "avg_e2e_score",
        "avg_semantic_similarity",
        "avg_rouge1_f",
        "avg_bleu",
    ],
    "tier_judge": [
        "consistency_score",
        "error_rate",
        "avg_latency_ms",
        "avg_faithfulness",
    ],
    "tier_judge_pipeline": [
        "accuracy",
        "precision",
        "recall",
        "f1_score",
        "avg_latency_ms",
    ],
    "tier_ux": ["cache_hit_rate", "context_preservation", "multi_turn_consistency"],
    "tier_real_users": [
        "mrr",
        "recall@1",
        "recall@5",
        "precision@1",
        "precision@5",
        "ndcg@5",
    ],
}

QUALITY_BASELINES = {
    "avg_intra_cluster_sim": 0.85,
    "avg_inter_cluster_dist": 0.30,
    "silhouette_score": 0.50,
    "mrr": 0.8,
    "hit_rate@1": 0.7,
    "hit_rate@5": 0.9,
    "hit_rate@10": 0.95,
    "recall@1": 0.7,
    "recall@5": 0.9,
    "recall@10": 0.95,
    "precision@1": 0.7,
    "precision@5": 0.18,
    "ndcg@5": 0.8,
    "avg_faithfulness": 4.5,
    "avg_answer_relevance": 4.2,
    "avg_e2e_score": 4.2,
    "avg_semantic_similarity": 0.85,
    "avg_rouge1_f": 0.50,
    "avg_rouge2_f": 0.30,
    "avg_rougeL_f": 0.45,
    "avg_bleu": 30.0,
    "consistency_score": 0.90,
    "error_rate": 0.05,
    "avg_latency_ms": 3000.0,
    "accuracy": 0.85,
    "precision": 0.85,
    "recall": 0.85,
    "f1_score": 0.85,
    "cache_hit_rate": 0.40,
    "context_preservation": 0.70,
    "multi_turn_consistency": 0.70,
}


def _safe_float(value, default: float = 0.0) -> float:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –∫ float."""
    try:
        return float(value)
    except (TypeError, ValueError):
        return default


class RAGBenchmarkDashboard:
    """–î–∞—à–±–æ—Ä–¥ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–∞ Retrieval –∏ Generation."""

    def __init__(self):
        self.engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)
        ensure_benchmark_runs_schema(self.engine)
        self.runs = self._load_runs()

    def _load_runs(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∑–∞–ø—É—Å–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã benchmark_runs."""

        def normalize_metrics(raw_metrics):
            if isinstance(raw_metrics, dict):
                return raw_metrics
            if isinstance(raw_metrics, str):
                try:
                    parsed = json.loads(raw_metrics)
                    if isinstance(parsed, dict):
                        return parsed
                except json.JSONDecodeError:
                    return {}
            return {}

        with Session(self.engine) as session:
            records = session.scalars(
                select(BenchmarkRun).order_by(BenchmarkRun.timestamp.asc())
            ).all()

        runs = []
        for record in records:
            timestamp_str = record.timestamp.strftime("%Y%m%d_%H%M%S")
            runs.append(
                {
                    "id": record.id,
                    "timestamp": timestamp_str,
                    "timestamp_readable": record.timestamp.strftime(
                        "%Y-%m-%d %H:%M:%S"
                    ),
                    "git_branch": record.git_branch or "unknown",
                    "git_commit_hash": record.git_commit_hash or "unknown",
                    "run_author": record.run_author or "unknown",
                    "dataset_file": record.dataset_file or "unknown",
                    "dataset_type": record.dataset_type or "synthetic",
                    "judge_model": record.judge_model or "unknown",
                    "generation_model": record.generation_model or "unknown",
                    "overall_status": record.overall_status,
                    "tier_0": normalize_metrics(record.tier_0_metrics),
                    "tier_1": normalize_metrics(record.tier_1_metrics),
                    "tier_2": normalize_metrics(record.tier_2_metrics),
                    "tier_3": normalize_metrics(record.tier_3_metrics),
                    "tier_judge": normalize_metrics(record.tier_judge_metrics),
                    "tier_judge_pipeline": normalize_metrics(
                        record.tier_judge_pipeline_metrics
                    ),
                    "tier_ux": normalize_metrics(record.tier_ux_metrics),
                    "tier_real_users": normalize_metrics(record.real_user_metrics),
                }
            )

        logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω–æ %s –∑–∞–ø—É—Å–∫–æ–≤ –∏–∑ benchmark_runs", len(runs))
        return runs

    def get_latest_run(self) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
        if not self.runs:
            return None
        return self.runs[-1]

    def get_metric_history(
        self, tier: str, metric: str
    ) -> tuple[List[str], List[float]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º –∑–∞–ø—É—Å–∫–∞–º."""
        dates: List[str] = []
        values: List[float] = []

        for run in self.runs:
            tier_metrics = run.get(tier, {})
            value = tier_metrics.get(metric)
            if value is None:
                continue
            try:
                values.append(float(value))
                dates.append(run["timestamp"])
            except (TypeError, ValueError):
                continue

        return dates, values

    def _metric_options_for_tier(self, tier: str) -> List[str]:
        return METRICS_BY_TIER.get(tier, [])

    def _build_series_rows(
        self,
        dates: List[str],
        values: List[float],
        metric: str,
        series_name: str,
    ) -> List[Dict[str, str | float]]:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ –¥–ª—è gr.LinePlot —Å baseline-–ª–∏–Ω–∏–µ–π."""
        rows: List[Dict[str, str | float]] = []

        rendered_dates = []
        for d in dates:
            if "_" in d:
                date_part, time_part = d.split("_", 1)
                rendered_dates.append(f"{date_part}-{time_part[:2]}")
            else:
                rendered_dates.append(d[:8])

        rendered_values = values[:]
        if len(rendered_dates) == 1:
            rendered_dates.append(f"{rendered_dates[0]}_p")
            rendered_values.append(rendered_values[0])

        for date, value in zip(rendered_dates, rendered_values):
            rows.append({"timestamp": date, "value": value, "series": series_name})

        baseline = QUALITY_BASELINES.get(metric)
        if baseline is not None:
            for date in rendered_dates:
                rows.append(
                    {"timestamp": date, "value": baseline, "series": "Baseline"}
                )

        return rows

    def create_interface(self):
        """–°–æ–∑–¥–∞—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio."""
        if not GRADIO_AVAILABLE:
            raise ImportError(
                "Gradio –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ: uv sync --extra dashboard"
            )

        with gr.Blocks(title=APP_TITLE) as demo:
            gr.Markdown(f"# {APP_TITLE}")
            gr.Markdown(
                "–ü–∞–Ω–µ–ª—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ Retrieval, Generation –∏ end-to-end "
                "–æ—Ç–≤–µ—Ç–æ–≤ RAG-—Å–∏—Å—Ç–µ–º—ã."
            )

            with gr.Accordion("System Info", open=False):
                gr.Markdown(
                    "\n".join(
                        [
                            f"- Judge model: `{os.getenv('BENCHMARKS_JUDGE_MODEL') or os.getenv('JUDGE_MODEL') or Config.JUDGE_MODEL or 'unknown'}`",
                            f"- Generation model: `{os.getenv('GENERATION_MODEL') or Config.MISTRAL_MODEL or 'unknown'}`",
                            f"- Embedding model: `{Config.EMBEDDING_MODEL_PATH}`",
                        ]
                    )
                )

            with gr.Tab("Run Details"):
                self._create_run_details_tab()

            with gr.Tab("Runs Registry"):
                self._create_all_runs_tab()

            with gr.Tab("Metric History"):
                self._create_history_tab()

            with gr.Tab("Tier Comparison"):
                self._create_comparison_tab()

            with gr.Tab("Run Dataset"):
                self._create_run_dataset_tab()

            with gr.Tab("–°–ø—Ä–∞–≤–∫–∞"):
                self._create_reference_tab()

        return demo

    def _create_run_details_tab(self):
        if not self.runs:
            gr.Markdown("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–ø—É—Å–∫–∞—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü–µ benchmark_runs.")
            return

        ordered_runs = list(reversed(self.runs))

        def format_run_choice(run: Dict) -> str:
            return (
                f"{run['timestamp_readable']} | {run.get('dataset_type', 'synthetic')} | "
                f"{run['git_commit_hash'][:7]} | {run.get('dataset_file', 'N/A')}"
            )

        run_choices = [format_run_choice(r) for r in ordered_runs]

        gr.Markdown("### –í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–ø—É—Å–∫ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –º–µ—Ç—Ä–∏–∫")

        def get_run_metrics(selected: str) -> tuple:
            run = next(
                (r for r in ordered_runs if format_run_choice(r) == selected),
                None,
            )
            if not run:
                return "–ó–∞–ø—É—Å–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω", []

            info = "\n".join(
                [
                    f"### –ó–∞–ø—É—Å–∫: {run['timestamp_readable']}",
                    f"- **Branch:** `{run['git_branch']}`",
                    f"- **Commit:** `{run['git_commit_hash']}`",
                    f"- **Author:** `{run['run_author']}`",
                    f"- **Dataset:** `{run.get('dataset_file', 'N/A')}`",
                    f"- **Dataset type:** `{run.get('dataset_type', 'synthetic')}`",
                    f"- **Judge model:** `{run.get('judge_model', 'N/A')}`",
                    f"- **Generation model:** `{run.get('generation_model', 'N/A')}`",
                    f"- **Overall status:** `{run['overall_status']}`",
                ]
            )

            rows = []

            tier_0 = run.get("tier_0", {})
            for m in [
                "avg_intra_cluster_sim",
                "avg_inter_cluster_dist",
                "silhouette_score",
            ]:
                rows.append(
                    [
                        "Tier 0 (Embedding)",
                        m.replace("_", " ").title(),
                        round(_safe_float(tier_0.get(m)), 4),
                    ]
                )

            tier_1 = run.get("tier_1", {})
            for m in ["mrr", "hit_rate@1", "hit_rate@5", "hit_rate@10"]:
                rows.append(
                    [
                        "Tier 1 (Retrieval)",
                        m.replace("_", " ").title(),
                        round(_safe_float(tier_1.get(m)), 4),
                    ]
                )

            tier_2 = run.get("tier_2", {})
            for m in [
                "avg_faithfulness",
                "avg_answer_relevance",
                "avg_rouge1_f",
                "avg_rougeL_f",
                "avg_bleu",
            ]:
                rows.append(
                    [
                        "Tier 2 (Generation)",
                        m.replace("_", " ").title(),
                        round(_safe_float(tier_2.get(m)), 4),
                    ]
                )

            tier_3 = run.get("tier_3", {})
            for m in [
                "avg_e2e_score",
                "avg_semantic_similarity",
                "avg_rouge1_f",
                "avg_bleu",
            ]:
                rows.append(
                    [
                        "Tier 3 (End-to-End)",
                        m.replace("_", " ").title(),
                        round(_safe_float(tier_3.get(m)), 4),
                    ]
                )

            tier_judge = run.get("tier_judge", {})
            for m in [
                "consistency_score",
                "error_rate",
                "avg_latency_ms",
                "avg_faithfulness",
            ]:
                rows.append(
                    [
                        "Tier Judge (Qwen)",
                        m.replace("_", " ").title(),
                        round(_safe_float(tier_judge.get(m)), 4),
                    ]
                )

            tier_judge_pipeline = run.get("tier_judge_pipeline", {})
            for m in ["accuracy", "precision", "recall", "f1_score", "avg_latency_ms"]:
                rows.append(
                    [
                        "Tier Judge Pipeline (Mistral)",
                        m.replace("_", " ").title(),
                        round(_safe_float(tier_judge_pipeline.get(m)), 4),
                    ]
                )

            tier_ux = run.get("tier_ux", {})
            for m in [
                "cache_hit_rate",
                "context_preservation",
                "multi_turn_consistency",
            ]:
                rows.append(
                    [
                        "Tier UX",
                        m.replace("_", " ").title(),
                        round(_safe_float(tier_ux.get(m)), 4),
                    ]
                )

            tier_real = run.get("tier_real_users", {})
            for m in [
                "mrr",
                "recall@1",
                "recall@5",
                "precision@1",
                "precision@5",
                "ndcg@5",
            ]:
                rows.append(
                    [
                        "Real Users (Retrieval)",
                        m.replace("_", " ").title(),
                        round(_safe_float(tier_real.get(m)), 4),
                    ]
                )

            return info, rows

        initial_info, initial_rows = get_run_metrics(run_choices[0])

        run_selector = gr.Dropdown(
            choices=run_choices,
            value=run_choices[0],
            label="–í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–ø—É—Å–∫",
        )
        run_info = gr.Markdown(value=initial_info)
        metrics_table = gr.Dataframe(
            value=initial_rows,
            headers=["Tier", "Metric", "Value"],
            interactive=False,
            wrap=True,
        )

        def update_run(selected: str):
            info, rows = get_run_metrics(selected)
            return info, rows
            return info, rows

        run_selector.change(
            fn=update_run,
            inputs=[run_selector],
            outputs=[run_info, metrics_table],
        )

        def load_markdown_report(selected: str) -> str:
            run = next(
                (r for r in ordered_runs if format_run_choice(r) == selected),
                None,
            )
            if not run:
                return "–ó–∞–ø—É—Å–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω"

            dataset_file = run.get("dataset_file", "")
            if not dataset_file:
                return "–ò–º—è —Ñ–∞–π–ª–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∑–∞–ø—É—Å–∫–µ"

            base_name = dataset_file.replace(".json", "")
            possible_paths = [
                os.path.join("benchmarks/reports", f"{base_name}.md"),
                os.path.join(
                    "benchmarks/reports", f"rag_benchmark_{base_name.split('_')[-1]}.md"
                ),
            ]

            for path in possible_paths:
                if os.path.exists(path):
                    try:
                        with open(path, "r", encoding="utf-8") as f:
                            return f.read()
                    except Exception as e:
                        return f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}"

            return f"Markdown –æ—Ç—á—ë—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–∫–∞–ª–∏: {possible_paths}"

        gr.Markdown("---")
        gr.Markdown("### üìÑ Markdown Report")

        initial_markdown = load_markdown_report(run_choices[0])

        markdown_display = gr.Markdown(value=initial_markdown)

        def update_markdown(selected: str):
            return load_markdown_report(selected)

        run_selector.change(
            fn=update_markdown,
            inputs=[run_selector],
            outputs=[markdown_display],
        )

    def _create_history_tab(self):
        gr.Markdown("### Historical trend for selected metric")

        all_tiers = list(METRICS_BY_TIER.keys())

        default_tier = "tier_1"
        default_metric = "mrr"
        if not self.get_metric_history(default_tier, default_metric)[0]:
            for candidate_tier in all_tiers:
                for candidate_metric in self._metric_options_for_tier(candidate_tier):
                    if self.get_metric_history(candidate_tier, candidate_metric)[0]:
                        default_tier = candidate_tier
                        default_metric = candidate_metric
                        break
                if default_metric != "mrr" or default_tier != "tier_1":
                    break

        with gr.Row():
            tier_dropdown = gr.Dropdown(
                choices=all_tiers,
                value=default_tier,
                label="Tier",
            )
            metric_dropdown = gr.Dropdown(
                choices=self._metric_options_for_tier(default_tier),
                value=default_metric,
                label="Metric",
            )

        initial_history = pd.DataFrame(
            self._build_series_rows(
                *self.get_metric_history(default_tier, default_metric),
                default_metric,
                default_metric,
            )
        )

        plot = gr.LinePlot(
            value=initial_history,
            x="timestamp",
            y="value",
            color="series",
            title="Metric vs Baseline",
            x_title="Run",
            y_title="Value",
            tooltip="all",
            height=420,
            color_map={"Baseline": "#808080"},
            sort="x",
        )

        def update_metric_choices(tier: str):
            options = self._metric_options_for_tier(tier)
            value = options[0] if options else None
            return gr.Dropdown(choices=options, value=value)

        def update_plot(tier: str, metric: str):
            if not metric:
                return pd.DataFrame(columns=["timestamp", "value", "series"])
            dates, values = self.get_metric_history(tier, metric)
            if not dates:
                return pd.DataFrame(columns=["timestamp", "value", "series"])
            rows = self._build_series_rows(dates, values, metric, metric)
            return pd.DataFrame(rows)

        tier_dropdown.change(
            fn=update_metric_choices,
            inputs=[tier_dropdown],
            outputs=[metric_dropdown],
        ).then(
            fn=update_plot,
            inputs=[tier_dropdown, metric_dropdown],
            outputs=[plot],
        )

        metric_dropdown.change(
            fn=update_plot,
            inputs=[tier_dropdown, metric_dropdown],
            outputs=[plot],
        )

    def _create_comparison_tab(self):
        gr.Markdown("### Compare tiers by metric")

        comparison_metrics = sorted(
            {metric for metrics in METRICS_BY_TIER.values() for metric in metrics}
        )

        tier_labels = {
            "tier_0": "Tier 0 (Embedding)",
            "tier_1": "Tier 1 (Retrieval)",
            "tier_2": "Tier 2 (Generation)",
            "tier_3": "Tier 3 (E2E)",
            "tier_judge": "Tier Judge (Qwen)",
            "tier_judge_pipeline": "Tier Judge Pipeline",
            "tier_ux": "Tier UX",
            "tier_real_users": "Real Users",
        }

        metric_dropdown = gr.Dropdown(
            choices=comparison_metrics,
            value="mrr",
            label="Metric",
        )

        initial_comparison_rows: List[Dict[str, str | float]] = []
        for tier_name, label in tier_labels.items():
            if tier_name in METRICS_BY_TIER:
                dates, values = self.get_metric_history(tier_name, "mrr")
                if dates:
                    initial_comparison_rows.extend(
                        self._build_series_rows(dates, values, "mrr", label)
                    )

        plot = gr.LinePlot(
            value=pd.DataFrame(initial_comparison_rows),
            x="timestamp",
            y="value",
            color="series",
            title="Tier comparison",
            x_title="Run",
            y_title="Value",
            tooltip="all",
            height=420,
            color_map={"Baseline": "#808080"},
            sort="x",
        )

        def update_comparison_plot(metric: str):
            all_rows: List[Dict[str, str | float]] = []
            series_by_tier = {
                "tier_0": "Tier 0 (Embed)",
                "tier_1": "Tier 1 (Retrieval)",
                "tier_2": "Tier 2 (Generation)",
                "tier_3": "Tier 3 (E2E)",
                "tier_judge": "Tier Judge (Qwen)",
                "tier_judge_pipeline": "Tier Judge Pipeline",
                "tier_ux": "Tier UX",
                "tier_real_users": "Real Users",
            }

            longest_dates: List[str] = []
            for tier_name, label in series_by_tier.items():
                if tier_name not in METRICS_BY_TIER:
                    continue
                if metric not in METRICS_BY_TIER[tier_name]:
                    continue
                dates, values = self.get_metric_history(tier_name, metric)
                if not dates:
                    continue
                if len(dates) > len(longest_dates):
                    longest_dates = dates[:]
                all_rows.extend(
                    self._build_series_rows(dates, values, metric, series_name=label)
                )

            if not all_rows:
                return pd.DataFrame(columns=["timestamp", "value", "series"])

            baseline = QUALITY_BASELINES.get(metric)
            if baseline is not None and longest_dates:
                baseline_dates = longest_dates
                if len(baseline_dates) == 1:
                    baseline_dates = [
                        baseline_dates[0],
                        f"{baseline_dates[0]}_point",
                    ]
                all_rows.extend(
                    {
                        "timestamp": date,
                        "value": baseline,
                        "series": "Baseline",
                    }
                    for date in baseline_dates
                )

            deduped = []
            seen = set()
            for row in all_rows:
                key = (row["timestamp"], row["value"], row["series"])
                if key in seen:
                    continue
                seen.add(key)
                deduped.append(row)
            return pd.DataFrame(deduped)

        metric_dropdown.change(
            fn=update_comparison_plot,
            inputs=[metric_dropdown],
            outputs=[plot],
        )

    def _create_all_runs_tab(self):
        mode_filter = gr.Dropdown(
            choices=["all", "synthetic", "manual", "real-users"],
            value="all",
            label="–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –¥–∞—Ç–∞—Å–µ—Ç–∞",
        )

        def build_registry_rows(filter_mode: str):
            rows = []
            for run in reversed(self.runs):
                run_mode = run.get("dataset_type", "synthetic")
                if filter_mode != "all" and run_mode != filter_mode:
                    continue
                t1 = run.get("tier_1", {})
                t2 = run.get("tier_2", {})
                t3 = run.get("tier_3", {})
                tjp = run.get("tier_judge_pipeline", {})
                rows.append(
                    [
                        run["timestamp_readable"][:16],
                        run["git_branch"][:12] if run.get("git_branch") else "N/A",
                        run.get("dataset_type", "synthetic")[:8],
                        round(_safe_float(t1.get("mrr")), 2),
                        round(_safe_float(t1.get("hit_rate@5")), 2),
                        round(_safe_float(t2.get("avg_faithfulness")), 2),
                        round(_safe_float(t2.get("avg_answer_relevance")), 2),
                        round(_safe_float(t3.get("avg_e2e_score")), 2),
                        round(_safe_float(tjp.get("accuracy")), 2) if tjp else "-",
                        run["overall_status"][:8],
                    ]
                )
            return rows

        headers = [
            "Time",
            "Branch",
            "Type",
            "MRR",
            "Hit@5",
            "Faith",
            "Relev",
            "E2E",
            "TJP Acc",
            "Status",
        ]

        initial_rows = build_registry_rows("all")
        table = gr.Dataframe(
            value=initial_rows,
            headers=headers,
            interactive=False,
            wrap=False,
        )

        def update_table(filter_mode: str):
            return gr.Dataframe(value=build_registry_rows(filter_mode), headers=headers)

        mode_filter.change(
            fn=update_table,
            inputs=[mode_filter],
            outputs=[table],
        )

    def _create_run_dataset_tab(self):
        ordered_runs = list(reversed(self.runs))

        if not ordered_runs:
            gr.Markdown("–ù–µ—Ç –∑–∞–ø—É—Å–∫–æ–≤ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞.")
            return

        def format_run_choice(run: Dict) -> str:
            return (
                f"{run['timestamp_readable']} | {run['dataset_type']} | "
                f"{run['git_commit_hash']} | {run['dataset_file']}"
            )

        def get_run_choices(filter_mode: str) -> List[str]:
            if filter_mode == "all":
                return [format_run_choice(run) for run in ordered_runs]
            return [
                format_run_choice(run)
                for run in ordered_runs
                if run.get("dataset_type") == filter_mode
            ]

        def load_dataset_preview(selected: str) -> tuple:
            if not selected:
                return "", []

            run = next(
                (item for item in ordered_runs if format_run_choice(item) == selected),
                None,
            )
            if run is None:
                return "–ù–µ –Ω–∞–π–¥–µ–Ω –≤—ã–±—Ä–∞–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫", []

            if run.get("dataset_type") == "real-users":
                real_metrics = run.get("tier_real_users", {})
                info = "\n".join(
                    [
                        f"**Dataset:** `{run.get('dataset_file', 'unknown')}`",
                        f"**Mode:** `{run.get('dataset_type', 'real-users')}`",
                        f"**MRR:** `{_safe_float(real_metrics.get('mrr')):.4f}`",
                        f"**Recall@5:** `{_safe_float(real_metrics.get('recall@5')):.4f}`",
                        f"**NDCG@5:** `{_safe_float(real_metrics.get('ndcg@5')):.4f}`",
                    ]
                )
                return info, []

            dataset_file = run.get("dataset_file") or ""
            benchmark_dir = Path(__file__).resolve().parent
            project_root = benchmark_dir.parent
            candidate_paths = [
                benchmark_dir / "data" / dataset_file,
                project_root / "benchmarks" / "data" / dataset_file,
                Path.cwd() / "data" / dataset_file,
                Path.cwd() / "benchmarks" / "data" / dataset_file,
            ]
            dataset_path = next(
                (path for path in candidate_paths if path.exists()),
                candidate_paths[0],
            )

            if not dataset_path.exists():
                return (
                    f"**Dataset:** `{dataset_file}`\n\n–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ `{dataset_path}`",
                    [],
                )

            try:
                import json

                with open(dataset_path, "r", encoding="utf-8") as f:
                    dataset = json.load(f)

                preview = []
                for row in dataset[:30]:
                    preview.append(
                        [
                            row.get("chunk_id"),
                            row.get("question", ""),
                            row.get("ground_truth_answer", ""),
                            row.get("confluence_url", ""),
                            ", ".join(
                                map(str, row.get("relevant_chunk_ids", []) or [])
                            ),
                            ", ".join(row.get("relevant_urls", []) or []),
                        ]
                    )

                meta = (
                    f"**Dataset:** `{dataset_file}`\n"
                    f"\n**Mode:** `{run.get('dataset_type', 'synthetic')}`\n"
                    f"\n**Rows:** {len(dataset)}\n"
                    f"\n**Linked run:** `{run['timestamp_readable']}`"
                )

                if dataset and isinstance(dataset[0], dict):
                    if (
                        "relevant_chunk_ids" in dataset[0]
                        or "relevant_urls" in dataset[0]
                    ):
                        meta += (
                            "\n\n`Manual dataset detected`: –¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ–ª—è "
                            "`relevant_chunk_ids`, `relevant_urls`, `notes`."
                        )

                return meta, preview
            except Exception as error:
                return (
                    f"**Dataset:** `{dataset_file}`\n\n–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: `{error}`",
                    [],
                )

        mode_filter = gr.Dropdown(
            choices=["all", "synthetic", "manual", "real-users"],
            value="all",
            label="–¢–∏–ø –∑–∞–ø—É—Å–∫–∞",
        )

        run_choices = get_run_choices("all")

        initial_meta, initial_preview = load_dataset_preview(run_choices[0])

        run_selector = gr.Dropdown(
            choices=run_choices,
            value=run_choices[0],
            label="–í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–ø—É—Å–∫",
        )
        dataset_meta = gr.Markdown(value=initial_meta)
        dataset_preview = gr.Dataframe(
            value=initial_preview,
            headers=[
                "chunk_id",
                "question",
                "ground_truth_answer",
                "confluence_url",
                "relevant_chunk_ids",
                "relevant_urls",
            ],
            interactive=False,
            wrap=True,
        )

        def update_on_filter_change(filter_mode: str):
            options = get_run_choices(filter_mode)
            if not options:
                return (
                    gr.Dropdown(choices=[], value=None),
                    "",
                    [[]],
                )
            meta, preview = load_dataset_preview(options[0])
            return (
                gr.Dropdown(choices=options, value=options[0]),
                meta,
                preview,
            )

        def update_on_run_select(selected: str):
            meta, preview = load_dataset_preview(selected)
            return meta, preview

        mode_filter.change(
            fn=update_on_filter_change,
            inputs=[mode_filter],
            outputs=[run_selector, dataset_meta, dataset_preview],
        )

        run_selector.change(
            fn=update_on_run_select,
            inputs=[run_selector],
            outputs=[dataset_meta, dataset_preview],
        )

    def _create_reference_tab(self):
        gr.Markdown(
            """
# –°–ø—Ä–∞–≤–∫–∞ –ø–æ —Å–∏—Å—Ç–µ–º–µ –æ—Ü–µ–Ω–∫–∏ RAG

## –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ

–î–∞—à–±–æ—Ä–¥ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ Retrieval-Augmented Generation —Å–∏—Å—Ç–µ–º—ã. 
–û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏–∫—É –∏–∑–º–µ–Ω–µ–Ω–∏–π –º–µ–∂–¥—É –∑–∞–ø—É—Å–∫–∞–º–∏.

---

## –£—Ä–æ–≤–Ω–∏ –æ—Ü–µ–Ω–∫–∏ (Tiers)

### Tier 0: Embedding Quality

–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM.

- **avg_intra_cluster_sim**: —Å—Ä–µ–¥–Ω—è—è –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞. –ß–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ embeddings –≥—Ä—É–ø–ø–∏—Ä—É—é—Ç—Å—è –ø–æ —Å–º—ã—Å–ª—É.
- **avg_inter_cluster_dist**: —Å—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏. –ß–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã.
- **silhouette_score**: —Å–∏–ª—É—ç—Ç-–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ—Ç -1 –¥–æ 1. –ó–Ω–∞—á–µ–Ω–∏—è –±–ª–∏–∂–µ –∫ 1 —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ö–æ—Ä–æ—à—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é.

### Tier 1: Retrieval

–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

- **HitRate@K**: –¥–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –≤ —Ç–æ–ø-K.
- **MRR (Mean Reciprocal Rank)**: —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞–Ω–≥–∞ –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
- **Recall@K**: –ø–æ–ª–Ω–æ—Ç–∞ - –¥–æ–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞.
- **Precision@K**: —Ç–æ—á–Ω–æ—Å—Ç—å - –¥–æ–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å—Ä–µ–¥–∏ —Ç–æ–ø-K.
- **NDCG@K**: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –≤—ã–≥–æ–¥–∞.

–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è MRR –∏ Recall –æ–∑–Ω–∞—á–∞—é—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.

### Tier 2: Generation

–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.

- **avg_faithfulness**: –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (—à–∫–∞–ª–∞ 1-5).
- **avg_answer_relevance**: –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –≤–æ–ø—Ä–æ—Å—É (—à–∫–∞–ª–∞ 1-5).

–≠—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –≤—ã—Å—Ç–∞–≤–ª—è—é—Ç—Å—è LLM-—Å—É–¥—å–µ–π. –ó–Ω–∞—á–µ–Ω–∏—è –≤—ã—à–µ 4.0 —Å—á–∏—Ç–∞—é—Ç—Å—è —Ö–æ—Ä–æ—à–∏–º–∏.

### Tier 3: End-to-End

–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω retrieval + generation.

- **avg_e2e_score**: –æ–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ LLM-—Å—É–¥—å–µ–π (—à–∫–∞–ª–∞ 1-5).
- **avg_semantic_similarity**: –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–∞–º–∏.

### Tier Judge (Qwen)

–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å judge-–º–æ–¥–µ–ª–∏ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö.

- **consistency_score**: –¥–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–¥–µ –æ—Ü–µ–Ω–∫–∞ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –∑–∞–ø—É—Å–∫–µ.
- **error_rate**: –¥–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–¥–µ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ API.
- **avg_latency_ms**: —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ API –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö.

### Tier Judge Pipeline (Mistral)

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç production judge, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç "–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –∏–ª–∏ –Ω–µ—Ç".

- **accuracy**: —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–π —Å—É–¥—å–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
- **precision/recall/f1_score**: –¥–ª—è –∫–ª–∞—Å—Å–∞ "–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ—Ç–≤–µ—Ç".

### Tier UX

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è.

- **cache_hit_rate**: –¥–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤ –∫—ç—à–µ –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.
- **context_preserve**: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –º–Ω–æ–≥–æturn –¥–∏–∞–ª–æ–≥–∞—Ö.
- **multi_turn_consistency**: —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏.

### Real Users

–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã QuestionAnswer.

- –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–µ –∂–µ –º–µ—Ç—Ä–∏–∫–∏ retrieval: MRR, Recall@K, Precision@K, NDCG@K.
- –ü–æ–∑–≤–æ–ª—è—é—Ç –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º —Ç—Ä–∞—Ñ–∏–∫–µ.

---

## –¢–∏–ø—ã –∑–∞–ø—É—Å–∫–æ–≤

### Synthetic

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ LLM. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.

### Manual / –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è

–î–∞—Ç–∞—Å–µ—Ç, –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–ª—è —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏. –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å —É—á–∞—Å—Ç–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞-—ç–∫—Å–ø–µ—Ä—Ç–∞.

### Real Users

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã QuestionAnswer. –ù–∞–∏–±–æ–ª–µ–µ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π.

---

## –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–æ—Ä–æ–≥–∏ (Baseline)

| Tier | –ú–µ—Ç—Ä–∏–∫–∞ | –ü–æ—Ä–æ–≥ |
|------|---------|-------|
| Tier 0 | avg_intra_cluster_sim | >= 0.85 |
| Tier 0 | silhouette_score | >= 0.50 |
| Tier 1 | MRR | >= 0.80 |
| Tier 1 | HitRate@5 | >= 0.90 |
| Tier 1 | HitRate@10 | >= 0.95 |
| Tier 2 | Faithfulness | >= 4.5 |
| Tier 2 | Answer Relevance | >= 4.2 |
| Tier 3 | E2E Score | >= 4.2 |
| Tier 3 | Semantic Similarity | >= 0.85 |
| Tier Judge | consistency_score | >= 0.90 |
| Tier Judge | error_rate | <= 0.05 |
| Tier Judge Pipeline | accuracy | >= 0.85 |
| Tier Judge Pipeline | f1_score | >= 0.85 |

---

## –†–∞–±–æ—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π

- **Metric History**: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏.
- **Tier Comparison**: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π –Ω–∞ –æ–¥–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ.
- **Runs Registry**: —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–ø—É—Å–∫–æ–≤ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±–∑–æ—Ä–∞.

–ü—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–π –æ–±—Ä–∞—â–∞–π—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∏–∂–µ baseline - –æ–Ω–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ —Å–∏—Å—Ç–µ–º–µ.
            """
        )


def find_free_port(start_port=7860, max_port=7870):
    """–ù–∞–π—Ç–∏ —Å–≤–æ–±–æ–¥–Ω—ã–π –ø–æ—Ä—Ç –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ."""
    import socket

    for port in range(start_port, max_port + 1):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.bind(("0.0.0.0", port))
                return port
            except OSError:
                continue
    return None


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –¥–∞—à–±–æ—Ä–¥–∞."""
    if not GRADIO_AVAILABLE:
        print("Gradio –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: uv sync --extra dashboard")
        return

    import argparse

    parser = argparse.ArgumentParser(description="–ó–∞–ø—É—Å–∫ –¥–∞—à–±–æ—Ä–¥–∞ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤")
    parser.add_argument(
        "--port",
        type=int,
        default=None,
        help="–ü–æ—Ä—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –¥–∞—à–±–æ—Ä–¥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –ø–µ—Ä–≤—ã–π —Å–≤–æ–±–æ–¥–Ω—ã–π –æ—Ç 7860)",
    )
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—Ç
    if args.port:
        port = args.port
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–æ—Ä—Ç: {port}")
    else:
        port = find_free_port()
        if port:
            logger.info(f"–ù–∞–π–¥–µ–Ω —Å–≤–æ–±–æ–¥–Ω—ã–π –ø–æ—Ä—Ç: {port}")
        else:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–æ–±–æ–¥–Ω—ã–π –ø–æ—Ä—Ç –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 7860-7870")
            return

    dashboard = RAGBenchmarkDashboard()
    interface = dashboard.create_interface()
    interface.launch(server_name="0.0.0.0", server_port=port, share=False, debug=True)


if __name__ == "__main__":
    main()
