"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–æ–ª–æ—Ç–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM-—Å—É–¥—å—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∏–¥–µ–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞–Ω–∫–æ–≤.
"""

import argparse
import json
import logging
import os
import sys
from datetime import datetime
from difflib import SequenceMatcher
from pathlib import Path
from dotenv import load_dotenv

sys.path.insert(0, str(Path(__file__).parent.parent))

from qa.config import Config
from qa.database import Chunk, create_engine
from benchmarks.utils.llm_judge import LLMJudge

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

logger = logging.getLogger(__name__)

# –Ø–≤–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º .env.docker –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
# –í Docker –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è .env.docker –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ docker compose
load_dotenv(dotenv_path=".env.docker")


def generate_synthetic_dataset(
    engine, num_samples: int, output_path: str, skip_existing: bool = True
):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç.

    Args:
        engine: –î–≤–∏–∂–æ–∫ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        skip_existing: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å —á–∞–Ω–∫–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —É–∂–µ –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã
    """
    from sqlalchemy import func, select
    from sqlalchemy.orm import Session

    judge = LLMJudge()

    def normalize_question(question: str) -> str:
        return " ".join(question.lower().split())

    def is_too_similar(candidate: str, existing: set[str]) -> bool:
        normalized_candidate = normalize_question(candidate)
        if normalized_candidate in existing:
            return True
        for existing_question in existing:
            if (
                SequenceMatcher(None, normalized_candidate, existing_question).ratio()
                >= 0.92
            ):
                return True
        return False

    with Session(engine) as session:
        total_chunks = session.scalars(select(func.count(Chunk.id))).one()
        logger.info(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –≤ –ë–î: {total_chunks}")

        available_chunks = session.scalars(
            select(Chunk)
            .where(Chunk.embedding.isnot(None))
            .where(Chunk.text.isnot(None))
            .order_by(func.random())
        ).all()

        logger.info(f"–ß–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {len(available_chunks)}")

        if len(available_chunks) < num_samples:
            logger.warning(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: "
                f"—Ç—Ä–µ–±—É–µ—Ç—Å—è {num_samples}, –¥–æ—Å—Ç—É–ø–Ω–æ {len(available_chunks)}"
            )
            num_samples = len(available_chunks)

        dataset = []
        seen_questions: set[str] = set()

        for i, chunk in enumerate(available_chunks, 1):
            if len(dataset) >= num_samples:
                break
            try:
                result = judge.generate_question_from_chunk(chunk.text)
                question = result["question"].strip()

                if is_too_similar(question, seen_questions):
                    logger.info(
                        "–î—É–±–ª–∏–∫–∞—Ç/—Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏–π –≤–æ–ø—Ä–æ—Å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: %s", question
                    )
                    continue

                seen_questions.add(normalize_question(question))

                dataset_item = {
                    "chunk_id": chunk.id,
                    "chunk_text": chunk.text[:500],
                    "question": question,
                    "ground_truth_answer": result["ground_truth_answer"],
                    "confluence_url": chunk.confluence_url,
                }

                dataset.append(dataset_item)

                logger.info(f"[{len(dataset)}/{num_samples}] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {question}")

                if len(dataset) % 10 == 0:
                    logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {len(dataset)}/{num_samples}")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –¥–ª—è —á–∞–Ω–∫–∞ {chunk.id}: {e}")
                continue

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)

    logger.info(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")
    logger.info(f"üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(dataset)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI —Å–∫—Ä–∏–ø—Ç–∞."""
    parser = argparse.ArgumentParser(
        description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤"
    )

    parser.add_argument(
        "--num-samples",
        type=int,
        default=100,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (default: 100)",
    )

    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é versioned dataset_YYYYMMDD_HHMMSS.json)",
    )

    parser.add_argument(
        "--check-only",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç",
    )

    args = parser.parse_args()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = args.output or f"benchmarks/data/dataset_{timestamp}.json"

    engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)

    if args.check_only:
        if os.path.exists(output_path):
            with open(output_path, "r", encoding="utf-8") as f:
                dataset = json.load(f)
            print(f"–î–∞—Ç–∞—Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {output_path}")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(dataset)}")
            if dataset:
                print(f"\n–ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
                print(json.dumps(dataset[0], ensure_ascii=False, indent=2))
        else:
            print(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {output_path}")
        return

    generate_synthetic_dataset(engine, args.num_samples, output_path)


if __name__ == "__main__":
    main()
