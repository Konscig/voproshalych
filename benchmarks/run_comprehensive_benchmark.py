"""CLI —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ RAG-—Å–∏—Å—Ç–µ–º—ã.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python run_comprehensive_benchmark.py --tier all --limit 50
    python run_comprehensive_benchmark.py --tier 1 --dataset benchmarks/data/dataset_20260216_143000.json
"""

import argparse
import glob
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

import numpy as np
from sqlalchemy import func

sys.path.insert(0, str(Path(__file__).parent.parent))

from qa.config import Config
from qa.database import (
    BenchmarkRun,
    Chunk,
    create_engine,
    ensure_benchmark_runs_schema,
)
from qa.database import QuestionAnswer
from benchmarks.models.real_queries_benchmark import RealQueriesBenchmark
from benchmarks.models.rag_benchmark import RAGBenchmark
from benchmarks.utils.llm_judge import LLMJudge
from benchmarks.utils.report_generator import ReportGenerator
from sentence_transformers import SentenceTransformer

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

logger = logging.getLogger(__name__)

# –Ø–≤–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º .env.docker –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
# –í Docker –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è .env.docker –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ docker compose
from dotenv import load_dotenv

load_dotenv(dotenv_path=".env.docker")


def check_prerequisites(
    engine,
    mode: str,
    dataset_path: Optional[str] = None,
) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤.

    Args:
        engine: –î–≤–∏–∂–æ–∫ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        mode: –†–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞ (synthetic/manual/real-users)
        dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É

    Returns:
        True –µ—Å–ª–∏ –≤—Å–µ —É—Å–ª–æ–≤–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã
    """
    from sqlalchemy import select
    from sqlalchemy.orm import Session

    logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π...")

    with Session(engine) as session:
        total_chunks = session.scalars(select(Chunk)).all()
        chunks_with_embeddings = [
            c for c in total_chunks if c.embedding is not None and len(c.embedding) > 0
        ]

        logger.info(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(total_chunks)}")
        logger.info(f"–ß–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {len(chunks_with_embeddings)}")

        if len(chunks_with_embeddings) == 0:
            logger.error(
                "‚ùå –ù–µ—Ç —á–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏! "
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ: python benchmarks/generate_embeddings.py --chunks"
            )
            return False

    if mode in {"synthetic", "manual"} and (
        not dataset_path or not os.path.exists(dataset_path)
    ):
        logger.error(
            f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}\n"
            "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç: "
            "python benchmarks/generate_dataset.py --max-questions 50"
        )
        return False

    if mode == "real-users":
        with Session(engine) as session:
            total_real = session.scalar(
                select(func.count(QuestionAnswer.id)).where(
                    QuestionAnswer.embedding.isnot(None)
                )
            )
        if not total_real:
            logger.error("‚ùå –í —Ç–∞–±–ª–∏—Ü–µ question_answer –Ω–µ—Ç –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏")
            return False

    logger.info("‚úÖ –í—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã")
    return True


def load_dataset(dataset_path: str, limit: Optional[int] = None) -> list:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ JSON —Ñ–∞–π–ª–∞.

    Args:
        dataset_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞—Ç–∞—Å–µ—Ç–∞
        limit: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π

    Returns:
        –°–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π –¥–∞—Ç–∞—Å–µ—Ç–∞
    """
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ {dataset_path}")

    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)

    if limit:
        dataset = dataset[:limit]

    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –∑–∞–ø–∏—Å–µ–π")

    return dataset


def resolve_dataset_path(dataset_path: str) -> str:
    """–†–∞–∑—Ä–µ—à–∏—Ç—å –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É, —É—á–∏—Ç—ã–≤–∞—è versioned —Ñ–∞–π–ª—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
    if dataset_path != "benchmarks/data/golden_dataset_synthetic.json":
        return dataset_path

    if os.path.exists(dataset_path):
        return dataset_path

    candidates = sorted(glob.glob("benchmarks/data/dataset_*.json"))
    if candidates:
        latest = candidates[-1]
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π versioned –¥–∞—Ç–∞—Å–µ—Ç: %s", latest)
        return latest

    return dataset_path


def resolve_manual_dataset_path(
    manual_dataset: Optional[str], dataset_path: str
) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å –∫ manual dataset."""
    if manual_dataset:
        return manual_dataset
    return dataset_path


def run_benchmark(
    engine,
    encoder: SentenceTransformer,
    judge: Optional[LLMJudge],
    dataset: Optional[list],
    tier: str,
    mode: str,
    top_k: int = 10,
    real_score_filter: int | None = 5,
    real_limit: int = 500,
    real_latest: bool = False,
):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –±–µ–Ω—á–º–∞—Ä–∫.

    Args:
        engine: –î–≤–∏–∂–æ–∫ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        encoder: –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        judge: LLM-—Å—É–¥—å—è
        dataset: –î–∞—Ç–∞—Å–µ—Ç
        tier: –£—Ä–æ–≤–µ–Ω—å –±–µ–Ω—á–º–∞—Ä–∫–∞ (1, 2, 3, all)
        mode: –†–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞ (synthetic/manual/real-users)
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ (Tier 1)

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
    """
    if mode == "real-users":
        real_benchmark = RealQueriesBenchmark(engine, encoder)
        return {
            "tier_real_users": real_benchmark.run(
                top_k=top_k,
                score_filter=real_score_filter,
                limit=real_limit,
                latest=real_latest,
            )
        }

    benchmark = RAGBenchmark(engine, encoder, judge)
    dataset = dataset or []

    if tier == "all":
        return benchmark.run_all_tiers(dataset, top_k=top_k)
    elif tier == "1":
        return {"tier_1": benchmark.run_tier_1(dataset, top_k=top_k)}
    elif tier == "2":
        return {"tier_2": benchmark.run_tier_2(dataset)}
    elif tier == "3":
        return {"tier_3": benchmark.run_tier_3(dataset)}
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –±–µ–Ω—á–º–∞—Ä–∫–∞: {tier}")


def save_results(
    results: dict,
    output_dir: str,
    dataset_name: str,
    engine,
    mode: str,
):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞.

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        dataset_name: –ò–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        engine: –î–≤–∏–∂–æ–∫ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    """
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    def to_builtin(value: Any) -> Any:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å numpy-—Ç–∏–ø—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–∏–ø—ã Python."""
        if isinstance(value, dict):
            return {k: to_builtin(v) for k, v in value.items()}
        if isinstance(value, list):
            return [to_builtin(v) for v in value]
        if isinstance(value, np.generic):
            return value.item()
        return value

    normalized_results: dict = to_builtin(results)

    report_generator = ReportGenerator()
    run_metadata = report_generator.get_run_metadata()
    overall_status = report_generator.evaluate_overall_status(normalized_results)

    artifact_payload = dict(normalized_results)
    artifact_payload["run_metadata"] = run_metadata
    artifact_payload["overall_status"] = overall_status
    artifact_payload["dataset_file"] = os.path.basename(dataset_name)
    artifact_payload["dataset_type"] = mode

    json_path = os.path.join(output_dir, f"rag_benchmark_{timestamp}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(artifact_payload, f, ensure_ascii=False, indent=2)

    markdown_path = os.path.join(output_dir, f"rag_benchmark_{timestamp}.md")
    markdown_report = report_generator.generate_benchmark_report(
        normalized_results,
        dataset_name=dataset_name,
        metadata=run_metadata,
    )

    with open(markdown_path, "w", encoding="utf-8") as f:
        f.write(markdown_report)

    ensure_benchmark_runs_schema(engine)

    from sqlalchemy.orm import Session

    with Session(engine) as session:
        session.add(
            BenchmarkRun(
                timestamp=datetime.now(),
                git_branch=run_metadata["git_branch"],
                git_commit_hash=run_metadata["git_commit_hash"],
                run_author=run_metadata["run_author"],
                dataset_file=os.path.basename(dataset_name),
                dataset_type=mode,
                tier_1_metrics=normalized_results.get("tier_1"),
                tier_2_metrics=normalized_results.get("tier_2"),
                tier_3_metrics=normalized_results.get("tier_3"),
                real_user_metrics=normalized_results.get("tier_real_users"),
                overall_status=overall_status,
            )
        )
        session.commit()

    logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    logger.info(f"   JSON: {json_path}")
    logger.info(f"   Markdown: {markdown_path}")
    logger.info("   DB table: benchmark_runs")


def print_results(results: dict):
    """–í—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ –≤ –∫–æ–Ω—Å–æ–ª—å.

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
    """
    print("\n" + "=" * 60)
    print("RAG BENCHMARK RESULTS")
    print("=" * 60 + "\n")

    for tier_name, tier_results in results.items():
        print(f"üìä {tier_name.upper()}")
        print("-" * 60)

        for key, value in tier_results.items():
            if key == "tier":
                continue
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
            else:
                print(f"  {key}: {value}")

        print()

    print("=" * 60 + "\n")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI —Å–∫—Ä–∏–ø—Ç–∞."""
    parser = argparse.ArgumentParser(
        description="–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ RAG-—Å–∏—Å—Ç–µ–º—ã"
    )

    parser.add_argument(
        "--tier",
        type=str,
        choices=["1", "2", "3", "all"],
        default="all",
        help="–£—Ä–æ–≤–µ–Ω—å –±–µ–Ω—á–º–∞—Ä–∫–∞ (1, 2, 3 –∏–ª–∏ all)",
    )

    parser.add_argument(
        "--mode",
        type=str,
        choices=["synthetic", "manual", "real-users"],
        default="synthetic",
        help="–†–µ–∂–∏–º –±–µ–Ω—á–º–∞—Ä–∫–∞: synthetic, manual –∏–ª–∏ real-users",
    )

    parser.add_argument(
        "--dataset",
        type=str,
        default="benchmarks/data/golden_dataset_synthetic.json",
        help=(
            "–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞—Ç–∞—Å–µ—Ç–∞. –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—É—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏ —Ñ–∞–π–ª "
            "–Ω–µ –Ω–∞–π–¥–µ–Ω, –±—É–¥–µ—Ç –≤—ã–±—Ä–∞–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π benchmarks/data/dataset_*.json"
        ),
    )

    parser.add_argument(
        "--manual-dataset",
        type=str,
        default=None,
        help="–ü—É—Ç—å –∫ manual dataset (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ --mode manual)",
    )

    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞",
    )

    parser.add_argument(
        "--top-k",
        type=int,
        default=10,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ (Tier 1, default: 10)",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default="benchmarks/reports",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
    )

    parser.add_argument(
        "--skip-checks",
        action="store_true",
        help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π",
    )

    parser.add_argument(
        "--real-score",
        type=int,
        default=5,
        help="–§–∏–ª—å—Ç—Ä score –¥–ª—è real-users —Ä–µ–∂–∏–º–∞ (default: 5)",
    )

    parser.add_argument(
        "--real-limit",
        type=int,
        default=500,
        help="–õ–∏–º–∏—Ç –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è real-users —Ä–µ–∂–∏–º–∞",
    )

    parser.add_argument(
        "--real-latest",
        action="store_true",
        help="–ë—Ä–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ real-user –≤–æ–ø—Ä–æ—Å—ã –ø–æ created_at",
    )

    args = parser.parse_args()

    engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)

    model_path = Config.EMBEDDING_MODEL_PATH
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_path}")
    logger.info("–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1-2 –º–∏–Ω—É—Ç—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ...")
    encoder = SentenceTransformer(model_path, device="cpu")
    logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

    resolved_dataset = resolve_dataset_path(args.dataset)
    if args.mode == "manual":
        resolved_dataset = resolve_manual_dataset_path(
            args.manual_dataset, args.dataset
        )

    if not args.skip_checks:
        if not check_prerequisites(
            engine,
            mode=args.mode,
            dataset_path=resolved_dataset,
        ):
            sys.exit(1)

    dataset: Optional[list] = None
    if args.mode in {"synthetic", "manual"}:
        dataset = load_dataset(resolved_dataset, args.limit)

    needs_judge = args.mode in {"synthetic", "manual"} and args.tier in {
        "2",
        "3",
        "all",
    }
    judge = LLMJudge() if needs_judge else None

    if dataset is not None:
        logger.info(
            "–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ mode=%s, tier=%s, dataset_rows=%s",
            args.mode,
            args.tier,
            len(dataset),
        )
    else:
        logger.info("–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ mode=%s, tier=%s", args.mode, args.tier)

    results = run_benchmark(
        engine,
        encoder,
        judge,
        dataset,
        args.tier,
        args.mode,
        args.top_k,
        real_score_filter=args.real_score,
        real_limit=args.real_limit,
        real_latest=args.real_latest,
    )

    dataset_name = resolved_dataset
    if args.mode == "real-users":
        dataset_name = f"real_users_score_{args.real_score}"

    save_results(
        results,
        args.output_dir,
        dataset_name=dataset_name,
        engine=engine,
        mode=args.mode,
    )
    print_results(results)

    logger.info("‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à—ë–Ω")


if __name__ == "__main__":
    main()
