"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è RAG-–±–µ–Ω—á–º–∞—Ä–∫–æ–≤.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µ–∂–∏–º—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:
- synthetic: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ LLM
- export-annotation: —ç–∫—Å–ø–æ—Ä—Ç synthetic/manual –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Ä—É—á–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
"""

import argparse
import hashlib
import json
import logging
import os
import shutil
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv

sys.path.insert(0, str(Path(__file__).parent.parent))

from qa.config import Config
from qa.database import Chunk, create_engine
from benchmarks.utils.llm_judge import LLMJudge

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

logger = logging.getLogger(__name__)

# –Ø–≤–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º .env.docker –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
# –í Docker –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è .env.docker –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ docker compose
load_dotenv(dotenv_path=".env.docker")


def _generate_item_id(text: str, prefix: str = "") -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–µ—à–∞ —Ç–µ–∫—Å—Ç–∞."""
    hash_obj = hashlib.md5(text.encode("utf-8"))
    return f"{prefix}{hash_obj.hexdigest()[:12]}"


def _create_dataset_item(
    question: str,
    ground_truth_answer: str,
    chunk_id: Optional[int] = None,
    chunk_text: Optional[str] = None,
    confluence_url: Optional[str] = None,
    source: str = "synthetic",
    question_source: str = "synthetic",
    relevant_chunk_ids: Optional[List[int]] = None,
    user_score: Optional[int] = None,
    question_answer_id: Optional[int] = None,
    is_relevant_chunk_matched: Optional[int] = None,
) -> Dict[str, Any]:
    """–°–æ–∑–¥–∞—Ç—åÊ†áÂáÜÂåñ–Ω—É—é –∑–∞–ø–∏—Å—å –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""
    item = {
        "id": _generate_item_id(question, f"{source[:3]}_"),
        "source": source,
        "question_source": question_source,
        "question": question.strip(),
        "ground_truth_answer": ground_truth_answer.strip(),
    }

    if chunk_id is not None:
        item["chunk_id"] = chunk_id
    if chunk_text:
        item["chunk_text"] = chunk_text[:500] if len(chunk_text) > 500 else chunk_text
    if confluence_url:
        item["confluence_url"] = confluence_url
    if relevant_chunk_ids:
        item["relevant_chunk_ids"] = relevant_chunk_ids
    if user_score is not None:
        item["user_score"] = user_score
    if question_answer_id is not None:
        item["question_answer_id"] = question_answer_id
    if is_relevant_chunk_matched is not None:
        item["is_relevant_chunk_matched"] = is_relevant_chunk_matched

    return item


def _load_existing_dataset(dataset_path: str) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è."""
    if not dataset_path or not os.path.exists(dataset_path):
        return []

    with open(dataset_path, "r", encoding="utf-8") as file:
        payload = json.load(file)

    if not isinstance(payload, list):
        logger.warning("–§–∞–π–ª %s –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º", dataset_path)
        return []

    return payload


def generate_synthetic_dataset(
    engine,
    max_questions: int,
    output_path: str,
    skip_existing_dataset: str | None = None,
    generation_attempts: int = 3,
):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç.

    Args:
        engine: –î–≤–∏–∂–æ–∫ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        max_questions: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        skip_existing_dataset: –ü—É—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
        generation_attempts: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
    """
    from sqlalchemy import func, select
    from sqlalchemy.orm import Session

    judge = LLMJudge()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    generation_errors: List[Dict[str, Any]] = []

    existing_dataset = _load_existing_dataset(skip_existing_dataset or "")
    existing_chunk_ids = {
        item.get("chunk_id")
        for item in existing_dataset
        if isinstance(item, dict) and item.get("chunk_id") is not None
    }
    dataset: List[Dict[str, Any]] = [
        item for item in existing_dataset if isinstance(item, dict)
    ]

    if existing_dataset:
        logger.info(
            "–ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç: %s –∑–∞–ø–∏—Å–µ–π, %s —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö chunk_id",
            len(existing_dataset),
            len(existing_chunk_ids),
        )

    with Session(engine) as session:
        total_chunks = session.scalar(select(func.count(Chunk.id))) or 0
        logger.info(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –≤ –ë–î: {total_chunks}")

        available_chunks = session.scalars(
            select(Chunk)
            .where(Chunk.embedding.isnot(None))
            .where(Chunk.text.isnot(None))
            .where(Chunk.text != "")
            .order_by(Chunk.id.asc())
        ).all()

        available_chunks_total = len(available_chunks)
        logger.info(f"–ß–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {available_chunks_total}")

        for i, chunk in enumerate(available_chunks, 1):
            if len(dataset) >= max_questions:
                logger.info("–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç max_questions=%s", max_questions)
                break
            if chunk.id in existing_chunk_ids:
                continue

            try:
                result = None
                last_error: Exception | None = None
                for attempt in range(1, generation_attempts + 1):
                    try:
                        result = judge.generate_question_from_chunk(chunk.text)
                        break
                    except Exception as error:  # noqa: PERF203
                        last_error = error
                        logger.warning(
                            "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ chunk_id=%s (attempt %s/%s): %s",
                            chunk.id,
                            attempt,
                            generation_attempts,
                            error,
                        )

                if result is None:
                    generation_errors.append(
                        {
                            "chunk_id": chunk.id,
                            "reason": str(last_error or "generation_failed"),
                        }
                    )
                    continue

                dataset_item = _create_dataset_item(
                    question=result["question"],
                    ground_truth_answer=result["ground_truth_answer"],
                    chunk_id=chunk.id,
                    chunk_text=chunk.text,
                    confluence_url=chunk.confluence_url,
                    source="synthetic",
                    question_source="synthetic",
                )

                dataset.append(dataset_item)
                existing_chunk_ids.add(chunk.id)

                logger.info(
                    "[%s/%s] chunk_id=%s, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –≤–æ–ø—Ä–æ—Å",
                    len(dataset),
                    max_questions,
                    chunk.id,
                )

                if len(dataset) % 10 == 0:
                    logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {len(dataset)}/{max_questions}")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –¥–ª—è —á–∞–Ω–∫–∞ {chunk.id}: {e}")
                generation_errors.append({"chunk_id": chunk.id, "reason": str(e)})
                continue

            if i % 200 == 0:
                logger.info("–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —á–∞–Ω–∫–æ–≤: %s/%s", i, available_chunks_total)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)

    logger.info(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")
    logger.info(f"üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(dataset)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
    if generation_errors:
        logger.warning("üìõ –û—à–∏–±–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: %s", len(generation_errors))
        for err in generation_errors[:5]:
            logger.warning(f"  - {err}")
        if len(generation_errors) > 5:
            logger.warning(f"  ... –∏ –µ—â—ë {len(generation_errors) - 5} –æ—à–∏–±–æ–∫")

    print("\n=== –ò—Ç–æ–≥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===")
    print(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º: {available_chunks_total}")
    print(f"–£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø–∞—Ä: {len(dataset)}")
    if generation_errors:
        print(f"–û—à–∏–±–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {len(generation_errors)}")
    print(f"–§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞: {output_path}")


def export_for_annotation(
    input_dataset: str,
    output_path: str,
    include_annotations: bool = False,
):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ä—É—á–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏.

    –°–æ–∑–¥–∞–µ—Ç CSV/JSONL —Ñ–∞–π–ª, —É–¥–æ–±–Ω—ã–π –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–æ–º.

    Args:
        input_dataset: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        include_annotations: –í–∫–ª—é—á–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏–∑ –ë–î
    """
    dataset = _load_existing_dataset(input_dataset)
    if not dataset:
        logger.error("–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", input_dataset)
        return

    export_data = []
    for item in dataset:
        export_item = {
            "id": item.get("id", ""),
            "question": item.get("question", ""),
            "ground_truth_answer": item.get("ground_truth_answer", ""),
            "source": item.get("source", "unknown"),
            "question_source": item.get("question_source", "unknown"),
            "question_answer_id": item.get("question_answer_id"),
            "chunk_id": item.get("chunk_id"),
            "chunk_text": item.get("chunk_text", ""),
            "confluence_url": item.get("confluence_url"),
            "relevant_chunk_ids": item.get("relevant_chunk_ids", []),
            "relevant_urls": item.get("relevant_urls", []),
            "user_score": item.get("user_score"),
            "is_relevant_chunk_matched": item.get("is_relevant_chunk_matched"),
            "is_question_ok": 1,
            "is_answer_ok": 1,
            "is_chunk_ok": 1,
            "notes": "",
        }
        export_data.append(export_item)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    if output_path.endswith(".csv"):
        import csv

        with open(output_path, "w", newline="", encoding="utf-8") as f:
            if export_data:
                writer = csv.DictWriter(
                    f, fieldnames=list(export_data[0].keys()), extrasaction="ignore"
                )
                writer.writeheader()
                writer.writerows(export_data)
    else:
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)

    logger.info(f"–≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {output_path}")
    print(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(export_data)} –∑–∞–ø–∏—Å–µ–π –≤ {output_path}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI —Å–∫—Ä–∏–ø—Ç–∞."""
    parser = argparse.ArgumentParser(
        description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è RAG-–±–µ–Ω—á–º–∞—Ä–∫–æ–≤"
    )

    parser.add_argument(
        "--mode",
        type=str,
        choices=[
            "synthetic",
            "export-annotation",
        ],
        default="synthetic",
        help="–†–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: synthetic (–∏–∑ —á–∞–Ω–∫–æ–≤), export-annotation (—ç–∫—Å–ø–æ—Ä—Ç –¥–ª—è —Ä—É—á–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏)",
    )

    parser.add_argument(
        "--max-questions",
        type=int,
        default=500,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç (default: 500)",
    )

    parser.add_argument(
        "--num-samples",
        type=int,
        default=None,
        help="–£—Å—Ç–∞—Ä–µ–≤—à–∏–π —Ñ–ª–∞–≥ (–∞–ª–∏–∞—Å –¥–ª—è --max-questions)",
    )

    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é versioned dataset_YYYYMMDD_HHMMSS.json)",
    )

    parser.add_argument(
        "--check-only",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç",
    )

    parser.add_argument(
        "--skip-existing-dataset",
        type=str,
        default=None,
        help=(
            "–ü—É—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, "
            "—á–∞–Ω–∫–∏ –∏–∑ –Ω–µ–≥–æ –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã"
        ),
    )

    parser.add_argument(
        "--generation-attempts",
        type=int,
        default=3,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞",
    )

    args = parser.parse_args()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)

    if args.mode == "export-annotation":
        input_dataset = args.output or "benchmarks/data/dataset_latest.json"
        export_path = f"benchmarks/data/annotation_{timestamp}.json"
        export_for_annotation(input_dataset, export_path)
        return

    if args.check_only:
        output_path = args.output or f"benchmarks/data/dataset_{timestamp}.json"
        if os.path.exists(output_path):
            with open(output_path, "r", encoding="utf-8") as f:
                dataset = json.load(f)
            print(f"–î–∞—Ç–∞—Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {output_path}")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(dataset)}")
            if dataset:
                print(f"\n–ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
                print(json.dumps(dataset[0], ensure_ascii=False, indent=2))
        else:
            print(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {output_path}")
        return

    mode_prefix = {
        "synthetic": "synthetic",
    }.get(args.mode, "synthetic")

    output_path = (
        args.output or f"benchmarks/data/dataset_{mode_prefix}_{timestamp}.json"
    )

    max_questions = args.max_questions
    if args.num_samples is not None:
        max_questions = args.num_samples
        logger.warning(
            "–§–ª–∞–≥ --num-samples —É—Å—Ç–∞—Ä–µ–ª, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --max-questions. "
            "–ò—Å–ø–æ–ª—å–∑—É–µ–º max_questions=%s",
            max_questions,
        )

    if args.mode == "synthetic":
        generate_synthetic_dataset(
            engine,
            max_questions=max_questions,
            output_path=output_path,
            skip_existing_dataset=args.skip_existing_dataset,
            generation_attempts=max(1, args.generation_attempts),
        )


if __name__ == "__main__":
    main()
