"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–æ–ª–æ—Ç–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM-—Å—É–¥—å—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∏–¥–µ–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞–Ω–∫–æ–≤.
"""

import argparse
import json
import logging
import os
import random
import sys
from datetime import datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from qa.config import Config
from qa.database import Chunk, create_engine
from benchmarks.utils.llm_judge import LLMJudge

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

logger = logging.getLogger(__name__)


def generate_synthetic_dataset(
    engine, num_samples: int, output_path: str, skip_existing: bool = True
):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç.

    Args:
        engine: –î–≤–∏–∂–æ–∫ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        skip_existing: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å —á–∞–Ω–∫–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —É–∂–µ –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã
    """
    from sqlalchemy import select
    from sqlalchemy.orm import Session

    judge = LLMJudge()

    with Session(engine) as session:
        all_chunks = session.scalars(select(Chunk)).all()
        logger.info(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –≤ –ë–î: {len(all_chunks)}")

        chunks_with_embeddings = [
            c for c in all_chunks if c.embedding is not None and len(c.embedding) > 0
        ]
        logger.info(f"–ß–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {len(chunks_with_embeddings)}")

        if len(chunks_with_embeddings) < num_samples:
            logger.warning(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: "
                f"—Ç—Ä–µ–±—É–µ—Ç—Å—è {num_samples}, –¥–æ—Å—Ç—É–ø–Ω–æ {len(chunks_with_embeddings)}"
            )
            num_samples = len(chunks_with_embeddings)

        random.seed(42)
        selected_chunks = random.sample(chunks_with_embeddings, num_samples)

        dataset = []

        for i, chunk in enumerate(selected_chunks, 1):
            try:
                result = judge.generate_question_from_chunk(chunk.text)

                dataset_item = {
                    "chunk_id": chunk.id,
                    "chunk_text": chunk.text[:500],
                    "question": result["question"],
                    "ground_truth_answer": result["ground_truth_answer"],
                    "confluence_url": chunk.confluence_url,
                }

                dataset.append(dataset_item)

                logger.info(f"[{i}/{num_samples}] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {result['question']}")

                if i % 10 == 0:
                    logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{num_samples}")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –¥–ª—è —á–∞–Ω–∫–∞ {chunk.id}: {e}")
                continue

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)

    logger.info(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")
    logger.info(f"üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(dataset)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI —Å–∫—Ä–∏–ø—Ç–∞."""
    parser = argparse.ArgumentParser(
        description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤"
    )

    parser.add_argument(
        "--num-samples",
        type=int,
        default=50,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (default: 50)",
    )

    parser.add_argument(
        "--output",
        type=str,
        default="benchmarks/data/golden_dataset_synthetic.json",
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞",
    )

    parser.add_argument(
        "--check-only",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç",
    )

    args = parser.parse_args()

    engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)

    if args.check_only:
        if os.path.exists(args.output):
            with open(args.output, "r", encoding="utf-8") as f:
                dataset = json.load(f)
            print(f"–î–∞—Ç–∞—Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {args.output}")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(dataset)}")
            if dataset:
                print(f"\n–ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
                print(json.dumps(dataset[0], ensure_ascii=False, indent=2))
        else:
            print(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.output}")
        return

    generate_synthetic_dataset(engine, args.num_samples, args.output)


if __name__ == "__main__":
    main()
