# Научное описание метрик бенчмарков RAG-системы

Документ представляет собой исчерпывающее академическое описание всех метрик, используемых в системе бенчмарков RAG-системы «Вопрошалыч». Для каждой метрики приводятся теоретическое обоснование, формальные определения, вычислительные пайплайны и анализ достоверности.

---

## 1. Введение и теоретические основания

### 1.1 Цель бенчмаркинга

Оценка качества систем Retrieval-Augmented Generation (RAG) представляет собой многоаспектную задачу, требующую комплексного подхода к измерению производительности на различных этапах информационно-поискового конвейера. Система бенчмарков «Вопрошалыч» реализует иерархическую структуру оценки, охватывающую:

1. **Качество векторных представлений** (Tier 0) — фундаментальный уровень, определяющий способность модели захватывать семантические отношения между текстами
2. **Эффективность поиска** (Tier 1) — оценка релевантности извлекаемых документов
3. **Качество генерации** (Tier 2) — измерение способности модели формировать достоверные и релевантные ответы при идеальном контексте
4. **Сквозное качество** (Tier 3) — комплексная оценка всего пайплайна в условиях, приближенных к реальной эксплуатации

### 1.2 Классификация метрик

Метрики системы классифицируются по нескольким измерениям:

**По уровню абстракции:**
- *Алгоритмические метрики* — формальные математические функции, вычисляемые без привлечения внешних моделей (HitRate, MRR, NDCG, ROUGE, BLEU)
- *Метрики на основе LLM* — оценки, получаемые от языковых моделей в роли «судьи» (Faithfulness, Answer Relevance, E2E Score)
- *Гибридные метрики* — комбинированные показатели, использующие несколько источников информации (Semantic Similarity)

**По целевому аспекту качества:**
- *Метрики релевантности* — измеряют соответствие между запросом и результатами поиска
- *Метрики качества текста* — оценивают характеристики сгенерированного текста
- *Метрики согласованности* — измеряют стабильность и воспроизводимость результатов
- *Метрики пользовательского опыта* — оценивают аспекты, влияющие на восприятие системы конечным пользователем

### 1.3 Библиотеки и инструменты (фактически в коде)

Ниже перечислены библиотеки, которые реально используются в текущей реализации
вычисления метрик и аналитики:

| Библиотека | Где используется | Назначение |
|-----------|------------------|------------|
| `pgvector` + `SQLAlchemy` | Tier 1, Tier 3, Real-users, utilization/topic scripts | Векторный поиск через `cosine_distance` в PostgreSQL |
| `sentence-transformers` | Tier 0/1/3/UX, real-users, source analytics | Эмбеддинги для вопросов/ответов/чанков |
| `scikit-learn` | Tier 0, topic coverage | NearestNeighbors, pairwise distances, KMeans |
| `rouge-score` | Tier 2/3 | ROUGE-метрики |
| `sacrebleu` | Tier 2/3 | BLEU-метрика |
| `openai` (client) | Tier 2/3/Judge | Вызовы LLM-судьи |
| `plotly`, `pandas`, `umap-learn` | Dashboard, vector space, analytics | Визуализация и аналитические представления |

Дополнительно:
- `Ragas` в текущей реализации **не используется**.
- `BERTScore` поддержан в util-модуле как опциональный код, но отключён в
  основном benchmark pipeline.

### 1.3 Библиотеки и инструменты, используемые в коде

Ниже перечислены библиотеки, которые **фактически используются** в текущей
реализации для вычисления метрик, retrieval-оценки и аналитики.

| Библиотека | Статус | Где используется | Назначение |
|------------|--------|------------------|------------|
| `numpy` | используется | Tier 0/1/2/3/Judge/UX | Агрегации, векторная алгебра, статистики |
| `scikit-learn` | используется | Tier 0, Topic Coverage | NearestNeighbors, pairwise distances, KMeans |
| `sqlalchemy` + `pgvector` | используется | Tier 1/3, Real-users, util/topic | Retrieval через `cosine_distance` в PostgreSQL |
| `sentence-transformers` | используется | Все tier, util/topic | Генерация эмбеддингов вопросов и ответов |
| `openai` | используется | `LLMJudge` | API-клиент для judge-моделей |
| `requests` | используется | Tier Judge Pipeline, `qa.main` | Вызовы production Mistral API |
| `rouge-score` | используется | Tier 2/3 | Метрики ROUGE-1/2/L |
| `sacrebleu` | используется | Tier 2/3 | Метрика BLEU |
| `pandas` | используется | Dashboard, UMAP скрипт | Табличная аналитика и подготовка данных |
| `plotly` | используется | Dashboard, UMAP скрипт | Графики и интерактивные визуализации |
| `umap-learn` | используется | Vector Space анализ | Снижение размерности для проекций 2D/3D |

Дополнительно:

- `ragas` — **не используется** в текущем коде.
- `bert-score` — частично подготовлен в `text_metrics.py`, но в текущем
  benchmark-пайплайне отключён (`include_bertscore=False`).

---

## 2. Уровни оценки (Tiers)

### 2.1 Сводная таблица уровней

| Tier | Название | Уровень абстракции | Основные метрики |
|------|----------|-------------------|------------------|
| 0 | Embedding Quality (Intrinsic) | Инфраструктурный | NN Distance, Spread, Pairwise Distance |
| 1 | Retrieval | Поисковый | HitRate, MRR, NDCG |
| 2 | Generation | Генеративный | Faithfulness, Answer Relevance, ROUGE, BLEU |
| 3 | End-to-End | Системный | E2E Score, Semantic Similarity |
| Judge | LLM Judge Quality | Мета-оценочный | Consistency Score, Error Rate |
| Judge Pipeline | Production Judge | Операционный | Accuracy, Precision, Recall, F1 |
| UX | User Experience | Пользовательский | Cache Hit Rate, Context Preservation |

### 2.2 Какие компоненты системы оцениваются

Термины «поиск» и «генерация» в рамках бенчмарков декомпозируются на
конкретные подсистемы:

| Уровень | Какие компоненты покрывает | Что не покрывает напрямую |
|--------|------------------------------|---------------------------|
| Tier 0 | Геометрия эмбеддингов и векторного пространства | Скорость БД, ранжирование по URL, UX |
| Tier 1 | Retrieval в PostgreSQL+pgvector, ранжирование top-k | Генерацию ответа, judge-фильтрацию |
| Tier 2 | Генерация при фиксированном контексте + judge-оценка текста | Качество retrieval и индексной структуры |
| Tier 3 | Полный E2E (retrieval + generation + оценка ответа) | Долгосрочные пользовательские эффекты |
| Tier Judge | Стабильность и надёжность benchmark judge | Качество production judge решения show/no-show |
| Tier Judge Pipeline | Классификация production judge (show/no-show) | Качество текста генерации как таковое |
| Tier UX | Повторяемость ответа, кэш и сохранение контекста | Тонкая фактическая correctness каждого ответа |
| Real-users | Поведение retrieval на живом трафике | Полный gold generation benchmark |

---

## 3. Tier 0: Качество эмбеддингов (Intrinsic)

### 3.0 Декомпозиция оцениваемых компонентов

Tier 0 оценивает именно **векторные представления и геометрию пространства**,
без проверки SQL-поиска и без проверки генерации.

Компоненты, которые покрывает Tier 0:

- **Embedding model quality**: насколько эмбеддер формирует полезную структуру
  близостей/дистанций между текстами.
- **Vector space geometry**: плотность, разброс, эффективная размерность,
  наличие выбросов и «схлопывания» пространства.
- **Data health сигнал**: косвенный контроль деградации датасета (дубликаты,
  шум, неравномерность покрытия тем).

Компоненты, которые Tier 0 не покрывает:

- эффективность индекса в БД;
- качество ранжирования retrieval;
- качество генерации ответа.

### 3.1 Теоретическое обоснование

Tier 0 оценивает геометрию векторного пространства **без требований к разметке**
(`cluster_id`, `label`). Это делает анализ устойчивым для synthetic/manual/realq
датасетов, где явные кластеры могут отсутствовать.

### 3.2 Почему обновлён подход

Ранее Tier 0 использовал silhouette и intra/inter cluster метрики, которые требуют
явных меток кластеров. Для большинства текущих датасетов такие поля отсутствуют,
поэтому метрики были статистически невалидны. Новый Tier 0 использует intrinsic
метрики, вычисляемые напрямую по эмбеддингам.

### 3.3 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `total_samples` | int | Количество векторов в анализе |
| `avg_nn_distance` | float | Среднее расстояние до k ближайших соседей |
| `std_nn_distance` | float | Стандартное отклонение расстояний до соседей |
| `density_score` | float | Плотность пространства (`1 / avg_nn_distance`) |
| `avg_spread` | float | Среднее расстояние до центроида |
| `max_spread` | float | Максимальное расстояние до центроида |
| `spread_std` | float | Разброс расстояний до центроида |
| `effective_dimensionality` | int | Число компонент для 95% дисперсии |
| `avg_pairwise_distance` | float | Среднее попарное расстояние |
| `std_pairwise_distance` | float | Разброс попарных расстояний |
| `min_pairwise_distance` | float | Минимальное попарное расстояние |
| `max_pairwise_distance` | float | Максимальное попарное расстояние |

### 3.4 Формулы

Для матрицы эмбеддингов $E \in \mathbb{R}^{N \times D}$:

- Ближайшие соседи:

$$
\text{avg\_nn\_distance} = \frac{1}{N \cdot k}\sum_{i=1}^{N}\sum_{j=1}^{k} d_{i,j}
$$

$$
\text{density\_score} = \frac{1}{\text{avg\_nn\_distance} + \varepsilon}
$$

- Разброс относительно центроида $\mu = \frac{1}{N}\sum_i e_i$:

$$
\text{avg\_spread} = \frac{1}{N}\sum_{i=1}^{N} ||e_i - \mu||_2
$$

- Эффективная размерность:

$$
\text{effective\_dimensionality} = \min\left\{m :
\frac{\sum_{i=1}^{m}\lambda_i}{\sum_{i=1}^{D}\lambda_i} \ge 0.95\right\}
$$

где $\lambda_i$ — собственные значения ковариационной матрицы.

- Попарные расстояния:

$$
\text{avg\_pairwise\_distance} =
\frac{2}{N(N-1)}\sum_{1 \le i < j \le N} d(e_i, e_j)
$$

### 3.5 Интерпретация

- `avg_nn_distance` ниже -> локальная компактность выше.
- `std_nn_distance` выше -> неоднородная плотность (возможные «дыры» в пространстве).
- `density_score` выше -> пространство плотнее.
- `effective_dimensionality` слишком низкая -> возможная деградация/схлопывание.
- `max_pairwise_distance` и `max_spread` помогают выявлять выбросы.

### 3.6 Визуальная валидация

Для ручной диагностики структуры пространства используйте UMAP-визуализацию:

- `python benchmarks/visualize_vector_space.py --limit 5000`
- `python benchmarks/visualize_vector_space.py --limit 3000 --3d`

Эта визуализация не является метрикой, но помогает интерпретировать Tier 0.

---

## 4. Tier 1: Точность поиска (Retrieval Accuracy)

### 4.0 Декомпозиция оцениваемых компонентов

Tier 1 оценивает не абстрактный «поиск вообще», а конкретную цепочку retrieval:

- **Query embedding**: преобразование пользовательского вопроса в вектор.
- **Vector distance function**: сравнение по `cosine_distance`.
- **DB retrieval execution**: SQL-запрос к PostgreSQL + pgvector, возврат top-k.
- **Ranking quality**: место релевантного документа в выдаче.

Что важно: Tier 1 в текущем виде не изолирует отдельно вклад каждого звена.
Метрики отражают их совместный итог.

Компоненты вне покрытия Tier 1:

- генерация ответа (LLM);
- пост-обработка и judge-решение «показывать/не показывать».

### 4.1 Теоретическое обоснование

Tier 1 оценивает **эффективность информационно-поисковой системы** на уровне извлечения релевантных документов. Это критический компонент RAG, поскольку качество конечного ответа напрямую зависит от релевантности контекста, переданного генеративной модели.

В контексте RAG-системы поиск выполняет функцию **селекции контекста** — из всей базы знаний необходимо извлечь наиболее релевантные фрагменты для формирования ответа. Ошибки на этом этапе носят катастрофический характер: нерелевантный контекст приводит к генерации недостоверной информации (галлюцинациям).

### 4.2 Почему мы используем эти метрики

Совокупность метрик HitRate, MRR и NDCG образует стандарт де-факто в академических и промышленных системах оценки информационного поиска (TREC, BEIR, MTEB). Каждая метрика захватывает различные аспекты качества поиска:

- **HitRate** — практическая метрика, отражающая вероятность нахождения хотя бы одного релевантного документа
- **MRR** — учитывает ранг первого релевантного результата, что критично для пользовательского опыта
- **NDCG** — наиболее информативная метрика, учитывающая как факт релевантности, так и её степень (через позицию)

### 4.3 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `hit_rate@1` | float | Hit Rate при top-1 (0-1) |
| `hit_rate@5` | float | Hit Rate при top-5 (0-1) |
| `hit_rate@10` | float | Hit Rate при top-10 (0-1) |
| `mrr` | float | Mean Reciprocal Rank (0-1) |
| `recall@K` | float | Recall@K для K=1,3,5,10 |
| `precision@K` | float | Precision@K для K=1,3,5,10 |
| `ndcg@5` | float | NDCG@5 (0-1) |
| `ndcg@10` | float | NDCG@10 (0-1) |
| `retrieval_consistency` | float | Стабильность top-k выдачи при повторном одинаковом запросе |

### 4.4 Детальное описание метрик

#### 4.4.1 Hit Rate@K

**Определение:** Доля запросов, для которых хотя бы один релевантный документ присутствует среди первых $K$ результатов поиска.

$$ \text{HitRate}@K = \frac{1}{|Q|} \sum_{q \in Q} \mathbb{1}\left[\exists d \in \text{top}_K(q) : \text{rel}(q, d) = 1\right] $$

где $Q$ — множество запросов, $\text{top}_K(q)$ — первые $K$ результатов для запроса $q$, $\mathbb{1}[\cdot]$ — индикаторная функция, $\text{rel}(q, d)$ — бинарная релевантность.

**Вычислительный пайплайн:**
1. Для каждого запроса $q \in Q$ выполняется поиск с возвратом top-$K$ результатов
2. Проверяется наличие релевантного документа в полученном списке
3. Считается доля запросов с хотя бы одним релевантным результатом

**Интерпретация:**
- HitRate@1 = 1.0: идеальный поиск, первый результат всегда релевантен
- HitRate@10 = 0.5: в 50% случаев релевантный документ присутствует в top-10
- Для RAG-систем критически важно значение HitRate@1

**Доверие к метрике:** Высокое. Метрика интуитивно понятна и широко используется на практике. Ограничение: не учитывает количество релевантных документов и их позиции.

---

#### 4.4.2 Mean Reciprocal Rank (MRR)

**Определение:** Среднее значение обратного ранга первого релевантного документа.

$$ \text{MRR} = \frac{1}{|Q|} \sum_{q \in Q} \frac{1}{\text{rank}_q} $$

где $\text{rank}_q$ — позиция первого релевантного документа в результатах поиска для запроса $q$. Если релевантный документ отсутствует, $\frac{1}{\text{rank}_q} = 0$.

**Вычислительный пайплайн:**
1. Для каждого запроса $q$ выполняется поиск
2. Находится позиция первого релевантного документа ($\text{rank}$)
3. Вычисляется $1/\text{rank}$ (0 если релевантных документов нет)
4. Результаты усредняются по всем запросам

**Интерпретация:**
- MRR = 1.0: первый результат всегда релевантен
- MRR = 0.5: в среднем первый релевантный результат на позиции 2
- MRR = 0.1: первый релевантный результат в среднем на позиции 10

**Доверие к метрике:** Высокое. MRR является стандартной метрикой в информационном поиске (voorhees@trec). Особенно релевантен для систем с малым количеством отображаемых результатов.

---

#### 4.4.3 Recall@K и Precision@K

**Определение:**

$$ \text{Recall}@K = \frac{|\text{rel}_{\text{top}_K}|}{|\text{rel}_{\text{all}}|} $$

$$ \text{Precision}@K = \frac{|\text{rel}_{\text{top}_K}|}{K} $$

где $|\text{rel}_{\text{top}_K}|$ — количество релевантных документов в top-$K$, $|\text{rel}_{\text{all}}|$ — общее количество релевантных документов в коллекции.

**Вычислительный пайплайн:**
1. Для каждого запроса $q$ выполняется поиск и определяется множество релевантных документов
2. Вычисляется $|\text{rel}_{\text{top}_K}|$ — количество релевантных документов в top-$K$
3. Вычисляется Recall@K и Precision@K
4. Результаты усредняются по запросам

**Интерпретация:**
- Recall@K = 1.0: все релевантные документы найдены (при $K \geq |\text{rel}_{\text{all}}|$)
- Precision@K = 1.0: все $K$ результатов релевантны
- Trade-off: при фиксированном $K$ увеличение Recall приводит к снижению Precision

**Доверие к метрике:** Высокое. Recall и Precision — фундаментальные метрики информационного поиска, введённые в работе Кливленда (Cleverdon, 1960). Позволяют оценить полноту и точность поиска раздельно.

---

#### 4.4.4 Normalized Discounted Cumulative Gain (NDCG@K)

**Определение:** Нормализованный дисконтированный кумулятивный выигрыш — метрика, учитывающая как факт релевантности, так и позицию релевантного документа.

$$ \text{NDCG}@K = \frac{\text{DCG}@K}{\text{IDCG}@K} $$

**Discounted Cumulative Gain (DCG):**

$$ \text{DCG}@K = \sum_{i=1}^{K} \frac{\text{rel}_i}{\log_2(i+1)} $$

где $\text{rel}_i$ — релевантность документа на позиции $i$ (в бинарном случае: 1 или 0).

**Ideal DCG (IDCG):** DCG при идеальном упорядочивании (релевантные документы расположены первыми):

$$ \text{IDCG}@K = \sum_{i=1}^{\min(K, |\text{rel}_{\text{all}}|)} \frac{1}{\log_2(i+1)} $$

**Вычислительный пайпайн:**
1. Для каждого запроса $q$ документы упорядочиваются по убыванию предсказанной релевантности
2. Вычисляется DCG@K с учётом дисконтирования по позиции
3. Вычисляется IDCG@K — DCG при идеальном порядке
4. NDCG@K = DCG@K / IDCG@K
5. Результаты усредняются

**Интерпретация:**
- NDCG@K = 1.0: идеальное упорядочивание, все релевантные документы на первых позициях
- NDCG@K = 0.0: нет релевантных документов в top-$K$
- NDCG@K > 0.9: отличное качество поиска
- NDCG@K = 0.5–0.9: приемлемое качество
- NDCG@K < 0.5: требуется улучшение

**Теоретическое обоснование:** NDCG был введён Jarvelin и Kekalainen (2000) для преодоления ограничений бинарных метрик. Дисконтирование отражает тот факт, что пользователи менее склонны просматривать результаты на низких позициях. Нормализация позволяет корректно сравнивать запросы с разным количеством релевантных документов.

**Доверие к метрике:** Очень высокое. NDCG является стандартом в современных бенчмарках (TREC, MTEB, BEIR). Метрика теоретически обоснована и эмпирически валидирована.

---

## 5. Tier 2: Качество генерации

### 5.0 Декомпозиция оцениваемых компонентов

Tier 2 изолирует **генерацию при заданном контексте**. Это означает:

- retrieval как источник ошибок минимизирован (контекст берётся из датасета);
- оценивается, как LLM интерпретирует контекст и формирует ответ;
- LLM-судья оценивает ответ по критериям faithfulness/relevance.

Компоненты, которые покрывает Tier 2:

- **Prompt + generation model behavior** на фиксированном контексте;
- **Grounded response quality** (нет ли фактов вне контекста);
- **Text realization quality** (ROUGE/BLEU как поверхностный сигнал).

Компоненты вне покрытия Tier 2:

- качество retrieval и ранжирования;
- стабильность полного production-пайплайна.

### 5.1 Теоретическое обоснование

Tier 2 оценивает **способность генеративной модели формировать качественные ответы** при условии, что контекст (релевантные документы) уже извлечён. Это позволяет изолировать проблемы генерации от проблем поиска и провести диагностику каждого компонента независимо.

В данном tier используется парадигма **идеального контекста** — модели предоставляется вся релевантная информация из датасета, что позволяет оценить её способность корректно эту информацию использовать и трансформировать в ответ на естественном языке.

### 5.2 Почему мы используем эти метрики

Качество генерации текста является многомерным понятием, которое невозможно адекватно оценить единственной метрикой. Система использует три категории оценок:

1. **LLM-as-a-Judge** — современный подход, использующий способность больших языковых моделей к пониманию и оценке текста
2. **Алгоритмические метрики (ROUGE, BLEU)** — классические статистические методы, основанные на сравнении n-грамм
3. Комбинированный подход обеспечивает как глубину оценки (LLM), так и воспроизводимость (алгоритмические метрики)

### 5.3 Метрики LLM-судьи

| Метрика | Тип | Описание |
|---------|-----|----------|
| `avg_faithfulness` | float | Средняя оценка фактичности (0-1) |
| `avg_answer_relevance` | float | Средняя оценка релевантности ответа (0-1) |
| `avg_answer_correctness` | float | Средняя оценка корректности относительно эталона (1-5) |
| `response_exact_match_rate` | float | Доля дословно совпадающих ответов при повторном запуске |
| `response_semantic_consistency` | float | Средняя семантическая согласованность ответов при повторах |

### 5.4 Метрики текстового сравнения

| Метрика | Тип | Описание |
|---------|-----|----------|
| `avg_rouge1_f` | float | ROUGE-1 F-measure (0-1) |
| `avg_rouge2_f` | float | ROUGE-2 F-measure (0-1) |
| `avg_rougeL_f` | float | ROUGE-L F-measure (0-1) |
| `avg_bleu` | float | BLEU score (0-100) |

### 5.5 Детальное описание метрик

#### 5.5.1 Faithfulness (фактичность)

**Определение:** Оценка степени согласованности сгенерированного ответа с предоставленным контекстом. Ответ считается фактически достоверным, если вся информация в нём содержится в контексте и не является выдуманной (галлюцинацией).

**Промпт LLM-судьи (упрощённый):**
```
Оцени фактичность ответа на основе контекста.
Контекст: [контекст из базы знаний]
Вопрос: [вопрос пользователя]
Ответ: [сгенерированный системой ответ]

Оцени по шкале от 0 до 1, где:
1.0 - ответ полностью согласуется с контекстом, нет галлюцинаций
0.5 - ответ частично согласуется с контекстом
0.0 - ответ содержит галлюцинации, противоречит контексту
```

**Вычислительный пайплайн:**
1. Для каждого вопроса из датасета извлекается контекст (chunk_text)
2. Модель генерации (Mistral) создаёт ответ на основе контекста
3. LLM-судья (Qwen) оценивает фактичность ответа по шкале 0–1
4. Оценки усредняются по всем вопросам

**Интерпретация:**
- Faithfulness = 1.0: все ответы полностью достоверны
- Faithfulness > 0.8: высокое качество, минимальные галлюцинации
- Faithfulness < 0.5: серьёзные проблемы с достоверностью

**Теоретическое обоснование:** Faithfulness напрямую измеряет критически важную характеристику RAG-систем — склонность к галлюцинациям. Исследования показывают (Zhang et al., 2023), что даже современные LLM склонны к генерации недостоверной информации, и измерение этого аспекта необходимо для обеспечения надёжности системы.

**Доверие к метрике:** Среднее-высокое. LLM-as-a-Judge демонстрирует высокую корреляцию с человеческими оценками (Zheng et al., 2023). Ограничения: возможная систематическая предвзятость конкретной модели-судьи.

---

#### 5.5.2 Answer Relevance (релевантность ответа)

**Определение:** Оценка степени соответствия сгенерированного ответа заданному вопросу. Ответ релевантен, если он адресует именно тот вопрос, который был задан.

**Промпт LLM-судьи (упрощённый):**
```
Оцени релевантность ответа на вопрос.
Вопрос: [вопрос пользователя]
Ответ: [сгенерированный системой ответ]

Оцени по шкале от 0 до 1, где:
1.0 - ответ полностью отвечает на вопрос
0.5 - ответ частично отвечает на вопрос
0.0 - ответ не отвечает на вопрос
```

**Вычислительный пайплайн:**
1. Для каждого вопроса генерируется ответ
2. LLM-судья оценивает релевантность
3. Оценки усредняются

**Интерпретация:**
- Answer Relevance = 1.0: все ответы точно соответствуют вопросам
- Answer Relevance > 0.8: высокое качество понимания запросов

**Доверие к метрике:** Среднее-высокое. Аналогично Faithfulness.

---

#### 5.5.3 ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

**Определение:** Семейство метрик, основанных на подсчёте перекрытия n-грамм между сгенерированным текстом (гипотезой) и эталонным текстом (референсом).

$$ \text{ROUGE-}N = \frac{\sum_{g \in \text{Grams}_N(hypothesis)} \min(\text{count}_H(g), \text{count}_R(g))}{\sum_{g \in \text{Grams}_N(reference)} \text{count}_R(g)} $$

где $N$ — размер n-граммы (1 для униграмм, 2 для биграмм), $\text{Grams}_N(\cdot)$ — множество n-грамм текста, $\text{count}_H(g)$ — частота n-граммы $g$ в гипотезе, $\text{count}_R(g)$ — частота в референсе.

**ROUGE-L (на основе LCS — Longest Common Subsequence):**

$$ \text{ROUGE-L} = \frac{(1+\beta^2) \cdot \text{Recall}_L \cdot \text{Precision}_L}{\text{Recall}_L + \beta^2 \cdot \text{Precision}_L} $$

$$ \text{Recall}_L = \frac{\text{LCS}(H, R)}{|R|}, \quad \text{Precision}_L = \frac{\text{LCS}(H, R)}{|H|} $$

**F-measure:**

$$ F = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} $$

**Вычислительный пайплайн:**
1. Токенизация гипотезы и референса
2. Извлечение n-грамм (для ROUGE-1: униграммы, ROUGE-2: биграммы, ROUGE-L: LCS)
3. Подсчёт перекрытия с учётом ограничений
4. Вычисление Precision, Recall и F-measure

**Интерпретация:**
- ROUGE-L-F > 0.5: хорошее качество
- ROUGE-1-F > 0.4: приемлемое качество для коротких текстов
- Обратить внимание: высокий ROUGE не гарантирует семантическую правильность

**Теоретическое обоснование:** ROUGE был разработан в National Institute of Standards and Technology (NIST) для автоматической оценки резюме текстов (Lin, 2004). Метрика стала стандартом в области машинного перевода и суммаризации.

**Доверие к метрике:** Среднее. ROUGE широко используется, но имеет известные ограничения:
- Не учитывает семантику, только лексическое перекрытие
- Чувствителен к поверхностным вариациям
- Не всегда коррелирует с человеческими оценками

---

#### 5.5.4 BLEU (Bilingual Evaluation Understudy)

**Определение:** Метрика, первоначально разработанная для оценки машинного перевода, основанная на точности n-грамм с штрафом за краткость.

$$ \text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right) $$

где $p_n$ — точность n-грамм, $w_n$ — вес (обычно $1/N$), BP — brevity penalty (штраф за краткость).

**Brevity Penalty:**

$$ \text{BP} = \begin{cases} 1 & \text{если } c > r \\ \exp(1 - r/c) & \text{если } c \leq r \end{cases} $$

где $c$ — длина гипотезы, $r$ — длина референса.

**Precision n-грамм:**

$$ p_n = \frac{\sum_{g \in \text{Grams}_n(H)} \min(\text{count}_H(g), \text{count}_R(g))}{\sum_{g \in \text{Grams}_n(H)} \text{count}_H(g)} $$

**Вычислительный пайплайн:**
1. Токенизация (обычно на уровне слов или subword)
2. Подсчёт n-грамм для каждого $n$ от 1 до 4
3. Вычисление точности для каждого $n$
4. Применение brevity penalty
5. Вычисление геометрического среднего

**Интерпретация:**
- BLEU = 100: идеальное совпадение (практически недостижимо)
- BLEU > 50: очень хорошее качество машинного перевода
- BLEU 20–40: приемлемое качество
- BLEU < 20: требуется существенное улучшение

**Теоретическое обоснование:** BLEU был представлен в работе Papineni et al. (2002) и стал де-факто стандартом для оценки машинного перевода на протяжении более чем десятилетия.

**Доверие к метрике:** Среднее-низкое для генерации сводных текстов. BLEU имеет существенные ограничения:
- Не учитывает смысл
- Зависит от совпадения на уровне поверхностных форм
- Penalizeет валидные перефразирования
- Особенно ограничен для открытых задач генерации

---

## 6. Tier 3: Сквозное качество (End-to-End)

### 6.0 Декомпозиция оцениваемых компонентов

Tier 3 покрывает **полный пользовательский путь** от вопроса до финального
ответа, включая взаимодействие между подсистемами.

Компоненты, которые покрывает Tier 3:

- **Retrieval path**: query embedding + pgvector поиск top-1;
- **Context-to-answer generation** через production generation model;
- **End-user visible answer quality** (E2E judge + similarity/text metrics);
- **Cross-component coupling**: как ошибки retrieval усиливаются в generation.

Компоненты вне покрытия Tier 3:

- операционные SLA (пропускная способность/очереди/нагрузка);
- бизнес-метрики продукта (удержание, удовлетворённость по сессиям).

### 6.1 Теоретическое обоснование

Tier 3 оценивает **полный RAG-пайплайн** в условиях, максимально приближенных к реальной эксплуатации. В отличие от Tier 2, где используется идеальный контекст из датасета, Tier 3 выполняет реальный поиск через production-код (`qa.confluence_retrieving.get_chunk()`), что позволяет оценить систему в целом.

Этот tier является наиболее релевантным для оценки пользовательского опыта, поскольку именно он отражает реальную производительность системы.

### 6.2 Почему мы используем эти метрики

Сквозная оценка позволяет выявить проблемы, которые не обнаруживаются при изолированном тестировании компонентов:
- Взаимодействие между поиском и генерацией
- Влияние качества поиска на качество генерации
- Согласованность production-кода с ожиданиями

### 6.3 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `avg_e2e_score` | float | Средняя E2E оценка качества (0-1) |
| `avg_semantic_similarity` | float | Среднее семантическое сходство (cosine, -1 to 1) |
| `avg_dot_similarity` | float | Средняя dot-product схожесть ответов |
| `avg_euclidean_distance` | float | Средняя евклидова дистанция между ответом и эталоном |
| `response_exact_match_rate` | float | Повторяемость ответа при идентичном запросе |
| `response_semantic_consistency` | float | Семантическая согласованность E2E ответов |
| `avg_rouge1_f` | float | ROUGE-1 F-measure (0-1) |
| `avg_rouge2_f` | float | ROUGE-2 F-measure (0-1) |
| `avg_rougeL_f` | float | ROUGE-L F-measure (0-1) |
| `avg_bleu` | float | BLEU score (0-100) |

### 6.4 Детальное описание метрик

#### 6.4.1 E2E Score

**Определение:** Комплексная оценка качества ответа, учитывающая вопрос, сгенерированный ответ и эталонный ответ.

**Промпт LLM-судьи (упрощённый):**
```
Оцени общую полезность ответа на основе вопроса и эталонного ответа.
Вопрос: [вопрос пользователя]
Эталонный ответ: [ground_truth_answer]
Сгенерированный ответ: [system_answer]

Оцени по шкале от 0 до 1, где:
1.0 - сгенерированный ответ столь же полезен, как эталонный
0.5 - сгенерированный ответ частично полезен
0.0 - сгенерированный ответ не полезен
```

**Вычислительный пайплайн:**
1. Для каждого вопроса выполняется retrieval (top-1 через production код)
2. Генерируется ответ через Mistral
3. LLM-судья оценивает E2E качество
4. Оценки усредняются

**Интерпретация:** Аналогично Faithfulness и Answer Relevance, но в контексте реального использования.

---

#### 6.4.2 Semantic Similarity (семантическое сходство)

**Определение:** Косинусное сходство между векторными представлениями сгенерированного ответа и эталонного ответа.

$$ \text{semantic\_similarity} = \cos(\vec{e}_{sys}, \vec{e}_{gt}) = \frac{\vec{e}_{sys} \cdot \vec{e}_{gt}}{|\vec{e}_{sys}| \cdot |\vec{e}_{gt}|} $$

где $\vec{e}_{sys}$ — эмбеддинг сгенерированного ответа, $\vec{e}_{gt}$ — эмбеддинг эталонного ответа.

**Вычислительный пайплайн:**
1. Генерируется ответ системы
2. Оба текста (системный и эталонный) преобразуются в эмбеддинги через SentenceTransformer
3. Вычисляется косинусное сходство
4. Результаты усредняются

#### 4.4.5 Retrieval Consistency

**Определение:** Доля повторных retrieval-вызовов, в которых список top-k
идентичен первому вызову для того же вопроса.

$$
\text{retrieval\_consistency} = \frac{1}{|Q|} \sum_{q \in Q}
\frac{1}{R-1}\sum_{r=2}^{R} \mathbb{1}[topK_{q,r} = topK_{q,1}]
$$

где $R$ — число повторов (`consistency_runs`).

**Вычислительный пайплайн:**
1. Для каждого вопроса выполняется baseline retrieval.
2. Выполняются повторные retrieval-вызовы с тем же embedding.
3. Сравниваются списки chunk_id в top-k.
4. Считается средняя доля совпадений.

**Интерпретация:**
- $\to 1.0$: семантически идентичные ответы
- $\to 0.0$: ортогональные ответы
- $\to -1.0$: противоположные по смыслу

**Теоретическое обоснование:** Semantic Similarity преодолевает ограничение лексических метрик (ROUGE, BLEU), поскольку работает на уровне семантики, а не поверхностных форм. Использование предобученных эмбеддингов позволяет уловить синонимию и перефразирование.

**Доверие к метрике:** Высокое для оценки семантической близости. Ограничения:
- Зависит от качества модели эмбеддингов
- Не различает частичную релевантность

---

## 7. Real-Users Benchmark

### 7.0 Декомпозиция оцениваемых компонентов

Real-Users Benchmark оценивает retrieval на реальных вопросах и отражает:

- **domain realism**: реальные формулировки, сленг, неоднозначность;
- **production retrieval behavior** на живых запросах;
- **robustness к данным из эксплуатации**, а не только к synthetic данным.

Ограничение компонента:

- ground truth строится через proxy (`confluence_url`), поэтому это не
  эквивалент «золотой» ручной разметки по всем документам.

### 7.1 Теоретическое обоснование

Real-Users Benchmark использует **реальные данные эксплуатации** — вопросы пользователей из продуктивной системы. Это обеспечивает максимальную релевантность оценок, поскольку тестирование проводится на реальных, а не синтетических данных.

Ключевая особенность: в качестве ground truth используется `QuestionAnswer.confluence_url` — URL документа, который был показан пользователю в реальном диалоге. Это позволяет оценить, насколько хорошо система воспроизводит поведение production-системы.

### 7.2 Метрики

Аналогичны Tier 1: HitRate, MRR, NDCG, Recall, Precision.

### 7.3 Ground Truth

**Определение:** `QuestionAnswer.confluence_url` из таблицы `question_answer` базы данных.

**Обоснование:** URL представляет собой прокси-метку релевантности — в production-системе этот документ был признан достаточно релевантным для показа пользователю.

**Ограничения:**
- Не все вопросы имеют URL (возможны NULL значения)
- URL может быть неполным или неактуальным
- Не учитывает полноту ответа

### 7.4 Роль Real-users в общей системе

Real-users следует рассматривать в двух ролях:
1. **Оценка retrieval на живых данных** (операционная диагностика).
2. **Источник продуктовой и доменной аналитики** (topic coverage,
   chunk utilization, поведенческие паттерны пользователей).

Для строгой оценки генерации (Tier 2/3) предпочтительны synthetic/manual датасеты
с более контролируемым ground truth.

---

## 8. Tier Judge: Качество LLM-судьи

### 8.0 Декомпозиция оцениваемых компонентов

Tier Judge оценивает не RAG-ответ как таковой, а **инструмент оценки**:

- **repeatability**: повторяемость оценки на одинаковом входе;
- **judge reliability**: устойчивость к ошибкам API/парсинга;
- **operational viability**: latency судьи для массовых прогонов.

Это мета-уровень контроля качества, который влияет на доверие ко всем
LLM-основанным метрикам в Tier 2/3.

### 8.1 Теоретическое обоснование

Tier Judge оценивает **качество и надёжность самого инструмента оценки** — LLM-судьи. Поскольку значительная часть метрик системы основана на LLM-as-a-Judge, критически важно убедиться в консистентности и точности судьи.

### 8.2 Почему мы используем эти метрики

LLM, выступающие в роли судьи, могут проявлять:
- **Неконсистентность** — разные оценки при повторном вызове на том же примере
- **Предвзятость** — систематическое предпочтение определённых типов ответов
- **Низкую точность** — неспособность корректно оценить качество

Измерение этих аспектов необходимо для обеспечения достоверности оценок.

Текущая реализация поддерживает два режима judge-оценивания:

- `direct` — прямой ответ модели-судьи;
- `reasoned` — CoT-style режим с внутренним пошаговым рассуждением и
  строгим форматом короткого результата.

Pairwise-сравнение (`A/B/Tie`) реализовано как отдельный экспериментальный
инструмент в `LLMJudge`, но не используется в базовых tier-метриках по
умолчанию.

### 8.3 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `consistency_score` | float | Согласованность при повторной оценке (0-1) |
| `error_rate` | float | Доля ошибок (0-1) |
| `avg_latency_ms` | float | Среднее время оценки (мс) |
| `avg_faithfulness` | float | Средняя оценка фактичности (0-1) |

### 8.4 Детальное описание метрик

#### 8.4.1 Consistency Score

**Определение:** Доля случаев, когда повторная оценка того же примера даёт согласованный результат (разница ≤ 0.5).

$$ \text{Consistency} = \frac{1}{|Q|} \sum_{q \in Q} \mathbb{1}\left[|s_1(q) - s_2(q)| \leq \delta\right] $$

где $s_1(q)$ — первая оценка, $s_2(q)$ — повторная оценка, $\delta = 0.5$ — порог согласованности.

**Вычислительный пайплайн:**
1. Для каждого примера выполняется первичная оценка
2. Выполняется повторная оценка того же примера
3. Вычисляется согласованность
4. Результаты усредняются

**Интерпретация:**
- Consistency = 1.0: полная согласованность
- Consistency > 0.9: высокая надёжность судьи
- Consistency < 0.7: судья нестабилен

**Доверие к метрике:** Высокое. Прямо измеряет критическую характеристику — воспроизводимость оценок.

---

## 9. Tier Judge Pipeline: Качество Production Judge

### 9.0 Декомпозиция оцениваемых компонентов

Tier Judge Pipeline оценивает production-задачу бинарного решения
«показывать ответ/не показывать»:

- **binary decision correctness** относительно размеченного ground truth;
- **risk profile** через Precision/Recall/F1 (ложные показы и ложные блокировки);
- **latency suitability** для использования в продовой цепочке.

В отличие от Tier Judge, тут фокус не на консистентности судьи по шкале, а на
классификационной пригодности для production-решения.

### 9.1 Теоретическое обоснование

Tier Judge Pipeline оценивает **production-компонент** — судью Mistral, который используется в реальном RAG-пайплайне для решения «показывать ответ пользователю или нет». Это критически важная функция, определяющая будет ли предоставлен ответ или система признает невозможность ответа.

### 9.2 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `accuracy` | float | Общая точность (0-1) |
| `precision` | float | Precision для класса "Yes" (0-1) |
| `recall` | float | Recall для класса "Yes" (0-1) |
| `f1_score` | float | F1-score (0-1) |
| `avg_latency_ms` | float | Среднее время оценки (мс) |

### 9.3 Детальное описание метрик

#### 9.3.1 Accuracy

**Определение:** Доля правильных решений судьи.

$$ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} $$

где:
- $TP$ (True Positive): судья предсказал "Yes", ground truth = "Yes"
- $TN$ (True Negative): судья предсказал "No", ground truth = "No"  
- $FP$ (False Positive): судья предсказал "Yes", ground truth = "No"
- $FN$ (False Negative): судья предсказал "No", ground truth = "Yes"

**Вычислительный пайплайн:**
1. Для каждого примера вызывается production judge с промптом
2. Ответ парсится (извлекается "Yes"/"No")
3. Сравнивается с ground_truth_show
4. Вычисляются TP, TN, FP, FN
5. Вычисляется Accuracy

**Интерпретация:**
- Accuracy > 0.95: отличное качество
- Accuracy 0.85–0.95: приемлемое качество
- Accuracy < 0.85: требует улучшения

---

#### 9.3.2 Precision, Recall, F1

**Определения:**

$$ \text{Precision} = \frac{TP}{TP + FP} $$

$$ \text{Recall} = \frac{TP}{TP + FN} $$

$$ F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} $$

**Интерпретация:**

- **Precision**: доля правильных "Yes" среди всех предсказанных "Yes"
- **Recall**: доля найденных "Yes" среди всех истинных "Yes"
- **F1**: гармоническое среднее Precision и Recall

**Trade-off:** В контексте RAG-системы обычно более важно высокое Recall (не пропустить показ ответа), но высокое Precision также критично (не показывать некачественные ответы). F1 балансирует эти требования.

---

## 10. Tier UX: Пользовательский опыт

### 10.0 Декомпозиция оцениваемых компонентов

Tier UX покрывает «мягкие» аспекты качества взаимодействия, которые
определяют субъективное восприятие системы:

- **cache reuse potential**: насколько часто можно переиспользовать ответы;
- **dialog continuity**: сохраняется ли тематическая связность в серии вопросов;
- **multi-turn coherence proxy**: согласованность последовательных шагов диалога.

Tier UX не заменяет Tier 1-3, а дополняет их продуктовой перспективой.

### 10.1 Теоретическое обоснование

Tier UX оценивает **аспекты, влияющие на восприятие системы конечным пользователем**, но не связанные напрямую с качеством ответов. Эти метрики важны для понимания общей эффективности системы с точки зрения пользователя.

### 10.2 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `cache_hit_rate` | float | Попадания в кэш похожих вопросов (0-1) |
| `context_preservation` | float | Сохранение контекста в многотуровых диалогах (0-1) |
| `multi_turn_consistency` | float | Согласованность между вопросами в диалоге (0-1) |

### 10.3 Детальное описание метрик

#### 10.3.1 Cache Hit Rate

**Определение:** Доля вопросов, для которых обнаружен семантически схожий вопрос в кэше (similarity > 0.85).

$$ \text{CacheHitRate} = \frac{1}{|Q|} \sum_{q \in Q} \mathbb{1}\left[\max_{c \in \text{Cache}} \cos(\vec{e}_q, \vec{e}_c) > \tau\right] $$

где $\tau = 0.85$ — порог кэширования.

**Вычислительный пайплайн:**
1. Загружаются исторические вопросы с высокой пользовательской оценкой
   (`QuestionAnswer.score == 5`) и их эмбеддинги.
2. Для каждого вопроса из тестовой сессии строится query-эмбеддинг.
3. Вычисляется максимальное косинусное сходство с вопросами «кэша».
4. Если максимум выше порога $\tau = 0.85$, фиксируется cache-hit.
5. Итоговая метрика — доля hit среди всех проверенных вопросов.

**Что это измеряет на практике:**
- потенциал экономии latency и токенов за счёт переиспользования ответов;
- повторяемость пользовательских паттернов в домене;
- возможность строить FAQ/response-cache без потери релевантности.

**Интерпретация:**
- CacheHitRate > 0.5: высокий потенциал для кэширования
- CacheHitRate < 0.1: система не использует преимущества кэша

**Вычислительный пайплайн:**
1. Из БД извлекаются вопросы с высокой пользовательской оценкой (`score=5`).
2. Для каждого вопроса тестовой сессии строится эмбеддинг.
3. Выполняется перебор кэша и ищется максимум косинусного сходства.
4. Если максимум выше порога (`0.85`), фиксируется cache hit.
5. Финальная метрика = hits / total.

---

#### 10.3.2 Context Preservation

**Определение:** Среднее косинусное сходство между последовательными вопросами в многотуровом диалоге.

$$ \text{ContextPreservation} = \frac{1}{|S|-1} \sum_{i=1}^{|S|-1} \cos(\vec{e}_{q_i}, \vec{e}_{q_{i+1}}) $$

где $S = [q_1, q_2, \ldots, q_n]$ — последовательность вопросов в сессии.

**Вычислительный пайплайн:**
1. Для каждой сессии берётся последовательность вопросов
   $q_1, q_2, ..., q_n$.
2. Каждый вопрос кодируется в эмбеддинг одной и той же моделью.
3. Для каждой пары соседних вопросов $(q_i, q_{i+1})$ считается
   косинусное сходство.
4. Сходства усредняются сначала внутри сессии, затем по всем сессиям.

**Что это измеряет на практике:**
- степень тематической связности многотурового общения;
- сохранение фокуса диалога (а не случайные «прыжки» между темами);
- потенциальную потребность в усилении dialog memory.

**Интерпретация:**
- ContextPreservation > 0.7: вопросы связаны, контекст сохраняется
- ContextPreservation < 0.3: вопросы не связаны

**Вычислительный пайплайн:**
1. Каждая диалоговая сессия разбивается на последовательность вопросов.
2. Для соседних вопросов вычисляются эмбеддинги.
3. Считается попарное косинусное сходство между соседними ходами.
4. Значения усредняются по сессии, затем по всем сессиям.
5. То же значение используется как `multi_turn_consistency` в текущей версии.

#### 10.3.3 Multi-turn Consistency

**Определение:** Прокси-метрика согласованности ответов/намерений в сессии.

В текущей реализации вычисляется через ту же процедуру, что и
`context_preservation`, и служит operational индикатором стабильности диалога.

**Вычислительный пайплайн:**
1. Берётся последовательность вопросов в каждой сессии.
2. Вычисляются косинусные сходства для соседних ходов.
3. Усреднение по сессиям.
4. Значение логируется отдельно как UX-метрика консистентности.

---

## 11. Рекомендации по использованию метрик

### 11.1 Выбор метрик в зависимости от цели

| Цель | Рекомендуемые метрики |
|------|----------------------|
| Диагностика поиска | HitRate@1, MRR, NDCG@10 |
| Диагностика генерации | Faithfulness, Answer Relevance |
| Сквозная оценка | E2E Score, Semantic Similarity |
| Оценка production judge | Accuracy, Precision, Recall, F1 |
| Оценка LLM-судьи | Consistency Score |
| Оптимизация кэша | Cache Hit Rate |

### 11.2 Пороговые значения

| Метрика | Отлично | Хорошо | Требует внимания |
|---------|---------|--------|------------------|
| HitRate@1 | > 0.8 | 0.5–0.8 | < 0.5 |
| MRR | > 0.8 | 0.5–0.8 | < 0.5 |
| NDCG@10 | > 0.8 | 0.5–0.8 | < 0.5 |
| Faithfulness | > 0.9 | 0.7–0.9 | < 0.7 |
| E2E Score | > 0.8 | 0.6–0.8 | < 0.6 |
| Accuracy (Judge Pipeline) | > 0.95 | 0.85–0.95 | < 0.85 |
| Consistency | > 0.95 | 0.85–0.95 | < 0.85 |

---

## 12. Дополнительные аналитические метрики источников

### 12.1 Chunk Utilization

Метрика показывает, какая доля всех чанков реально участвует в retrieval при
прогоне набора вопросов.

$$
\text{utilization\_rate} = \frac{|C_{used}|}{|C_{all}|}
$$

Где:
- $C_{all}$ — множество всех чанков с эмбеддингами;
- $C_{used}$ — множество чанков, попавших в top-k хотя бы один раз.

Интерпретация:
- низкое значение -> возможно много «мусорных» или недоступных для поиска данных;
- очень высокое значение при низком качестве retrieval -> возможный шум в индексе.

Ключевые поля: `total_chunks`, `used_chunks`, `unused_chunks`, `utilization_rate`.

### 12.2 Topic Coverage

Метрика анализирует покрытие тематических кластеров вопросов:

1. вопросы кластеризуются (KMeans) в `n_topics`;
2. для каждого вопроса темы выполняется retrieval top-k;
3. считается число уникальных чанков и URL на тему.

Итоговые поля:
- `n_topics`, `total_questions`, `avg_chunks_per_topic`;
- `topic_coverage[]` с `topic_id`, `question_count`, `unique_chunks`, `unique_urls`.

Интерпретация:
- темы с низким `unique_chunks` при большом `question_count` указывают на
  слабое покрытие знаний;
- перекос по `unique_urls` сигнализирует о переиспользовании узкого набора источников.

### 12.3 Визуализация векторного пространства

UMAP-визуализация (`2D/3D`) используется как диагностический инструмент:
- помогает обнаруживать выбросы, локальные кластеры и пустые зоны;
- дополняет Tier 0 intrinsic-метрики, но не заменяет их.

Скрипт: `benchmarks/visualize_vector_space.py`.

---

## 13. Заключение

Представленная система метрик обеспечивает комплексную оценку всех аспектов RAG-системы — от качества векторных представлений до пользовательского опыта. Комбинация алгоритмических метрик, LLM-as-a-Judge и специализированных метрик позволяет проводить многоуровневую диагностику и выявлять узкие места системы на различных уровнях абстракции.

Ключевые принципы системы:
1. **Иерархичность** — изоляция компонентов для точной диагностики
2. **Комплементарность** — использование различных подходов к оценке
3. **Воспроизводимость** — формальные метрики обеспечивают повторяемость
4. **Практическая релевантность** — метрики соотносятся с пользовательским опытом

---

## Ссылки

- Cleverdon, C. W. (1960). The Cranfield tests on index language devices. *Aslib Proceedings*.
- Jarvelin, K., & Kekalainen, J. (2000). IR evaluation methods for retrieving highly relevant documents. *SIGIR*.
- Lin, C. Y. (2004). ROUGE: A package for automatic evaluation of summaries. *ACL*.
- Papineni, K., et al. (2002). BLEU: a method for automatic evaluation of machine translation. *ACL*.
- Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. *J. Comput. Appl. Math.*
- Zhang, Y., et al. (2023). A survey on hallucination in large language models. *arXiv*.
- Zheng, L., et al. (2023). Judging LLM-as-a-judge with MT-bench and Chatbot Arena. *NeurIPS*.
