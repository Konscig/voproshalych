"""–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—à–±–æ—Ä–¥ –¥–ª—è –º–µ—Ç—Ä–∏–∫ RAG-–±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""

from __future__ import annotations

from datetime import datetime
import json
import logging
import os
from pathlib import Path
from typing import Dict, List, Optional

try:
    import gradio as gr
    import pandas as pd

    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False

from sqlalchemy import select
from sqlalchemy.orm import Session

from qa.config import Config

logger = logging.getLogger(__name__)

APP_TITLE = "RAG Quality Assurance System"

METRICS_BY_TIER = {
    "tier_0": [
        "avg_nn_distance",
        "std_nn_distance",
        "density_score",
        "avg_spread",
        "max_spread",
        "spread_std",
        "effective_dimensionality",
        "avg_pairwise_distance",
        "std_pairwise_distance",
        "min_pairwise_distance",
        "max_pairwise_distance",
    ],
    "tier_1": ["mrr", "hit_rate@1", "hit_rate@5", "hit_rate@10"],
    "tier_2": [
        "avg_faithfulness",
        "avg_answer_relevance",
        "avg_rouge1_f",
        "avg_rougeL_f",
        "avg_bleu",
    ],
    "tier_3": [
        "avg_e2e_score",
        "avg_semantic_similarity",
        "avg_rouge1_f",
        "avg_bleu",
    ],
    "tier_judge": [
        "consistency_score",
        "error_rate",
        "avg_latency_ms",
        "avg_faithfulness",
    ],
    "tier_judge_pipeline": [
        "accuracy",
        "precision",
        "recall",
        "f1_score",
        "avg_latency_ms",
    ],
    "tier_ux": ["cache_hit_rate", "context_preservation", "multi_turn_consistency"],
    "tier_real_users": [
        "mrr",
        "recall@1",
        "recall@5",
        "precision@1",
        "precision@5",
        "ndcg@5",
    ],
    "utilization_metrics": [
        "total_chunks",
        "used_chunks",
        "unused_chunks",
        "utilization_rate",
        "question_count",
        "top_k",
    ],
    "topic_coverage_metrics": [
        "n_topics",
        "total_questions",
        "avg_chunks_per_topic",
        "top_k",
    ],
}

QUALITY_BASELINES = {
    "avg_nn_distance": 0.30,
    "density_score": 3.00,
    "avg_pairwise_distance": 0.45,
    "mrr": 0.8,
    "hit_rate@1": 0.7,
    "hit_rate@5": 0.9,
    "hit_rate@10": 0.95,
    "recall@1": 0.7,
    "recall@5": 0.9,
    "recall@10": 0.95,
    "precision@1": 0.7,
    "precision@5": 0.18,
    "ndcg@5": 0.8,
    "avg_faithfulness": 4.5,
    "avg_answer_relevance": 4.2,
    "avg_e2e_score": 4.2,
    "avg_semantic_similarity": 0.85,
    "avg_rouge1_f": 0.50,
    "avg_rouge2_f": 0.30,
    "avg_rougeL_f": 0.45,
    "avg_bleu": 30.0,
    "consistency_score": 0.90,
    "error_rate": 0.05,
    "avg_latency_ms": 3000.0,
    "accuracy": 0.85,
    "precision": 0.85,
    "recall": 0.85,
    "f1_score": 0.85,
    "cache_hit_rate": 0.40,
    "context_preservation": 0.70,
    "multi_turn_consistency": 0.70,
}


def _safe_float(value, default: float = 0.0) -> float:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –∫ float."""
    try:
        return float(value)
    except (TypeError, ValueError):
        return default


class RAGBenchmarkDashboard:
    """–î–∞—à–±–æ—Ä–¥ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–∞ Retrieval –∏ Generation."""

    def __init__(self):
        self.reports_dir = Path("benchmarks/reports")
        self.runs = self._load_runs()

    def _load_runs(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∑–∞–ø—É—Å–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞."""
        benchmark_runs_path = self.reports_dir / "benchmark_runs.json"

        if not benchmark_runs_path.exists():
            logger.info("–§–∞–π–ª benchmark_runs.json –Ω–µ –Ω–∞–π–¥–µ–Ω, –∑–∞–≥—Ä—É–∑–∫–∞ 0 –∑–∞–ø—É—Å–∫–æ–≤")
            return []

        with open(benchmark_runs_path, "r", encoding="utf-8") as f:
            records = json.load(f)

        runs = []
        for record in records:
            timestamp_str = record.get("timestamp", "")[:15]
            runs.append(
                {
                    "id": record.get("id"),
                    "timestamp": timestamp_str,
                    "timestamp_readable": timestamp_str.replace("T", " ")[:19],
                    "git_branch": record.get("git_branch") or "unknown",
                    "git_commit_hash": record.get("git_commit_hash") or "unknown",
                    "run_author": record.get("run_author") or "unknown",
                    "dataset_file": record.get("dataset_file") or "unknown",
                    "dataset_type": record.get("dataset_type") or "synthetic",
                    "judge_model": record.get("judge_model") or "unknown",
                    "generation_model": record.get("generation_model") or "unknown",
                    "embedding_model": record.get("embedding_model") or "unknown",
                    "overall_status": record.get("overall_status"),
                    "tier_0": record.get("tier_0_metrics") or {},
                    "tier_1": record.get("tier_1_metrics") or {},
                    "tier_2": record.get("tier_2_metrics") or {},
                    "tier_3": record.get("tier_3_metrics") or {},
                    "tier_judge": record.get("tier_judge_metrics") or {},
                    "tier_judge_pipeline": record.get("tier_judge_pipeline_metrics")
                    or {},
                    "tier_ux": record.get("tier_ux_metrics") or {},
                    "tier_real_users": record.get("real_user_metrics") or {},
                    "utilization_metrics": record.get("utilization_metrics") or {},
                    "topic_coverage_metrics": record.get("topic_coverage_metrics")
                    or {},
                }
            )

        logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω–æ %s –∑–∞–ø—É—Å–∫–æ–≤ –∏–∑ benchmark_runs.json", len(runs))
        return runs

    def get_latest_run(self) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
        if not self.runs:
            return None
        return self.runs[-1]

    def get_metric_history(
        self, tier: str, metric: str
    ) -> tuple[List[str], List[float]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º –∑–∞–ø—É—Å–∫–∞–º."""
        dates: List[str] = []
        values: List[float] = []

        for run in self.runs:
            tier_metrics = run.get(tier, {})
            value = tier_metrics.get(metric)
            if value is None:
                continue
            try:
                values.append(float(value))
                dates.append(run["timestamp"])
            except (TypeError, ValueError):
                continue

        return dates, values

    def _metric_options_for_tier(self, tier: str) -> List[str]:
        return METRICS_BY_TIER.get(tier, [])

    def _build_series_rows(
        self,
        dates: List[str],
        values: List[float],
        metric: str,
        series_name: str,
    ) -> List[Dict[str, str | float]]:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ –¥–ª—è gr.LinePlot —Å baseline-–ª–∏–Ω–∏–µ–π."""
        rows: List[Dict[str, str | float]] = []

        rendered_dates = []
        for d in dates:
            if "_" in d:
                date_part, time_part = d.split("_", 1)
                rendered_dates.append(f"{date_part}-{time_part[:2]}")
            else:
                rendered_dates.append(d[:8])

        rendered_values = values[:]
        if len(rendered_dates) == 1:
            rendered_dates.append(f"{rendered_dates[0]}_p")
            rendered_values.append(rendered_values[0])

        for date, value in zip(rendered_dates, rendered_values):
            rows.append({"timestamp": date, "value": value, "series": series_name})

        baseline = QUALITY_BASELINES.get(metric)
        if baseline is not None:
            for date in rendered_dates:
                rows.append(
                    {"timestamp": date, "value": baseline, "series": "Baseline"}
                )

        return rows

    def create_interface(self):
        """–°–æ–∑–¥–∞—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio."""
        if not GRADIO_AVAILABLE:
            raise ImportError(
                "Gradio –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ: uv sync --extra dashboard"
            )

        with gr.Blocks(title=APP_TITLE) as demo:
            gr.Markdown(f"# {APP_TITLE}")
            gr.Markdown(
                "–ü–∞–Ω–µ–ª—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ Retrieval, Generation –∏ end-to-end "
                "–æ—Ç–≤–µ—Ç–æ–≤ RAG-—Å–∏—Å—Ç–µ–º—ã."
            )

            with gr.Accordion("System Info", open=False):
                gr.Markdown(
                    "\n".join(
                        [
                            f"- Judge model: `{os.getenv('BENCHMARKS_JUDGE_MODEL') or os.getenv('JUDGE_MODEL') or Config.JUDGE_MODEL or 'unknown'}`",
                            f"- Generation model: `{os.getenv('GENERATION_MODEL') or Config.MISTRAL_MODEL or 'unknown'}`",
                            f"- Embedding model: `{Config.EMBEDDING_MODEL_PATH}`",
                        ]
                    )
                )

            with gr.Tab("Run Details"):
                self._create_run_details_tab()

            with gr.Tab("Runs Registry"):
                self._create_all_runs_tab()

            with gr.Tab("Metric History"):
                self._create_history_tab()

            with gr.Tab("Tier Comparison"):
                self._create_comparison_tab()

            with gr.Tab("Run Dataset"):
                self._create_run_dataset_tab()

            with gr.Tab("Vector Space"):
                self._create_vector_space_tab()

            with gr.Tab("Chunk Utilization"):
                self._create_chunk_utilization_tab()

            with gr.Tab("Topic Coverage"):
                self._create_topic_coverage_tab()

            with gr.Tab("–°–ø—Ä–∞–≤–∫–∞"):
                self._create_reference_tab()

        return demo

    def _create_run_details_tab(self):
        if not self.runs:
            gr.Markdown(
                "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–ø—É—Å–∫–∞—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø–∏—Å–µ–π."
            )
            return

        ordered_runs = list(reversed(self.runs))

        def format_run_choice(run: Dict) -> str:
            return (
                f"{run['timestamp_readable']} | {run.get('dataset_type', 'synthetic')} | "
                f"{run['git_commit_hash'][:7]} | {run.get('dataset_file', 'N/A')}"
            )

        run_choices = [format_run_choice(r) for r in ordered_runs]

        gr.Markdown("### –í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–ø—É—Å–∫ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –º–µ—Ç—Ä–∏–∫")

        def get_run_metrics(selected: str) -> tuple:
            run = next(
                (r for r in ordered_runs if format_run_choice(r) == selected),
                None,
            )
            if not run:
                return "–ó–∞–ø—É—Å–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω", []

            info = "\n".join(
                [
                    f"### –ó–∞–ø—É—Å–∫: {run['timestamp_readable']}",
                    f"- **Branch:** `{run['git_branch']}`",
                    f"- **Commit:** `{run['git_commit_hash']}`",
                    f"- **Author:** `{run['run_author']}`",
                    f"- **Dataset:** `{run.get('dataset_file', 'N/A')}`",
                    f"- **Dataset type:** `{run.get('dataset_type', 'synthetic')}`",
                    f"- **Judge model:** `{run.get('judge_model', 'N/A')}`",
                    f"- **Generation model:** `{run.get('generation_model', 'N/A')}`",
                    f"- **Embedding model:** `{run.get('embedding_model', 'N/A')}`",
                    f"- **Overall status:** `{run['overall_status']}`",
                ]
            )

            rows = []

            tier_labels = {
                "tier_0": "Tier 0 (Embedding)",
                "tier_1": "Tier 1 (Retrieval)",
                "tier_2": "Tier 2 (Generation)",
                "tier_3": "Tier 3 (End-to-End)",
                "tier_judge": "Tier Judge (Qwen)",
                "tier_judge_pipeline": "Tier Judge Pipeline (Mistral)",
                "tier_ux": "Tier UX",
                "tier_real_users": "Real Users (Retrieval)",
                "utilization_metrics": "Chunk Utilization",
                "topic_coverage_metrics": "Topic Coverage",
            }

            for tier_key, tier_label in tier_labels.items():
                tier_metrics = run.get(tier_key, {})
                if not isinstance(tier_metrics, dict) or not tier_metrics:
                    continue
                for metric_name, metric_value in sorted(tier_metrics.items()):
                    if isinstance(metric_value, (float, int)):
                        rendered_value = round(float(metric_value), 4)
                    else:
                        rendered_value = json.dumps(
                            metric_value,
                            ensure_ascii=False,
                        )
                    rows.append(
                        [
                            tier_label,
                            metric_name,
                            rendered_value,
                        ]
                    )

            return info, rows

        initial_info, initial_rows = get_run_metrics(run_choices[0])

        run_selector = gr.Dropdown(
            choices=run_choices,
            value=run_choices[0],
            label="–í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–ø—É—Å–∫",
        )
        run_info = gr.Markdown(value=initial_info)
        metrics_table = gr.Dataframe(
            value=initial_rows,
            headers=["Tier", "Metric", "Value"],
            interactive=False,
            wrap=True,
        )

        def update_run(selected: str):
            info, rows = get_run_metrics(selected)
            return info, rows

        run_selector.change(
            fn=update_run,
            inputs=[run_selector],
            outputs=[run_info, metrics_table],
        )

        def load_markdown_report(selected: str) -> str:
            run = next(
                (r for r in ordered_runs if format_run_choice(r) == selected),
                None,
            )
            if not run:
                return "–ó–∞–ø—É—Å–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω"

            run_id = run.get("id")
            dataset_file = run.get("dataset_file", "")
            base_name = dataset_file.replace(".json", "") if dataset_file else ""
            possible_paths = [
                os.path.join("benchmarks/reports", f"rag_benchmark_{run_id}.md"),
                os.path.join("benchmarks/reports", f"{base_name}.md"),
                os.path.join("benchmarks/reports", f"dataset_{run_id}.md"),
            ]

            for path in possible_paths:
                if os.path.exists(path):
                    try:
                        with open(path, "r", encoding="utf-8") as f:
                            return f.read()
                    except Exception as e:
                        return f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}"

            return f"Markdown –æ—Ç—á—ë—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–∫–∞–ª–∏: {possible_paths}"

        gr.Markdown("---")
        gr.Markdown("### üìÑ Markdown Report")

        initial_markdown = load_markdown_report(run_choices[0])

        markdown_display = gr.Markdown(value=initial_markdown)

        def update_markdown(selected: str):
            return load_markdown_report(selected)

        run_selector.change(
            fn=update_markdown,
            inputs=[run_selector],
            outputs=[markdown_display],
        )

    def _create_history_tab(self):
        gr.Markdown("### Historical trend for selected metric")

        all_tiers = list(METRICS_BY_TIER.keys())

        default_tier = "tier_1"
        default_metric = "mrr"
        if not self.get_metric_history(default_tier, default_metric)[0]:
            for candidate_tier in all_tiers:
                for candidate_metric in self._metric_options_for_tier(candidate_tier):
                    if self.get_metric_history(candidate_tier, candidate_metric)[0]:
                        default_tier = candidate_tier
                        default_metric = candidate_metric
                        break
                if default_metric != "mrr" or default_tier != "tier_1":
                    break

        with gr.Row():
            tier_dropdown = gr.Dropdown(
                choices=all_tiers,
                value=default_tier,
                label="Tier",
            )
            metric_dropdown = gr.Dropdown(
                choices=self._metric_options_for_tier(default_tier),
                value=default_metric,
                label="Metric",
            )

        initial_history = pd.DataFrame(
            self._build_series_rows(
                *self.get_metric_history(default_tier, default_metric),
                default_metric,
                default_metric,
            )
        )

        plot = gr.LinePlot(
            value=initial_history,
            x="timestamp",
            y="value",
            color="series",
            title="Metric vs Baseline",
            x_title="Run",
            y_title="Value",
            tooltip="all",
            height=420,
            color_map={"Baseline": "#808080"},
            sort="x",
        )

        def update_metric_choices(tier: str):
            options = self._metric_options_for_tier(tier)
            value = options[0] if options else None
            return gr.Dropdown(choices=options, value=value)

        def update_plot(tier: str, metric: str):
            if not metric:
                return pd.DataFrame(columns=["timestamp", "value", "series"])
            dates, values = self.get_metric_history(tier, metric)
            if not dates:
                return pd.DataFrame(columns=["timestamp", "value", "series"])
            rows = self._build_series_rows(dates, values, metric, metric)
            return pd.DataFrame(rows)

        tier_dropdown.change(
            fn=update_metric_choices,
            inputs=[tier_dropdown],
            outputs=[metric_dropdown],
        ).then(
            fn=update_plot,
            inputs=[tier_dropdown, metric_dropdown],
            outputs=[plot],
        )

        metric_dropdown.change(
            fn=update_plot,
            inputs=[tier_dropdown, metric_dropdown],
            outputs=[plot],
        )

    def _create_comparison_tab(self):
        gr.Markdown("### Compare tiers by metric")

        comparison_metrics = sorted(
            {metric for metrics in METRICS_BY_TIER.values() for metric in metrics}
        )

        tier_labels = {
            "tier_0": "Tier 0 (Embedding)",
            "tier_1": "Tier 1 (Retrieval)",
            "tier_2": "Tier 2 (Generation)",
            "tier_3": "Tier 3 (E2E)",
            "tier_judge": "Tier Judge (Qwen)",
            "tier_judge_pipeline": "Tier Judge Pipeline",
            "tier_ux": "Tier UX",
            "tier_real_users": "Real Users",
            "utilization_metrics": "Chunk Utilization",
            "topic_coverage_metrics": "Topic Coverage",
        }

        metric_dropdown = gr.Dropdown(
            choices=comparison_metrics,
            value="mrr",
            label="Metric",
        )

        initial_comparison_rows: List[Dict[str, str | float]] = []
        for tier_name, label in tier_labels.items():
            if tier_name in METRICS_BY_TIER:
                dates, values = self.get_metric_history(tier_name, "mrr")
                if dates:
                    initial_comparison_rows.extend(
                        self._build_series_rows(dates, values, "mrr", label)
                    )

        plot = gr.LinePlot(
            value=pd.DataFrame(initial_comparison_rows),
            x="timestamp",
            y="value",
            color="series",
            title="Tier comparison",
            x_title="Run",
            y_title="Value",
            tooltip="all",
            height=420,
            color_map={"Baseline": "#808080"},
            sort="x",
        )

        def update_comparison_plot(metric: str):
            all_rows: List[Dict[str, str | float]] = []
            series_by_tier = {
                "tier_0": "Tier 0 (Embed)",
                "tier_1": "Tier 1 (Retrieval)",
                "tier_2": "Tier 2 (Generation)",
                "tier_3": "Tier 3 (E2E)",
                "tier_judge": "Tier Judge (Qwen)",
                "tier_judge_pipeline": "Tier Judge Pipeline",
                "tier_ux": "Tier UX",
                "tier_real_users": "Real Users",
                "utilization_metrics": "Chunk Utilization",
                "topic_coverage_metrics": "Topic Coverage",
            }

            longest_dates: List[str] = []
            for tier_name, label in series_by_tier.items():
                if tier_name not in METRICS_BY_TIER:
                    continue
                if metric not in METRICS_BY_TIER[tier_name]:
                    continue
                dates, values = self.get_metric_history(tier_name, metric)
                if not dates:
                    continue
                if len(dates) > len(longest_dates):
                    longest_dates = dates[:]
                all_rows.extend(
                    self._build_series_rows(dates, values, metric, series_name=label)
                )

            if not all_rows:
                return pd.DataFrame(columns=["timestamp", "value", "series"])

            baseline = QUALITY_BASELINES.get(metric)
            if baseline is not None and longest_dates:
                baseline_dates = longest_dates
                if len(baseline_dates) == 1:
                    baseline_dates = [
                        baseline_dates[0],
                        f"{baseline_dates[0]}_point",
                    ]
                all_rows.extend(
                    {
                        "timestamp": date,
                        "value": baseline,
                        "series": "Baseline",
                    }
                    for date in baseline_dates
                )

            deduped = []
            seen = set()
            for row in all_rows:
                key = (row["timestamp"], row["value"], row["series"])
                if key in seen:
                    continue
                seen.add(key)
                deduped.append(row)
            return pd.DataFrame(deduped)

        metric_dropdown.change(
            fn=update_comparison_plot,
            inputs=[metric_dropdown],
            outputs=[plot],
        )

    def _create_all_runs_tab(self):
        mode_filter = gr.Dropdown(
            choices=["all", "synthetic", "manual", "real-users"],
            value="all",
            label="–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –¥–∞—Ç–∞—Å–µ—Ç–∞",
        )

        def build_registry_rows(filter_mode: str):
            rows = []
            for run in reversed(self.runs):
                run_mode = run.get("dataset_type", "synthetic")
                if filter_mode != "all" and run_mode != filter_mode:
                    continue
                t1 = run.get("tier_1", {})
                t2 = run.get("tier_2", {})
                t3 = run.get("tier_3", {})
                tjp = run.get("tier_judge_pipeline", {})
                rows.append(
                    [
                        run["timestamp_readable"][:16],
                        run["git_branch"][:12] if run.get("git_branch") else "N/A",
                        run.get("dataset_type", "synthetic")[:8],
                        round(_safe_float(t1.get("mrr")), 2),
                        round(_safe_float(t1.get("hit_rate@5")), 2),
                        round(_safe_float(t2.get("avg_faithfulness")), 2),
                        round(_safe_float(t2.get("avg_answer_relevance")), 2),
                        round(_safe_float(t3.get("avg_e2e_score")), 2),
                        round(_safe_float(tjp.get("accuracy")), 2) if tjp else "-",
                        run["overall_status"][:8],
                    ]
                )
            return rows

        headers = [
            "Time",
            "Branch",
            "Type",
            "MRR",
            "Hit@5",
            "Faith",
            "Relev",
            "E2E",
            "TJP Acc",
            "Status",
        ]

        initial_rows = build_registry_rows("all")
        table = gr.Dataframe(
            value=initial_rows,
            headers=headers,
            interactive=False,
            wrap=False,
        )

        def update_table(filter_mode: str):
            return gr.Dataframe(value=build_registry_rows(filter_mode), headers=headers)

        mode_filter.change(
            fn=update_table,
            inputs=[mode_filter],
            outputs=[table],
        )

    def _create_run_dataset_tab(self):
        ordered_runs = list(reversed(self.runs))

        if not ordered_runs:
            gr.Markdown("–ù–µ—Ç –∑–∞–ø—É—Å–∫–æ–≤ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞.")
            return

        def format_run_choice(run: Dict) -> str:
            return (
                f"{run['timestamp_readable']} | {run['dataset_type']} | "
                f"{run['git_commit_hash']} | {run['dataset_file']}"
            )

        def get_run_choices(filter_mode: str) -> List[str]:
            if filter_mode == "all":
                return [format_run_choice(run) for run in ordered_runs]
            return [
                format_run_choice(run)
                for run in ordered_runs
                if run.get("dataset_type") == filter_mode
            ]

        def load_dataset_preview(selected: str) -> tuple:
            if not selected:
                return "", pd.DataFrame()

            run = next(
                (item for item in ordered_runs if format_run_choice(item) == selected),
                None,
            )
            if run is None:
                return "–ù–µ –Ω–∞–π–¥–µ–Ω –≤—ã–±—Ä–∞–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫", pd.DataFrame()

            if run.get("dataset_type") == "real-users":
                real_metrics = run.get("tier_real_users", {})
                info = "\n".join(
                    [
                        f"**Dataset:** `{run.get('dataset_file', 'unknown')}`",
                        f"**Mode:** `{run.get('dataset_type', 'real-users')}`",
                        f"**MRR:** `{_safe_float(real_metrics.get('mrr')):.4f}`",
                        f"**Recall@5:** `{_safe_float(real_metrics.get('recall@5')):.4f}`",
                        f"**NDCG@5:** `{_safe_float(real_metrics.get('ndcg@5')):.4f}`",
                    ]
                )
                return info, pd.DataFrame()

            dataset_file = run.get("dataset_file") or ""
            benchmark_dir = Path(__file__).resolve().parent
            project_root = benchmark_dir.parent
            candidate_paths = [
                benchmark_dir / "data" / dataset_file,
                project_root / "benchmarks" / "data" / dataset_file,
                Path.cwd() / "data" / dataset_file,
                Path.cwd() / "benchmarks" / "data" / dataset_file,
            ]
            dataset_path = next(
                (path for path in candidate_paths if path.exists()),
                candidate_paths[0],
            )

            if not dataset_path.exists():
                return (
                    f"**Dataset:** `{dataset_file}`\n\n–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ `{dataset_path}`",
                    pd.DataFrame(),
                )

            try:
                import json

                with open(dataset_path, "r", encoding="utf-8") as f:
                    dataset = json.load(f)

                all_keys = set()
                for item in dataset:
                    if isinstance(item, dict):
                        all_keys.update(item.keys())

                ordered_keys = sorted(all_keys)

                preview_rows = []
                for row in dataset[:30]:
                    if not isinstance(row, dict):
                        continue
                    rendered_row = {}
                    for key in ordered_keys:
                        value = row.get(key)
                        if isinstance(value, (dict, list)):
                            rendered_row[key] = json.dumps(value, ensure_ascii=False)
                        else:
                            rendered_row[key] = value
                    preview_rows.append(rendered_row)

                preview = pd.DataFrame(preview_rows)
                sample_records = json.dumps(dataset[:3], ensure_ascii=False, indent=2)

                meta = (
                    f"**Dataset:** `{dataset_file}`\n"
                    f"\n**Mode:** `{run.get('dataset_type', 'synthetic')}`\n"
                    f"\n**Rows:** {len(dataset)}\n"
                    f"\n**Linked run:** `{run['timestamp_readable']}`\n"
                    f"\n**–ü–æ–ª—è ({len(ordered_keys)}):** `{', '.join(ordered_keys)}`\n"
                    "\n**–ü–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏ (JSON):**\n"
                    f"```json\n{sample_records}\n```"
                )

                return meta, preview
            except Exception as error:
                return (
                    f"**Dataset:** `{dataset_file}`\n\n–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: `{error}`",
                    pd.DataFrame(),
                )

        mode_filter = gr.Dropdown(
            choices=["all", "synthetic", "manual", "real-users"],
            value="all",
            label="–¢–∏–ø –∑–∞–ø—É—Å–∫–∞",
        )

        run_choices = get_run_choices("all")

        initial_meta, initial_preview = load_dataset_preview(run_choices[0])

        run_selector = gr.Dropdown(
            choices=run_choices,
            value=run_choices[0],
            label="–í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–ø—É—Å–∫",
        )
        dataset_meta = gr.Markdown(value=initial_meta)
        dataset_preview = gr.Dataframe(
            value=initial_preview,
            interactive=False,
            wrap=True,
        )

        def update_on_filter_change(filter_mode: str):
            options = get_run_choices(filter_mode)
            if not options:
                return (
                    gr.Dropdown(choices=[], value=None),
                    "",
                    pd.DataFrame(),
                )
            meta, preview = load_dataset_preview(options[0])
            return (
                gr.Dropdown(choices=options, value=options[0]),
                meta,
                preview,
            )

        def update_on_run_select(selected: str):
            meta, preview = load_dataset_preview(selected)
            return meta, preview

        mode_filter.change(
            fn=update_on_filter_change,
            inputs=[mode_filter],
            outputs=[run_selector, dataset_meta, dataset_preview],
        )

        run_selector.change(
            fn=update_on_run_select,
            inputs=[run_selector],
            outputs=[dataset_meta, dataset_preview],
        )

    def _create_vector_space_tab(self):
        gr.Markdown("### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤")

        with gr.Row():
            limit_slider = gr.Slider(
                minimum=100,
                maximum=10000,
                value=3000,
                step=100,
                label="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤",
            )
            dim_radio = gr.Radio(["2D", "3D"], value="2D", label="–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å")
            color_by = gr.Dropdown(
                choices=["section", "cluster", "chunk_id"],
                value="section",
                label="–†–∞—Å–∫—Ä–∞—Å–∫–∞",
            )

        output_html = gr.HTML(label="–í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ")
        visualize_btn = gr.Button("–í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å")

        def run_visualization(limit: int, dimension: str, color: str):
            try:
                from benchmarks.visualize_vector_space import (
                    _load_chunk_embeddings,
                    visualize_embeddings,
                )

                dim = 3 if dimension == "3D" else 2
                output_path = (
                    self.reports_dir
                    / f"vector_space_dashboard_{dim}d_{int(limit)}_{color}.html"
                )
                embeddings, metadata = _load_chunk_embeddings(int(limit))
                html_path = visualize_embeddings(
                    embeddings=embeddings,
                    metadata=metadata,
                    output_path=str(output_path),
                    dim=dim,
                    color_by=color,
                )
                with open(html_path, "r", encoding="utf-8") as file:
                    return file.read()
            except Exception as error:
                return (
                    "<div style='padding:12px;border:1px solid #ddd;'>"
                    f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {error}</div>"
                )

        visualize_btn.click(
            fn=run_visualization,
            inputs=[limit_slider, dim_radio, color_by],
            outputs=[output_html],
        )

    def _create_chunk_utilization_tab(self):
        gr.Markdown("### –ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —á–∞–Ω–∫–æ–≤")
        if not self.runs:
            gr.Markdown("–ù–µ—Ç –∑–∞–ø—É—Å–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ utilization.")
            return

        ordered_runs = list(reversed(self.runs))

        def format_run_choice(run: Dict) -> str:
            return (
                f"{run['timestamp_readable']} | {run['dataset_type']} | "
                f"{run['git_commit_hash'][:7]}"
            )

        run_choices = [format_run_choice(run) for run in ordered_runs]
        run_selector = gr.Dropdown(
            choices=run_choices,
            value=run_choices[0],
            label="–í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–ø—É—Å–∫",
        )

        def build_utilization_view(selected: str):
            import plotly.express as px
            import plotly.graph_objects as go

            run = next(
                (item for item in ordered_runs if format_run_choice(item) == selected),
                None,
            )
            if run is None:
                return "–ó–∞–ø—É—Å–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω", go.Figure(), go.Figure()

            metrics = run.get("utilization_metrics") or {}
            if not metrics:
                return "–í –∑–∞–ø—É—Å–∫–µ –Ω–µ—Ç utilization_metrics", go.Figure(), go.Figure()

            used = int(metrics.get("used_chunks", 0))
            total = int(metrics.get("total_chunks", 0))
            unused = int(metrics.get("unused_chunks", max(total - used, 0)))
            rate = _safe_float(metrics.get("utilization_rate"))

            pie = px.pie(
                names=["used", "unused"],
                values=[used, unused],
                title="Utilization",
            )

            used_ids = metrics.get("used_chunk_ids") or []
            top_ids = used_ids[:20]
            top = px.bar(
                x=list(range(len(top_ids))),
                y=top_ids,
                labels={"x": "–ü–æ–∑–∏—Ü–∏—è", "y": "chunk_id"},
                title="–ü–µ—Ä–≤—ã–µ 20 –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö chunk_id",
            )

            text = (
                f"**Utilization rate:** `{rate:.4f}`\n"
                f"\n**Used chunks:** `{used}` –∏–∑ `{total}`"
            )
            return text, pie, top

        initial_text, initial_pie, initial_top = build_utilization_view(run_choices[0])
        summary = gr.Markdown(value=initial_text)
        pie_plot = gr.Plot(
            value=initial_pie,
            label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ vs –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ",
        )
        top_plot = gr.Plot(
            value=initial_top,
            label="–¢–æ–ø –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö chunk_id",
        )

        run_selector.change(
            fn=build_utilization_view,
            inputs=[run_selector],
            outputs=[summary, pie_plot, top_plot],
        )

    def _create_topic_coverage_tab(self):
        gr.Markdown("### –ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ–º")
        if not self.runs:
            gr.Markdown("–ù–µ—Ç –∑–∞–ø—É—Å–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ topic coverage.")
            return

        ordered_runs = list(reversed(self.runs))

        def format_run_choice(run: Dict) -> str:
            return (
                f"{run['timestamp_readable']} | {run['dataset_type']} | "
                f"{run['git_commit_hash'][:7]}"
            )

        run_choices = [format_run_choice(run) for run in ordered_runs]
        run_selector = gr.Dropdown(
            choices=run_choices,
            value=run_choices[0],
            label="–í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–ø—É—Å–∫",
        )

        def build_topic_view(selected: str):
            import plotly.express as px
            import plotly.graph_objects as go

            run = next(
                (item for item in ordered_runs if format_run_choice(item) == selected),
                None,
            )
            if run is None:
                return "–ó–∞–ø—É—Å–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω", go.Figure(), pd.DataFrame()

            metrics = run.get("topic_coverage_metrics") or {}
            topics = metrics.get("topic_coverage") or []
            if not metrics or not topics:
                return (
                    "–í –∑–∞–ø—É—Å–∫–µ –Ω–µ—Ç topic_coverage_metrics",
                    go.Figure(),
                    pd.DataFrame(),
                )

            table = pd.DataFrame(topics)
            plot = px.bar(
                table,
                x="topic_id",
                y="unique_chunks",
                hover_data=["question_count", "unique_urls"],
                title="–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ –ø–æ —Ç–µ–º–∞–º",
            )
            text = (
                f"**–¢–µ–º:** `{metrics.get('n_topics', 0)}`\n"
                f"\n**–í–æ–ø—Ä–æ—Å–æ–≤:** `{metrics.get('total_questions', 0)}`\n"
                f"\n**–°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ —á–∞–Ω–∫–æ–≤ –Ω–∞ —Ç–µ–º—É:** "
                f"`{_safe_float(metrics.get('avg_chunks_per_topic')):.2f}`"
            )
            return text, plot, table

        initial_text, initial_plot, initial_table = build_topic_view(run_choices[0])
        summary = gr.Markdown(value=initial_text)
        coverage_plot = gr.Plot(value=initial_plot, label="–ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ —Ç–µ–º–∞–º")
        coverage_table = gr.Dataframe(
            value=initial_table,
            interactive=False,
            wrap=True,
        )

        run_selector.change(
            fn=build_topic_view,
            inputs=[run_selector],
            outputs=[summary, coverage_plot, coverage_table],
        )

    def _create_reference_tab(self):
        gr.Markdown(
            """
# –°–ø—Ä–∞–≤–∫–∞ –ø–æ —Å–∏—Å—Ç–µ–º–µ –æ—Ü–µ–Ω–∫–∏ RAG

## –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ

–î–∞—à–±–æ—Ä–¥ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ Retrieval-Augmented Generation —Å–∏—Å—Ç–µ–º—ã. 
–û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏–∫—É –∏–∑–º–µ–Ω–µ–Ω–∏–π –º–µ–∂–¥—É –∑–∞–ø—É—Å–∫–∞–º–∏.

---

## –£—Ä–æ–≤–Ω–∏ –æ—Ü–µ–Ω–∫–∏ (Tiers)

### Tier 0: Embedding Quality

–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM.

- **avg_nn_distance**: —Å—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π. –ß–µ–º –Ω–∏–∂–µ, —Ç–µ–º –ø–ª–æ—Ç–Ω–µ–µ –ª–æ–∫–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞.
- **density_score**: –æ–±—Ä–∞—Ç–Ω–∞—è –≤–µ–ª–∏—á–∏–Ω–∞ –∫ avg_nn_distance. –ß–µ–º –≤—ã—à–µ, —Ç–µ–º –ø–ª–æ—Ç–Ω–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ.
- **effective_dimensionality**: —á–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è 95% –¥–∏—Å–ø–µ—Ä—Å–∏–∏.

### Tier 1: Retrieval

–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

- **HitRate@K**: –¥–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –≤ —Ç–æ–ø-K.
- **MRR (Mean Reciprocal Rank)**: —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞–Ω–≥–∞ –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
- **Recall@K**: –ø–æ–ª–Ω–æ—Ç–∞ - –¥–æ–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞.
- **Precision@K**: —Ç–æ—á–Ω–æ—Å—Ç—å - –¥–æ–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å—Ä–µ–¥–∏ —Ç–æ–ø-K.
- **NDCG@K**: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –≤—ã–≥–æ–¥–∞.

–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è MRR –∏ Recall –æ–∑–Ω–∞—á–∞—é—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.

### Tier 2: Generation

–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.

- **avg_faithfulness**: –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (—à–∫–∞–ª–∞ 1-5).
- **avg_answer_relevance**: –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –≤–æ–ø—Ä–æ—Å—É (—à–∫–∞–ª–∞ 1-5).

–≠—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –≤—ã—Å—Ç–∞–≤–ª—è—é—Ç—Å—è LLM-—Å—É–¥—å–µ–π. –ó–Ω–∞—á–µ–Ω–∏—è –≤—ã—à–µ 4.0 —Å—á–∏—Ç–∞—é—Ç—Å—è —Ö–æ—Ä–æ—à–∏–º–∏.

### Tier 3: End-to-End

–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω retrieval + generation.

- **avg_e2e_score**: –æ–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ LLM-—Å—É–¥—å–µ–π (—à–∫–∞–ª–∞ 1-5).
- **avg_semantic_similarity**: –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–∞–º–∏.

### Tier Judge (Qwen)

–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å judge-–º–æ–¥–µ–ª–∏ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö.

- **consistency_score**: –¥–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–¥–µ –æ—Ü–µ–Ω–∫–∞ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –∑–∞–ø—É—Å–∫–µ.
- **error_rate**: –¥–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–¥–µ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ API.
- **avg_latency_ms**: —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ API –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö.

### Tier Judge Pipeline (Mistral)

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç production judge, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç "–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –∏–ª–∏ –Ω–µ—Ç".

- **accuracy**: —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–π —Å—É–¥—å–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
- **precision/recall/f1_score**: –¥–ª—è –∫–ª–∞—Å—Å–∞ "–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ—Ç–≤–µ—Ç".

### Tier UX

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è.

- **cache_hit_rate**: –¥–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤ –∫—ç—à–µ –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.
- **context_preserve**: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –º–Ω–æ–≥–æturn –¥–∏–∞–ª–æ–≥–∞—Ö.
- **multi_turn_consistency**: —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏.

### Real Users

–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã QuestionAnswer.

- –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–µ –∂–µ –º–µ—Ç—Ä–∏–∫–∏ retrieval: MRR, Recall@K, Precision@K, NDCG@K.
- –ü–æ–∑–≤–æ–ª—è—é—Ç –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º —Ç—Ä–∞—Ñ–∏–∫–µ.

---

## –¢–∏–ø—ã –∑–∞–ø—É—Å–∫–æ–≤

### Synthetic

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ LLM. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.

### Manual / –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è

–î–∞—Ç–∞—Å–µ—Ç, –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–ª—è —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏. –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å —É—á–∞—Å—Ç–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞-—ç–∫—Å–ø–µ—Ä—Ç–∞.

### Real Users

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã QuestionAnswer. –ù–∞–∏–±–æ–ª–µ–µ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π.

---

## –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–æ—Ä–æ–≥–∏ (Baseline)

| Tier | –ú–µ—Ç—Ä–∏–∫–∞ | –ü–æ—Ä–æ–≥ |
|------|---------|-------|
| Tier 0 | density_score | >= 3.00 |
| Tier 0 | avg_nn_distance | <= 0.30 |
| Tier 1 | MRR | >= 0.80 |
| Tier 1 | HitRate@5 | >= 0.90 |
| Tier 1 | HitRate@10 | >= 0.95 |
| Tier 2 | Faithfulness | >= 4.5 |
| Tier 2 | Answer Relevance | >= 4.2 |
| Tier 3 | E2E Score | >= 4.2 |
| Tier 3 | Semantic Similarity | >= 0.85 |
| Tier Judge | consistency_score | >= 0.90 |
| Tier Judge | error_rate | <= 0.05 |
| Tier Judge Pipeline | accuracy | >= 0.85 |
| Tier Judge Pipeline | f1_score | >= 0.85 |

---

## –†–∞–±–æ—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π

- **Metric History**: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏.
- **Tier Comparison**: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π –Ω–∞ –æ–¥–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ.
- **Runs Registry**: —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–ø—É—Å–∫–æ–≤ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±–∑–æ—Ä–∞.

–ü—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–π –æ–±—Ä–∞—â–∞–π—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∏–∂–µ baseline - –æ–Ω–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ —Å–∏—Å—Ç–µ–º–µ.
            """
        )


def find_free_port(start_port=7860, max_port=7870):
    """–ù–∞–π—Ç–∏ —Å–≤–æ–±–æ–¥–Ω—ã–π –ø–æ—Ä—Ç –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ."""
    import socket

    for port in range(start_port, max_port + 1):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.bind(("0.0.0.0", port))
                return port
            except OSError:
                continue
    return None


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –¥–∞—à–±–æ—Ä–¥–∞."""
    if not GRADIO_AVAILABLE:
        print("Gradio –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: uv sync --extra dashboard")
        return

    import argparse

    parser = argparse.ArgumentParser(description="–ó–∞–ø—É—Å–∫ –¥–∞—à–±–æ—Ä–¥–∞ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤")
    parser.add_argument(
        "--port",
        type=int,
        default=None,
        help="–ü–æ—Ä—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –¥–∞—à–±–æ—Ä–¥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –ø–µ—Ä–≤—ã–π —Å–≤–æ–±–æ–¥–Ω—ã–π –æ—Ç 7860)",
    )
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—Ç
    if args.port:
        port = args.port
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–æ—Ä—Ç: {port}")
    else:
        port = find_free_port()
        if port:
            logger.info(f"–ù–∞–π–¥–µ–Ω —Å–≤–æ–±–æ–¥–Ω—ã–π –ø–æ—Ä—Ç: {port}")
        else:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–æ–±–æ–¥–Ω—ã–π –ø–æ—Ä—Ç –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 7860-7870")
            return

    dashboard = RAGBenchmarkDashboard()
    interface = dashboard.create_interface()
    interface.launch(server_name="0.0.0.0", server_port=port, share=False, debug=True)


if __name__ == "__main__":
    main()
