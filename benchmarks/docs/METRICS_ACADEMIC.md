<script type="text/javascript" 
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true
  }
});
</script>

<style>
  body {
    font-family: "Times New Roman", Times, serif;
  }
</style>

<br>
<br>
<br>
<br>

<div align="center">
    <strong>НАУЧНОЕ ОПИСАНИЕ МЕТРИК БЕНЧМАРКОВ RAG-СИСТЕМЫ</strong>
    <br><br>
    <strong>Система «Вопрошалыч»</strong>
    <br><br>
    <strong>Техническая документация</strong>
</div>

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br>

<div align="right">
    Автор:<br>
    Команда разработки Voproshalych<br>
</div>

<br><br><br><br><br><br>

<div align="center">
    Тюмень, 2026
</div>

<div style="page-break-after: always;"></div>

**Содержание**

1. Введение и теоретические основания
2. Уровни оценки (Tiers)
3. Tier 0: Качество эмбеддингов (Intrinsic)
4. Tier 1: Точность поиска (Retrieval Accuracy)
5. Tier 2: Качество генерации
6. Tier 3: Сквозное качество (End-to-End)
7. Real-Users Data: Источник продуктовой и доменной аналитики
8. Tier Judge: Качество LLM-судьи

<div style="page-break-after: always;"></div>

# 1. Введение и теоретические основания

## 1.1 Цель бенчмаркинга

Оценка качества систем Retrieval-Augmented Generation (RAG) представляет собой многоаспектную задачу, требующую комплексного подхода к измерению производительности на различных этапах информационно-поискового конвейера. Система бенчмарков «Вопрошалыч» реализует иерархическую структуру оценки, охватывающую:

1. **Качество векторных представлений** (Tier 0) — фундаментальный уровень, определяющий способность модели захватывать семантические отношения между текстами
2. **Эффективность поиска** (Tier 1) — оценка релевантности извлекаемых документов
3. **Качество генерации** (Tier 2) — измерение способности модели формировать достоверные и релевантные ответы при идеальном контексте
4. **Сквозное качество** (Tier 3) — комплексная оценка всего пайплайна в условиях, приближенных к реальной эксплуатации

## 1.2 Классификация метрик

Метрики системы классифицируются по нескольким измерениям:

**По уровню абстракции:**

- *Алгоритмические метрики* — формальные математические функции, вычисляемые без привлечения внешних моделей (HitRate, MRR, NDCG, ROUGE, BLEU)
- *Метрики на основе LLM* — оценки, получаемые от языковых моделей в роли «судьи» (Faithfulness, Answer Relevance, E2E Score)
- *Гибридные метрики* — комбинированные показатели, использующие несколько источников информации (Semantic Similarity)

**По целевому аспекту качества:**

- *Метрики релевантности* — измеряют соответствие между запросом и результатами поиска
- *Метрики качества текста* — оценивают характеристики сгенерированного текста
- *Метрики согласованности* — измеряют стабильность и воспроизводимость результатов
- *Метрики пользовательского опыта* — оценивают аспекты, влияющие на восприятие системы конечным пользователем

## 1.3 Библиотеки и инструменты

Ниже перечислены библиотеки, которые фактически используются в текущей реализации вычисления метрик и аналитики:

| Библиотека | Статус | Где используется | Назначение |
|------------|--------|------------------|------------|
| `numpy` | используется | Tier 0/1/2/3/Judge/UX | Агрегации, векторная алгебра, статистики |
| `scikit-learn` | используется | Tier 0, Topic Coverage | NearestNeighbors, pairwise distances, KMeans |
| `sqlalchemy` + `pgvector` | используется | Tier 1/3, Real-users, util/topic | Retrieval через `cosine_distance` в PostgreSQL |
| `sentence-transformers` | используется | Все tier, util/topic | Генерация эмбеддингов вопросов и ответов |
| `openai` | используется | `LLMJudge` | API-клиент для judge-моделей |
| `requests` | используется | Tier Judge Pipeline, `qa.main` | Вызовы production Mistral API |
| `rouge-score` | используется | Tier 2/3 | Метрики ROUGE-1/2/L |
| `sacrebleu` | используется | Tier 2/3 | Метрика BLEU |
| `pandas` | используется | Dashboard, UMAP скрипт | Табличная аналитика и подготовка данных |
| `plotly` | используется | Dashboard, UMAP скрипт | Графики и интерактивные визуализации |
| `umap-learn` | используется | Vector Space анализ | Снижение размерности для проекций 2D/3D |

Дополнительно:

- `ragas` — **не используется** в текущем коде.
- `bert-score` — частично подготовлен в `text_metrics.py`, но в текущем benchmark-пайплайне отключён (`include_bertscore=False`).

<div style="page-break-after: always;"></div>

# 2. Уровни оценки (Tiers)

## 2.1 Сводная таблица уровней

| Tier | Название | Уровень абстракции | Основные метрики |
|------|----------|-------------------|------------------|
| 0 | Embedding Quality (Intrinsic) | Инфраструктурный | NN Distance, Spread, Pairwise Distance |
| 1 | Retrieval | Поисковый | HitRate, MRR, NDCG |
| 2 | Generation | Генеративный | Faithfulness, Answer Relevance, ROUGE, BLEU |
| 3 | End-to-End | Системный | E2E Score, Semantic Similarity |
| Judge | LLM Judge Quality | Мета-оценочный | Consistency Score, Error Rate |
| Judge Pipeline | Production Judge | Операционный | Accuracy, Precision, Recall, F1 |
| UX | User Experience | Пользовательский | Cache Hit Rate, Context Preservation |

## 2.2 Какие компоненты системы оцениваются

Термины «поиск» и «генерация» в рамках бенчмарков декомпозируются на конкретные подсистемы:

| Уровень | Какие компоненты покрывает | Что не покрывает напрямую |
|--------|------------------------------|---------------------------|
| Tier 0 | Геометрия эмбеддингов и векторного пространства | Скорость БД, ранжирование по URL, UX |
| Tier 1 | Retrieval в PostgreSQL+pgvector, ранжирование top-k | Генерацию ответа, judge-фильтрацию |
| Tier 2 | Генерация при фиксированном контексте + judge-оценка текста | Качество retrieval и индексной структуры |
| Tier 3 | Полный E2E (retrieval + generation + оценка ответа) | Долгосрочные пользовательские эффекты |
| Tier Judge | Стабильность и надёжность benchmark judge | Качество production judge решения show/no-show |
| Tier Judge Pipeline | Классификация production judge (show/no-show) | Качество текста генерации как таковое |
| Tier UX | Повторяемость ответа, кэш и сохранение контекста | Тонкая фактическая correctness каждого ответа |
| Real-users | Поведение retrieval на живом трафике | Полный gold generation benchmark |

<div style="page-break-after: always;"></div>

# 3. Tier 0: Качество эмбеддингов (Intrinsic)

## 3.0 Декомпозиция оцениваемых компонентов

Tier 0 оценивает именно **векторные представления и геометрию пространства**, без проверки SQL-поиска и без проверки генерации.

Компоненты, которые покрывает Tier 0:

- **Embedding model quality**: насколько эмбеддер формирует полезную структуру близостей/дистанций между текстами.
- **Vector space geometry**: плотность, разброс, эффективная размерность, наличие выбросов и «схлопывания» пространства.
- **Data health сигнал**: косвенный контроль деградации датасета (дубликаты, шум, неравномерность покрытия тем).

Компоненты, которые Tier 0 не покрывает:

- эффективность индекса в БД;
- качество ранжирования retrieval;
- качество генерации ответа.

## 3.1 Теоретическое обоснование

Tier 0 оценивает геометрию векторного пространства **без требований к разметке** (`cluster_id`, `label`). Это делает анализ устойчивым для synthetic/manual/realq датасетов, где явные кластеры могут отсутствовать.

## 3.2 Почему обновлён подход

Ранее Tier 0 использовал silhouette и intra/inter cluster метрики, которые требуют явных меток кластеров. Для большинства текущих датасетов такие поля отсутствуют, поэтому метрики были статистически невалидны. Новый Tier 0 использует intrinsic метрики, вычисляемые напрямую по эмбеддингам.

## 3.3 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `total_samples` | int | Количество векторов в анализе |
| `avg_nn_distance` | float | Среднее расстояние до k ближайших соседей |
| `std_nn_distance` | float | Стандартное отклонение расстояний до соседей |
| `density_score` | float | Плотность пространства (`1 / avg_nn_distance`) |
| `avg_spread` | float | Среднее расстояние до центроида |
| `max_spread` | float | Максимальное расстояние до центроида |
| `spread_std` | float | Разброс расстояний до центроида |
| `effective_dimensionality` | int | Число компонент для 95% дисперсии |
| `avg_pairwise_distance` | float | Среднее попарное расстояние |
| `std_pairwise_distance` | float | Разброс попарных расстояний |
| `min_pairwise_distance` | float | Минимальное попарное расстояние |
| `max_pairwise_distance` | float | Максимальное попарное расстояние |

## 3.4 Формулы

Для матрицы эмбеддингов $E \in \mathbb{R}^{N \times D}$:

- Ближайшие соседи:

$$
\text{avg\_nn\_distance} = \frac{1}{N \cdot k}\sum_{i=1}^{N}\sum_{j=1}^{k} d_{i,j}
$$

$$
\text{density\_score} = \frac{1}{\text{avg\_nn\_distance} + \varepsilon}
$$

- Разброс относительно центроида $\mu = \frac{1}{N}\sum_i e_i$:

$$
\text{avg\_spread} = \frac{1}{N}\sum_{i=1}^{N} ||e_i - \mu||_2
$$

- Эффективная размерность:

$$
\text{effective\_dimensionality} = \min\left\{m :
\frac{\sum_{i=1}^{m}\lambda_i}{\sum_{i=1}^{D}\lambda_i} \ge 0.95\right\}
$$

где $\lambda_i$ — собственные значения ковариационной матрицы.

- Попарные расстояния:

$$
\text{avg\_pairwise\_distance} =
\frac{2}{N(N-1)}\sum_{1 \le i < j \le N} d(e_i, e_j)
$$

## 3.5 Интерпретация

- `avg_nn_distance` ниже -> локальная компактность выше.
- `std_nn_distance` выше -> неоднородная плотность (возможные «дыры» в пространстве).
- `density_score` выше -> пространство плотнее.
- `effective_dimensionality` слишком низкая -> возможная деградация/схлопывание.
- `max_pairwise_distance` и `max_spread` помогают выявлять выбросы.

## 3.6 Визуальная валидация

Для ручной диагностики структуры пространства используйте UMAP-визуализацию:

- `python benchmarks/visualize_vector_space.py --limit 5000`
- `python benchmarks/visualize_vector_space.py --limit 3000 --3d`

Эта визуализация не является метрикой, но помогает интерпретировать Tier 0.

<div style="page-break-after: always;"></div>

# 4. Tier 1: Точность поиска (Retrieval Accuracy)

## 4.0 Декомпозиция оцениваемых компонентов

Tier 1 оценивает не абстрактный «поиск вообще», а конкретную цепочку retrieval:

- **Query embedding**: преобразование пользовательского вопроса в вектор.
- **Vector distance function**: сравнение по `cosine_distance`.
- **DB retrieval execution**: SQL-запрос к PostgreSQL + pgvector, возврат top-k.
- **Ranking quality**: место релевантного документа в выдаче.

Что важно: Tier 1 в текущем виде не изолирует отдельно вклад каждого звена. Метрики отражают их совместный итог.

Компоненты вне покрытия Tier 1:

- генерация ответа (LLM);
- пост-обработка и judge-решение «показывать/не показывать».

## 4.1 Теоретическое обоснование

Tier 1 оценивает **эффективность информационно-поисковой системы** на уровне извлечения релевантных документов. Это критический компонент RAG, поскольку качество конечного ответа напрямую зависит от релевантности контекста, переданного генеративной модели.

В контексте RAG-системы поиск выполняет функцию **селекции контекста** — из всей базы знаний необходимо извлечь наиболее релевантные фрагменты для формирования ответа. Ошибки на этом этапе носят катастрофический характер: нерелевантный контекст приводит к генерации недостоверной информации (галлюцинациям).

## 4.2 Почему мы используем эти метрики

Совокупность метрик HitRate, MRR и NDCG образует стандарт де-факто в академических и промышленных системах оценки информационного поиска (TREC, BEIR, MTEB). Каждая метрика захватывает различные аспекты качества поиска:

- **HitRate** — практическая метрика, отражающая вероятность нахождения хотя бы одного релевантного документа
- **MRR** — учитывает ранг первого релевантного результата, что критично для пользовательского опыта
- **NDCG** — наиболее информативная метрика, учитывающая как факт релевантности, так и её степень (через позицию)

## 4.3 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `hit_rate@1` | float | Hit Rate при top-1 (0-1) |
| `hit_rate@5` | float | Hit Rate при top-5 (0-1) |
| `hit_rate@10` | float | Hit Rate при top-10 (0-1) |
| `mrr` | float | Mean Reciprocal Rank (0-1) |
| `recall@K` | float | Recall@K для K=1,3,5,10 |
| `precision@K` | float | Precision@K для K=1,3,5,10 |
| `ndcg@5` | float | NDCG@5 (0-1) |
| `ndcg@10` | float | NDCG@10 (0-1) |
| `retrieval_consistency` | float | Стабильность top-k выдачи при повторном одинаковом запросе |

## 4.4 Детальное описание метрик

### 4.4.1 Hit Rate@K

**Определение:** Доля запросов, для которых хотя бы один релевантный документ присутствует среди первых $K$ результатов поиска.

$$ \text{HitRate}@K = \frac{1}{|Q|} \sum_{q \in Q} \mathbb{1}\left[\exists d \in \text{top}_K(q) : \text{rel}(q, d) = 1\right] $$

где $Q$ — множество запросов, $\text{top}_K(q)$ — первые $K$ результатов для запроса $q$, $\mathbb{1}[\cdot]$ — индикаторная функция, $\text{rel}(q, d)$ — бинарная релевантность.

**Вычислительный пайплайн:**

1. Для каждого запроса $q \in Q$ выполняется поиск с возвратом top-$K$ результатов
2. Проверяется наличие релевантного документа в полученном списке
3. Считается доля запросов с хотя бы одним релевантным результатом

**Интерпретация:**

- HitRate@1 = 1.0: идеальный поиск, первый результат всегда релевантен
- HitRate@10 = 0.5: в 50% случаев релевантный документ присутствует в top-10
- Для RAG-систем критически важно значение HitRate@1

### 4.4.2 Mean Reciprocal Rank (MRR)

**Определение:** Среднее значение обратного ранга первого релевантного документа.

$$ \text{MRR} = \frac{1}{|Q|} \sum_{q \in Q} \frac{1}{\text{rank}_q} $$

где $\text{rank}_q$ — позиция первого релевантного документа в результатах поиска для запроса $q$. Если релевантный документ отсутствует, $\frac{1}{\text{rank}_q} = 0$.

**Интерпретация:**

- MRR = 1.0: первый результат всегда релевантен
- MRR = 0.5: в среднем первый релевантный результат на позиции 2
- MRR = 0.1: первый релевантный результат в среднем на позиции 10

### 4.4.3 Recall@K и Precision@K

**Определение:**

$$ \text{Recall}@K = \frac{|\text{rel}_{\text{top}_K}|}{|\text{rel}_{\text{all}}|} $$

$$ \text{Precision}@K = \frac{|\text{rel}_{\text{top}_K}|}{K} $$

где $|\text{rel}_{\text{top}_K}|$ — количество релевантных документов в top-$K$, $|\text{rel}_{\text{all}}|$ — общее количество релевантных документов в коллекции.

**Интерпретация:**

- Recall@K = 1.0: все релевантные документы найдены (при $K \geq |\text{rel}_{\text{all}}|$)
- Precision@K = 1.0: все $K$ результатов релевантны
- Trade-off: при фиксированном $K$ увеличение Recall приводит к снижению Precision

### 4.4.4 Normalized Discounted Cumulative Gain (NDCG@K)

**Определение:** Нормализованный дисконтированный кумулятивный выигрыш — метрика, учитывающая как факт релевантности, так и позицию релевантного документа.

$$ \text{NDCG}@K = \frac{\text{DCG}@K}{\text{IDCG}@K} $$

**Discounted Cumulative Gain (DCG):**

$$ \text{DCG}@K = \sum_{i=1}^{K} \frac{\text{rel}_i}{\log_2(i+1)} $$

где $\text{rel}_i$ — релевантность документа на позиции $i$ (в бинарном случае: 1 или 0).

**Ideal DCG (IDCG):** DCG при идеальном упорядочивании:

$$ \text{IDCG}@K = \sum_{i=1}^{\min(K, |\text{rel}_{\text{all}}|)} \frac{1}{\log_2(i+1)} $$

**Интерпретация:**

- NDCG@K = 1.0: идеальное упорядочивание, все релевантные документы на первых позициях
- NDCG@K = 0.0: нет релевантных документов в top-$K$
- NDCG@K > 0.9: отличное качество поиска
- NDCG@K = 0.5–0.9: приемлемое качество
- NDCG@K < 0.5: требуется улучшение

<div style="page-break-after: always;"></div>

# 5. Tier 2: Качество генерации

## 5.0 Декомпозиция оцениваемых компонентов

Tier 2 изолирует **генерацию при заданном контексте**. Это означает:

- retrieval как источник ошибок минимизирован (контекст берётся из датасета);
- оценивается, как LLM интерпретирует контекст и формирует ответ;
- LLM-судья оценивает ответ по критериям faithfulness/relevance.

Компоненты, которые покрывает Tier 2:

- **Prompt + generation model behavior** на фиксированном контексте;
- **Grounded response quality** (нет ли фактов вне контекста);
- **Text realization quality** (ROUGE/BLEU как поверхностный сигнал).

Компоненты вне покрытия Tier 2:

- качество retrieval и ранжирования;
- стабильность полного production-пайплайна.

## 5.1 Теоретическое обоснование

Tier 2 оценивает **способность генеративной модели формировать качественные ответы** при условии, что контекст (релевантные документы) уже извлечён. Это позволяет изолировать проблемы генерации от проблем поиска и провести диагностику каждого компонента независимо.

В данном tier используется парадигма **идеального контекста** — модели предоставляется вся релевантная информация из датасета, что позволяет оценить её способность корректно эту информацию использовать и трансформировать в ответ на естественном языке.

## 5.2 Почему мы используем эти метрики

Качество генерации текста является многомерным понятием, которое невозможно адекватно оценить единственной метрикой. Система использует три категории оценок:

1. **LLM-as-a-Judge** — современный подход, использующий способность больших языковых моделей к пониманию и оценке текста
2. **Алгоритмические метрики (ROUGE, BLEU)** — классические статистические методы, основанные на сравнении n-грамм
3. Комбинированный подход обеспечивает как глубину оценки (LLM), так и воспроизводимость (алгоритмические метрики)

## 5.3 Метрики LLM-судьи

| Метрика | Тип | Описание |
|---------|-----|----------|
| `avg_faithfulness` | float | Средняя оценка фактичности (0-1) |
| `avg_answer_relevance` | float | Средняя оценка релевантности ответа (0-1) |
| `avg_answer_correctness` | float | Средняя оценка корректности относительно эталона (1-5) |
| `response_exact_match_rate` | float | Доля дословно совпадающих ответов при повторном запуске |
| `response_semantic_consistency` | float | Средняя семантическая согласованность ответов при повторах |

## 5.4 Метрики текстового сравнения

| Метрика | Тип | Описание |
|---------|-----|----------|
| `avg_rouge1_f` | float | ROUGE-1 F-measure (0-1) |
| `avg_rouge2_f` | float | ROUGE-2 F-measure (0-1) |
| `avg_rougeL_f` | float | ROUGE-L F-measure (0-1) |
| `avg_bleu` | float | BLEU score (0-100) |

## 5.5 Детальное описание метрик

### 5.5.1 Faithfulness (фактичность)

**Определение:** Оценка степени согласованности сгенерированного ответа с предоставленным контекстом. Ответ считается фактически достоверным, если вся информация в нём содержится в контексте и не является выдуманной (галлюцинацией).

**Интерпретация:**

- Faithfulness = 1.0: все ответы полностью достоверны
- Faithfulness > 0.8: высокое качество, минимальные галлюцинации
- Faithfulness < 0.5: серьёзные проблемы с достоверностью

**Теоретическое обоснование:** Faithfulness напрямую измеряет критически важную характеристику RAG-систем — склонность к галлюцинациям.

### 5.5.2 Answer Relevance (релевантность ответа)

**Определение:** Оценка степени соответствия сгенерированного ответа заданному вопросу. Ответ релевантен, если он адресует именно тот вопрос, который был задан.

**Интерпретация:**

- Answer Relevance = 1.0: все ответы точно соответствуют вопросам
- Answer Relevance > 0.8: высокое качество понимания запросов

### 5.5.3 ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

**Определение:** Семейство метрик, основанных на подсчёте перекрытия n-грамм между сгенерированным текстом (гипотезой) и эталонным текстом (референсом).

$$ \text{ROUGE-}N = \frac{\sum_{g \in \text{Grams}_N(hypothesis)} \min(\text{count}_H(g), \text{count}_R(g))}{\sum_{g \in \text{Grams}_N(reference)} \text{count}_R(g)} $$

**Интерпретация:**

- ROUGE-L-F > 0.5: хорошее качество
- ROUGE-1-F > 0.4: приемлемое качество для коротких текстов

### 5.5.4 BLEU (Bilingual Evaluation Understudy)

**Определение:** Метрика, первоначально разработанная для оценки машинного перевода, основанная на точности n-грамм с штрафом за краткость.

$$ \text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right) $$

**Интерпретация:**

- BLEU = 100: идеальное совпадение (практически недостижимо)
- BLEU > 50: очень хорошее качество машинного перевода
- BLEU 20–40: приемлемое качество
- BLEU < 20: требуется существенное улучшение

<div style="page-break-after: always;"></div>

# 6. Tier 3: Сквозное качество (End-to-End)

## 6.0 Декомпозиция оцениваемых компонентов

Tier 3 покрывает **полный пользовательский путь** от вопроса до финального ответа, включая взаимодействие между подсистемами.

Компоненты, которые покрывает Tier 3:

- **Retrieval path**: query embedding + pgvector поиск top-1;
- **Context-to-answer generation** через production generation model;
- **End-user visible answer quality** (E2E judge + similarity/text metrics);
- **Cross-component coupling**: как ошибки retrieval усиливаются в generation.

Компоненты вне покрытия Tier 3:

- операционные SLA (пропускная способность/очереди/нагрузка);
- бизнес-метрики продукта (удержание, удовлетворённость по сессиям).

## 6.1 Теоретическое обоснование

Tier 3 оценивает **полный RAG-пайплайн** в условиях, максимально приближенных к реальной эксплуатации. В отличие от Tier 2, где используется идеальный контекст из датасета, Tier 3 выполняет реальный поиск через production-код (`qa.confluence_retrieving.get_chunk()`), что позволяет оценить систему в целом.

## 6.2 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `avg_e2e_score` | float | Средняя E2E оценка качества (0-1) |
| `avg_semantic_similarity` | float | Среднее семантическое сходство (cosine, -1 to 1) |
| `avg_dot_similarity` | float | Средняя dot-product схожесть ответов |
| `avg_euclidean_distance` | float | Средняя евклидова дистанция между ответом и эталоном |
| `response_exact_match_rate` | float | Повторяемость ответа при идентичном запросе |
| `response_semantic_consistency` | float | Семантическая согласованность E2E ответов |
| `avg_rouge1_f` | float | ROUGE-1 F-measure (0-1) |
| `avg_rouge2_f` | float | ROUGE-2 F-measure (0-1) |
| `avg_rougeL_f` | float | ROUGE-L F-measure (0-1) |
| `avg_bleu` | float | BLEU score (0-100) |

## 6.3 Детальное описание метрик

### 6.3.1 E2E Score

**Определение:** Комплексная оценка качества ответа, учитывающая вопрос, сгенерированный ответ и эталонный ответ.

**Интерпретация:** Аналогично Faithfulness и Answer Relevance, но в контексте реального использования.

### 6.3.2 Semantic Similarity (семантическое сходство)

**Определение:** Косинусное сходство между векторными представлениями сгенерированного ответа и эталонного ответа.

$$ \text{semantic\_similarity} = \cos(\vec{e}_{sys}, \vec{e}_{gt}) = \frac{\vec{e}_{sys} \cdot \vec{e}_{gt}}{|\vec{e}_{sys}| \cdot |\vec{e}_{gt}|} $$

**Интерпретация:**

- $\to 1.0$: семантически идентичные ответы
- $\to 0.0$: ортогональные ответы
- $\to -1.0$: противоположные по смыслу

<div style="page-break-after: always;"></div>

# 7. Real-Users Data: Источник продуктовой и доменной аналитики

## 7.0 Принцип использования

В текущей архитектуре real-user вопросы **не используются как benchmark-датасет** для Tier 1/2/3. Оценка retrieval/generation/e2e выполняется на synthetic и manual датасетах, где есть управляемый и проверяемый ground truth.

## 7.1 Роль в системе

Real-user корпус используется как аналитический слой для:

1. **Chunk Utilization** — доля реально используемых чанков;
2. **Topic Coverage** — покрытие тематических кластеров источниками;
3. **Domain Analysis** — поведенческие и лингвистические паттерны пользовательских вопросов.

## 7.2 Почему это корректно методологически

Для реальных вопросов обычно отсутствует полноценная эталонная разметка по релевантным документам и по корректному ответу. Поэтому их применение в качестве строгого benchmark-датасета приводит к смешению операционных сигналов и «золотых» метрик качества.

Использование real-user данных как аналитического источника позволяет:

- сохранять реализм предметной области;
- не искажать benchmark-метрики retrieval/generation;
- поддерживать трассируемый цикл улучшений (через utilization/topics/domain).

<div style="page-break-after: always;"></div>

# 8. Tier Judge: Качество LLM-судьи

## 8.0 Декомпозиция оцениваемых компонентов

Tier Judge оценивает не RAG-ответ как таковой, а **инструмент оценки**:

- **repeatability**: повторяемость оценки на одинаковом входе;
- **judge reliability**: устойчивость к ошибкам API/парсинга;
- **operational viability**: latency судьи для массовых прогонов.

Это мета-уровень контроля качества, который влияет на доверие ко всем LLM-основанным метрикам в Tier 2/3.

<div style="page-break-after: always;"></div>

**Конец документа**
