"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–æ–ª–æ—Ç–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM-—Å—É–¥—å—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∏–¥–µ–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞–Ω–∫–æ–≤.
"""

import argparse
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from dotenv import load_dotenv

sys.path.insert(0, str(Path(__file__).parent.parent))

from qa.config import Config
from qa.database import Chunk, create_engine
from benchmarks.utils.llm_judge import LLMJudge

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

logger = logging.getLogger(__name__)

# –Ø–≤–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º .env.docker –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
# –í Docker –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è .env.docker –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ docker compose
load_dotenv(dotenv_path=".env.docker")


def _load_existing_dataset(dataset_path: str) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è."""
    if not dataset_path or not os.path.exists(dataset_path):
        return []

    with open(dataset_path, "r", encoding="utf-8") as file:
        payload = json.load(file)

    if not isinstance(payload, list):
        logger.warning("–§–∞–π–ª %s –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º", dataset_path)
        return []

    return payload


def generate_synthetic_dataset(
    engine,
    max_questions: int,
    output_path: str,
    skip_existing_dataset: str | None = None,
    generation_attempts: int = 3,
):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç.

    Args:
        engine: –î–≤–∏–∂–æ–∫ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        max_questions: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        skip_existing_dataset: –ü—É—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
        generation_attempts: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
    """
    from sqlalchemy import func, select
    from sqlalchemy.orm import Session

    judge = LLMJudge()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    generation_errors: List[Dict[str, Any]] = []

    existing_dataset = _load_existing_dataset(skip_existing_dataset or "")
    existing_chunk_ids = {
        item.get("chunk_id")
        for item in existing_dataset
        if isinstance(item, dict) and item.get("chunk_id") is not None
    }
    dataset: List[Dict[str, Any]] = [
        item for item in existing_dataset if isinstance(item, dict)
    ]

    if existing_dataset:
        logger.info(
            "–ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç: %s –∑–∞–ø–∏—Å–µ–π, %s —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö chunk_id",
            len(existing_dataset),
            len(existing_chunk_ids),
        )

    with Session(engine) as session:
        total_chunks = session.scalar(select(func.count(Chunk.id))) or 0
        logger.info(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –≤ –ë–î: {total_chunks}")

        available_chunks = session.scalars(
            select(Chunk)
            .where(Chunk.embedding.isnot(None))
            .where(Chunk.text.isnot(None))
            .where(Chunk.text != "")
            .order_by(Chunk.id.asc())
        ).all()

        available_chunks_total = len(available_chunks)
        logger.info(f"–ß–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {available_chunks_total}")

        for i, chunk in enumerate(available_chunks, 1):
            if len(dataset) >= max_questions:
                logger.info("–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç max_questions=%s", max_questions)
                break
            if chunk.id in existing_chunk_ids:
                continue

            try:
                result = None
                last_error: Exception | None = None
                for attempt in range(1, generation_attempts + 1):
                    try:
                        result = judge.generate_question_from_chunk(chunk.text)
                        break
                    except Exception as error:  # noqa: PERF203
                        last_error = error
                        logger.warning(
                            "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ chunk_id=%s (attempt %s/%s): %s",
                            chunk.id,
                            attempt,
                            generation_attempts,
                            error,
                        )

                if result is None:
                    generation_errors.append(
                        {
                            "chunk_id": chunk.id,
                            "reason": str(last_error or "generation_failed"),
                        }
                    )
                    continue

                dataset_item = {
                    "chunk_id": chunk.id,
                    "chunk_text": chunk.text[:500],
                    "question": result["question"].strip(),
                    "ground_truth_answer": result["ground_truth_answer"],
                    "confluence_url": chunk.confluence_url,
                }

                dataset.append(dataset_item)
                existing_chunk_ids.add(chunk.id)

                logger.info(
                    "[%s/%s] chunk_id=%s, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –≤–æ–ø—Ä–æ—Å",
                    len(dataset),
                    max_questions,
                    chunk.id,
                )

                if len(dataset) % 10 == 0:
                    logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {len(dataset)}/{max_questions}")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –¥–ª—è —á–∞–Ω–∫–∞ {chunk.id}: {e}")
                generation_errors.append({"chunk_id": chunk.id, "reason": str(e)})
                continue

            if i % 200 == 0:
                logger.info("–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —á–∞–Ω–∫–æ–≤: %s/%s", i, available_chunks_total)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)

    error_report_path = f"benchmarks/data/dataset_errors_{timestamp}.json"
    with open(error_report_path, "w", encoding="utf-8") as f:
        json.dump(generation_errors, f, ensure_ascii=False, indent=2)

    logger.info(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")
    logger.info(f"üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(dataset)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
    logger.info("üìõ –û—à–∏–±–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: %s", len(generation_errors))
    logger.info("üßæ –û—Ç—á—ë—Ç –æ–± –æ—à–∏–±–∫–∞—Ö: %s", error_report_path)

    print("\n=== –ò—Ç–æ–≥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===")
    print(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º: {available_chunks_total}")
    print(f"–£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø–∞—Ä: {len(dataset)}")
    print(f"–û—à–∏–±–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {len(generation_errors)}")
    print(f"–§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞: {output_path}")
    print(f"–§–∞–π–ª –æ—à–∏–±–æ–∫: {error_report_path}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI —Å–∫—Ä–∏–ø—Ç–∞."""
    parser = argparse.ArgumentParser(
        description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤"
    )

    parser.add_argument(
        "--max-questions",
        type=int,
        default=500,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç (default: 500)",
    )

    parser.add_argument(
        "--num-samples",
        type=int,
        default=None,
        help="–£—Å—Ç–∞—Ä–µ–≤—à–∏–π —Ñ–ª–∞–≥ (–∞–ª–∏–∞—Å –¥–ª—è --max-questions)",
    )

    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é versioned dataset_YYYYMMDD_HHMMSS.json)",
    )

    parser.add_argument(
        "--check-only",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç",
    )

    parser.add_argument(
        "--skip-existing-dataset",
        type=str,
        default=None,
        help=(
            "–ü—É—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, "
            "—á–∞–Ω–∫–∏ –∏–∑ –Ω–µ–≥–æ –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã"
        ),
    )

    parser.add_argument(
        "--generation-attempts",
        type=int,
        default=3,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞",
    )

    args = parser.parse_args()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = args.output or f"benchmarks/data/dataset_{timestamp}.json"

    engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)

    if args.check_only:
        if os.path.exists(output_path):
            with open(output_path, "r", encoding="utf-8") as f:
                dataset = json.load(f)
            print(f"–î–∞—Ç–∞—Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {output_path}")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(dataset)}")
            if dataset:
                print(f"\n–ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
                print(json.dumps(dataset[0], ensure_ascii=False, indent=2))
        else:
            print(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {output_path}")
        return

    max_questions = args.max_questions
    if args.num_samples is not None:
        max_questions = args.num_samples
        logger.warning(
            "–§–ª–∞–≥ --num-samples —É—Å—Ç–∞—Ä–µ–ª, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --max-questions. "
            "–ò—Å–ø–æ–ª—å–∑—É–µ–º max_questions=%s",
            max_questions,
        )

    generate_synthetic_dataset(
        engine,
        max_questions=max_questions,
        output_path=output_path,
        skip_existing_dataset=args.skip_existing_dataset,
        generation_attempts=max(1, args.generation_attempts),
    )


if __name__ == "__main__":
    main()
