# Научное описание метрик бенчмарков RAG-системы

Документ представляет собой исчерпывающее академическое описание всех метрик, используемых в системе бенчмарков RAG-системы «Вопрошалыч». Для каждой метрики приводятся теоретическое обоснование, формальные определения, вычислительные пайплайны и анализ достоверности.

---

## 1. Введение и теоретические основания

### 1.1 Цель бенчмаркинга

Оценка качества систем Retrieval-Augmented Generation (RAG) представляет собой многоаспектную задачу, требующую комплексного подхода к измерению производительности на различных этапах информационно-поискового конвейера. Система бенчмарков «Вопрошалыч» реализует иерархическую структуру оценки, охватывающую:

1. **Качество векторных представлений** (Tier 0) — фундаментальный уровень, определяющий способность модели захватывать семантические отношения между текстами
2. **Эффективность поиска** (Tier 1) — оценка релевантности извлекаемых документов
3. **Качество генерации** (Tier 2) — измерение способности модели формировать достоверные и релевантные ответы при идеальном контексте
4. **Сквозное качество** (Tier 3) — комплексная оценка всего пайплайна в условиях, приближенных к реальной эксплуатации

### 1.2 Классификация метрик

Метрики системы классифицируются по нескольким измерениям:

**По уровню абстракции:**
- *Алгоритмические метрики* — формальные математические функции, вычисляемые без привлечения внешних моделей (HitRate, MRR, NDCG, ROUGE, BLEU)
- *Метрики на основе LLM* — оценки, получаемые от языковых моделей в роли «судьи» (Faithfulness, Answer Relevance, E2E Score)
- *Гибридные метрики* — комбинированные показатели, использующие несколько источников информации (Semantic Similarity)

**По целевому аспекту качества:**
- *Метрики релевантности* — измеряют соответствие между запросом и результатами поиска
- *Метрики качества текста* — оценивают характеристики сгенерированного текста
- *Метрики согласованности* — измеряют стабильность и воспроизводимость результатов
- *Метрики пользовательского опыта* — оценивают аспекты, влияющие на восприятие системы конечным пользователем

---

## 2. Уровни оценки (Tiers)

### 2.1 Сводная таблица уровней

| Tier | Название | Уровень абстракции | Основные метрики |
|------|----------|-------------------|------------------|
| 0 | Embedding Quality | Инфраструктурный | Silhouette Score, Intra/Inter Cluster Metrics |
| 1 | Retrieval | Поисковый | HitRate, MRR, NDCG |
| 2 | Generation | Генеративный | Faithfulness, Answer Relevance, ROUGE, BLEU |
| 3 | End-to-End | Системный | E2E Score, Semantic Similarity |
| Judge | LLM Judge Quality | Мета-оценочный | Consistency Score, Error Rate |
| Judge Pipeline | Production Judge | Операционный | Accuracy, Precision, Recall, F1 |
| UX | User Experience | Пользовательский | Cache Hit Rate, Context Preservation |

---

## 3. Tier 0: Качество эмбеддингов

### 3.1 Теоретическое обоснование

Tier 0 оценивает **внутреннее качество векторных представлений** — фундаментальную характеристику, от которой зависит успех всех последующих этапов RAG-пайплайна. Качество эмбеддингов определяет способность модели:

- **Различать семантически различные документы** — документы с разными темами должны иметь удалённые векторные представления
- **Группировать семантически схожие документы** — релевантные документы должны формировать компактные кластеры в векторном пространстве
- **Сохранять структурные отношения** — иерархические и ассоциативные связи между концепциями должны отражаться в геометрии пространства

### 3.2 Почему мы используем эти метрики

Кластерный анализ предоставляет инструментарий для исследования геометрической структуры векторного пространства без необходимости определения внешнего критерия качества. Это позволяет выявить системные проблемы с моделью эмбеддингов на ранних этапах, до того как они проявятся в конкретных поисковых задачах.

### 3.3 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `total_pairs` | int | Количество пар текстов в датасете |
| `n_clusters` | int | Количество уникальных кластеров |
| `avg_intra_cluster_sim` | float | Среднее внутриклассовое сходство (cosine similarity) |
| `avg_inter_cluster_dist` | float | Среднее межклассовое расстояние (euclidean distance) |
| `silhouette_score` | float | Силуэтный коэффициент качества кластеризации (-1 to 1) |

### 3.4 Детальное описание метрик

#### 3.4.1 Intra-Cluster Similarity (среднее внутриклассовое сходство)

**Определение:** Среднее значение косинусного сходства между всеми парами векторов, принадлежащих одному кластеру.

$$ \text{intra\_cluster\_sim}(C_i) = \frac{2}{|C_i|(|C_i|-1)} \sum_{1 \leq p < q \leq |C_i|} \cos(\vec{e}_p, \vec{e}_q) $$

где $C_i$ — $i$-й кластер, $\vec{e}_p, \vec{e}_q$ — векторные представления документов, $\cos(\vec{a}, \vec{b}) = \frac{\vec{a} \cdot \vec{b}}{|\vec{a}||\vec{b}|}$.

**Вычислительный пайплайн:**
1. Для каждого кластера $C_i$ извлекаются все векторные представления $\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_{|C_i|}$
2. Вычисляются попарные косинусные сходства для всех комбинаций $(p, q)$, где $p < q$
3. Результаты усредняются для получения внутрикластерного сходства кластера
4. Финальное значение — среднее по всем кластерам

**Интерпретация:**
- Значения, близкие к 1: элементы кластера семантически очень близки
- Значения, близкие к 0: высокая внутренняя неоднородность кластера
- Ожидаемый диапазон: 0.3–0.8 в зависимости от качества модели и характеристик данных

**Доверие к метрике:** Высокое. Метрика основана на формальном геометрическом определении и не зависит от субъективных оценок. Однако интерпретация требует понимания специфики предметной области.

---

#### 3.4.2 Inter-Cluster Distance (межклассовое расстояние)

**Определение:** Среднее евклидово расстояние между центроидами кластеров.

$$ \text{inter\_cluster\_dist} = \frac{2}{n(n-1)} \sum_{1 \leq i < j \leq n} \|\vec{\mu}_i - \vec{\mu}_j\|_2 $$

где $\vec{\mu}_i = \frac{1}{|C_i|}\sum_{\vec{e} \in C_i} \vec{e}$ — центроид $i$-го кластера, $\|\cdot\|_2$ — евклидова норма.

**Вычислительный пайплайн:**
1. Для каждого кластера вычисляется центроид как среднее арифметическое всех векторов
2. Вычисляются попарные евклидовы расстояния между центроидами
3. Результаты усредняются

**Интерпретация:**
- Высокое значение: кластеры хорошо разделены в пространстве
- Низкое значение: кластеры перекрываются, возможна высокая конфузия при поиске

**Доверие к метрике:** Высокое. Метрика дополняет intra-cluster similarity, обеспечивая полную картину кластерной структуры.

---

#### 3.4.3 Silhouette Score (силуэтный коэффициент)

**Определение:** Нормализованная мера того, насколько хорошо объект соответствует своему кластеру по сравнению с другими кластерами.

$$ s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}} $$

где:
- $a(i)$ — среднее расстояние от объекта $i$ до всех других объектов того же кластера (внутрикластерное расстояние)
- $b(i)$ — минимальное среднее расстояние от объекта $i$ до объектов любого другого кластера (ближайший соседний кластер)

$$ a(i) = \frac{1}{|C_{i}| - 1} \sum_{j \in C_i, j \neq i} d(i, j) $$

$$ b(i) = \min_{C_k \neq C_i} \frac{1}{|C_k|} \sum_{j \in C_k} d(i, j) $$

**Вычислительный пайплайн:**
1. Для каждого объекта $i$ вычисляется $a(i)$ — среднее расстояние до объектов того же кластера
2. Для каждого объекта $i$ вычисляется $b(i)$ — минимальное среднее расстояние до объектов других кластеров
3. Вычисляется силуэт $s(i)$ для каждого объекта
4. Финальное значение — среднее по всем объектам

**Интерпретация:**
- $s(i) \in [-1, 1]$:
  - $s(i) \to 1$: объект хорошо кластеризован
  - $s(i) \to 0$: объекты на границе кластеров
  - $s(i) \to -1$: объекты присвоены неправильному кластеру
- Общепринятые пороги:
  - $> 0.7$: сильная кластерная структура
  - $0.5–0.7$: разумная структура
  - $0.25–0.5$: слабая структура
  - $< 0.25$: кластерная структура отсутствует

**Доверие к метрике:** Очень высокое. Silhouette Score является стандартом в кластерном анализе (Rousseeuw, 1987) и широко валидирован в академической литературе. Метрика инвариантна к количеству кластеров и размеру выборки.

---

## 4. Tier 1: Точность поиска (Retrieval Accuracy)

### 4.1 Теоретическое обоснование

Tier 1 оценивает **эффективность информационно-поисковой системы** на уровне извлечения релевантных документов. Это критический компонент RAG, поскольку качество конечного ответа напрямую зависит от релевантности контекста, переданного генеративной модели.

В контексте RAG-системы поиск выполняет функцию **селекции контекста** — из всей базы знаний необходимо извлечь наиболее релевантные фрагменты для формирования ответа. Ошибки на этом этапе носят катастрофический характер: нерелевантный контекст приводит к генерации недостоверной информации (галлюцинациям).

### 4.2 Почему мы используем эти метрики

Совокупность метрик HitRate, MRR и NDCG образует стандарт де-факто в академических и промышленных системах оценки информационного поиска (TREC, BEIR, MTEB). Каждая метрика захватывает различные аспекты качества поиска:

- **HitRate** — практическая метрика, отражающая вероятность нахождения хотя бы одного релевантного документа
- **MRR** — учитывает ранг первого релевантного результата, что критично для пользовательского опыта
- **NDCG** — наиболее информативная метрика, учитывающая как факт релевантности, так и её степень (через позицию)

### 4.3 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `hit_rate@1` | float | Hit Rate при top-1 (0-1) |
| `hit_rate@5` | float | Hit Rate при top-5 (0-1) |
| `hit_rate@10` | float | Hit Rate при top-10 (0-1) |
| `mrr` | float | Mean Reciprocal Rank (0-1) |
| `recall@K` | float | Recall@K для K=1,3,5,10 |
| `precision@K` | float | Precision@K для K=1,3,5,10 |
| `ndcg@5` | float | NDCG@5 (0-1) |
| `ndcg@10` | float | NDCG@10 (0-1) |

### 4.4 Детальное описание метрик

#### 4.4.1 Hit Rate@K

**Определение:** Доля запросов, для которых хотя бы один релевантный документ присутствует среди первых $K$ результатов поиска.

$$ \text{HitRate}@K = \frac{1}{|Q|} \sum_{q \in Q} \mathbb{1}\left[\exists d \in \text{top}_K(q) : \text{rel}(q, d) = 1\right] $$

где $Q$ — множество запросов, $\text{top}_K(q)$ — первые $K$ результатов для запроса $q$, $\mathbb{1}[\cdot]$ — индикаторная функция, $\text{rel}(q, d)$ — бинарная релевантность.

**Вычислительный пайплайн:**
1. Для каждого запроса $q \in Q$ выполняется поиск с возвратом top-$K$ результатов
2. Проверяется наличие релевантного документа в полученном списке
3. Считается доля запросов с хотя бы одним релевантным результатом

**Интерпретация:**
- HitRate@1 = 1.0: идеальный поиск, первый результат всегда релевантен
- HitRate@10 = 0.5: в 50% случаев релевантный документ присутствует в top-10
- Для RAG-систем критически важно значение HitRate@1

**Доверие к метрике:** Высокое. Метрика интуитивно понятна и широко используется на практике. Ограничение: не учитывает количество релевантных документов и их позиции.

---

#### 4.4.2 Mean Reciprocal Rank (MRR)

**Определение:** Среднее значение обратного ранга первого релевантного документа.

$$ \text{MRR} = \frac{1}{|Q|} \sum_{q \in Q} \frac{1}{\text{rank}_q} $$

где $\text{rank}_q$ — позиция первого релевантного документа в результатах поиска для запроса $q$. Если релевантный документ отсутствует, $\frac{1}{\text{rank}_q} = 0$.

**Вычислительный пайплайн:**
1. Для каждого запроса $q$ выполняется поиск
2. Находится позиция первого релевантного документа ($\text{rank}$)
3. Вычисляется $1/\text{rank}$ (0 если релевантных документов нет)
4. Результаты усредняются по всем запросам

**Интерпретация:**
- MRR = 1.0: первый результат всегда релевантен
- MRR = 0.5: в среднем первый релевантный результат на позиции 2
- MRR = 0.1: первый релевантный результат в среднем на позиции 10

**Доверие к метрике:** Высокое. MRR является стандартной метрикой в информационном поиске (voorhees@trec). Особенно релевантен для систем с малым количеством отображаемых результатов.

---

#### 4.4.3 Recall@K и Precision@K

**Определение:**

$$ \text{Recall}@K = \frac{|\text{rel}_{\text{top}_K}|}{|\text{rel}_{\text{all}}|} $$

$$ \text{Precision}@K = \frac{|\text{rel}_{\text{top}_K}|}{K} $$

где $|\text{rel}_{\text{top}_K}|$ — количество релевантных документов в top-$K$, $|\text{rel}_{\text{all}}|$ — общее количество релевантных документов в коллекции.

**Вычислительный пайплайн:**
1. Для каждого запроса $q$ выполняется поиск и определяется множество релевантных документов
2. Вычисляется $|\text{rel}_{\text{top}_K}|$ — количество релевантных документов в top-$K$
3. Вычисляется Recall@K и Precision@K
4. Результаты усредняются по запросам

**Интерпретация:**
- Recall@K = 1.0: все релевантные документы найдены (при $K \geq |\text{rel}_{\text{all}}|$)
- Precision@K = 1.0: все $K$ результатов релевантны
- Trade-off: при фиксированном $K$ увеличение Recall приводит к снижению Precision

**Доверие к метрике:** Высокое. Recall и Precision — фундаментальные метрики информационного поиска, введённые в работе Кливленда (Cleverdon, 1960). Позволяют оценить полноту и точность поиска раздельно.

---

#### 4.4.4 Normalized Discounted Cumulative Gain (NDCG@K)

**Определение:** Нормализованный дисконтированный кумулятивный выигрыш — метрика, учитывающая как факт релевантности, так и позицию релевантного документа.

$$ \text{NDCG}@K = \frac{\text{DCG}@K}{\text{IDCG}@K} $$

**Discounted Cumulative Gain (DCG):**

$$ \text{DCG}@K = \sum_{i=1}^{K} \frac{\text{rel}_i}{\log_2(i+1)} $$

где $\text{rel}_i$ — релевантность документа на позиции $i$ (в бинарном случае: 1 или 0).

**Ideal DCG (IDCG):** DCG при идеальном упорядочивании (релевантные документы расположены первыми):

$$ \text{IDCG}@K = \sum_{i=1}^{\min(K, |\text{rel}_{\text{all}}|)} \frac{1}{\log_2(i+1)} $$

**Вычислительный пайпайн:**
1. Для каждого запроса $q$ документы упорядочиваются по убыванию предсказанной релевантности
2. Вычисляется DCG@K с учётом дисконтирования по позиции
3. Вычисляется IDCG@K — DCG при идеальном порядке
4. NDCG@K = DCG@K / IDCG@K
5. Результаты усредняются

**Интерпретация:**
- NDCG@K = 1.0: идеальное упорядочивание, все релевантные документы на первых позициях
- NDCG@K = 0.0: нет релевантных документов в top-$K$
- NDCG@K > 0.9: отличное качество поиска
- NDCG@K = 0.5–0.9: приемлемое качество
- NDCG@K < 0.5: требуется улучшение

**Теоретическое обоснование:** NDCG был введён Jarvelin и Kekalainen (2000) для преодоления ограничений бинарных метрик. Дисконтирование отражает тот факт, что пользователи менее склонны просматривать результаты на низких позициях. Нормализация позволяет корректно сравнивать запросы с разным количеством релевантных документов.

**Доверие к метрике:** Очень высокое. NDCG является стандартом в современных бенчмарках (TREC, MTEB, BEIR). Метрика теоретически обоснована и эмпирически валидирована.

---

## 5. Tier 2: Качество генерации

### 5.1 Теоретическое обоснование

Tier 2 оценивает **способность генеративной модели формировать качественные ответы** при условии, что контекст (релевантные документы) уже извлечён. Это позволяет изолировать проблемы генерации от проблем поиска и провести диагностику каждого компонента независимо.

В данном tier используется парадигма **идеального контекста** — модели предоставляется вся релевантная информация из датасета, что позволяет оценить её способность корректно эту информацию использовать и трансформировать в ответ на естественном языке.

### 5.2 Почему мы используем эти метрики

Качество генерации текста является многомерным понятием, которое невозможно адекватно оценить единственной метрикой. Система использует три категории оценок:

1. **LLM-as-a-Judge** — современный подход, использующий способность больших языковых моделей к пониманию и оценке текста
2. **Алгоритмические метрики (ROUGE, BLEU)** — классические статистические методы, основанные на сравнении n-грамм
3. Комбинированный подход обеспечивает как глубину оценки (LLM), так и воспроизводимость (алгоритмические метрики)

### 5.3 Метрики LLM-судьи

| Метрика | Тип | Описание |
|---------|-----|----------|
| `avg_faithfulness` | float | Средняя оценка фактичности (0-1) |
| `avg_answer_relevance` | float | Средняя оценка релевантности ответа (0-1) |

### 5.4 Метрики текстового сравнения

| Метрика | Тип | Описание |
|---------|-----|----------|
| `avg_rouge1_f` | float | ROUGE-1 F-measure (0-1) |
| `avg_rouge2_f` | float | ROUGE-2 F-measure (0-1) |
| `avg_rougeL_f` | float | ROUGE-L F-measure (0-1) |
| `avg_bleu` | float | BLEU score (0-100) |

### 5.5 Детальное описание метрик

#### 5.5.1 Faithfulness (фактичность)

**Определение:** Оценка степени согласованности сгенерированного ответа с предоставленным контекстом. Ответ считается фактически достоверным, если вся информация в нём содержится в контексте и не является выдуманной (галлюцинацией).

**Промпт LLM-судьи (упрощённый):**
```
Оцени фактичность ответа на основе контекста.
Контекст: [контекст из базы знаний]
Вопрос: [вопрос пользователя]
Ответ: [сгенерированный системой ответ]

Оцени по шкале от 0 до 1, где:
1.0 - ответ полностью согласуется с контекстом, нет галлюцинаций
0.5 - ответ частично согласуется с контекстом
0.0 - ответ содержит галлюцинации, противоречит контексту
```

**Вычислительный пайплайн:**
1. Для каждого вопроса из датасета извлекается контекст (chunk_text)
2. Модель генерации (Mistral) создаёт ответ на основе контекста
3. LLM-судья (Qwen) оценивает фактичность ответа по шкале 0–1
4. Оценки усредняются по всем вопросам

**Интерпретация:**
- Faithfulness = 1.0: все ответы полностью достоверны
- Faithfulness > 0.8: высокое качество, минимальные галлюцинации
- Faithfulness < 0.5: серьёзные проблемы с достоверностью

**Теоретическое обоснование:** Faithfulness напрямую измеряет критически важную характеристику RAG-систем — склонность к галлюцинациям. Исследования показывают (Zhang et al., 2023), что даже современные LLM склонны к генерации недостоверной информации, и измерение этого аспекта необходимо для обеспечения надёжности системы.

**Доверие к метрике:** Среднее-высокое. LLM-as-a-Judge демонстрирует высокую корреляцию с человеческими оценками (Zheng et al., 2023). Ограничения: возможная систематическая предвзятость конкретной модели-судьи.

---

#### 5.5.2 Answer Relevance (релевантность ответа)

**Определение:** Оценка степени соответствия сгенерированного ответа заданному вопросу. Ответ релевантен, если он адресует именно тот вопрос, который был задан.

**Промпт LLM-судьи (упрощённый):**
```
Оцени релевантность ответа на вопрос.
Вопрос: [вопрос пользователя]
Ответ: [сгенерированный системой ответ]

Оцени по шкале от 0 до 1, где:
1.0 - ответ полностью отвечает на вопрос
0.5 - ответ частично отвечает на вопрос
0.0 - ответ не отвечает на вопрос
```

**Вычислительный пайплайн:**
1. Для каждого вопроса генерируется ответ
2. LLM-судья оценивает релевантность
3. Оценки усредняются

**Интерпретация:**
- Answer Relevance = 1.0: все ответы точно соответствуют вопросам
- Answer Relevance > 0.8: высокое качество понимания запросов

**Доверие к метрике:** Среднее-высокое. Аналогично Faithfulness.

---

#### 5.5.3 ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

**Определение:** Семейство метрик, основанных на подсчёте перекрытия n-грамм между сгенерированным текстом (гипотезой) и эталонным текстом (референсом).

$$ \text{ROUGE-}N = \frac{\sum_{g \in \text{Grams}_N(hypothesis)} \min(\text{count}_H(g), \text{count}_R(g))}{\sum_{g \in \text{Grams}_N(reference)} \text{count}_R(g)} $$

где $N$ — размер n-граммы (1 для униграмм, 2 для биграмм), $\text{Grams}_N(\cdot)$ — множество n-грамм текста, $\text{count}_H(g)$ — частота n-граммы $g$ в гипотезе, $\text{count}_R(g)$ — частота в референсе.

**ROUGE-L (на основе LCS — Longest Common Subsequence):**

$$ \text{ROUGE-L} = \frac{(1+\beta^2) \cdot \text{Recall}_L \cdot \text{Precision}_L}{\text{Recall}_L + \beta^2 \cdot \text{Precision}_L} $$

$$ \text{Recall}_L = \frac{\text{LCS}(H, R)}{|R|}, \quad \text{Precision}_L = \frac{\text{LCS}(H, R)}{|H|} $$

**F-measure:**

$$ F = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} $$

**Вычислительный пайплайн:**
1. Токенизация гипотезы и референса
2. Извлечение n-грамм (для ROUGE-1: униграммы, ROUGE-2: биграммы, ROUGE-L: LCS)
3. Подсчёт перекрытия с учётом ограничений
4. Вычисление Precision, Recall и F-measure

**Интерпретация:**
- ROUGE-L-F > 0.5: хорошее качество
- ROUGE-1-F > 0.4: приемлемое качество для коротких текстов
- Обратить внимание: высокий ROUGE не гарантирует семантическую правильность

**Теоретическое обоснование:** ROUGE был разработан в National Institute of Standards and Technology (NIST) для автоматической оценки резюме текстов (Lin, 2004). Метрика стала стандартом в области машинного перевода и суммаризации.

**Доверие к метрике:** Среднее. ROUGE широко используется, но имеет известные ограничения:
- Не учитывает семантику, только лексическое перекрытие
- Чувствителен к поверхностным вариациям
- Не всегда коррелирует с человеческими оценками

---

#### 5.5.4 BLEU (Bilingual Evaluation Understudy)

**Определение:** Метрика, первоначально разработанная для оценки машинного перевода, основанная на точности n-грамм с штрафом за краткость.

$$ \text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right) $$

где $p_n$ — точность n-грамм, $w_n$ — вес (обычно $1/N$), BP — brevity penalty (штраф за краткость).

**Brevity Penalty:**

$$ \text{BP} = \begin{cases} 1 & \text{если } c > r \\ \exp(1 - r/c) & \text{если } c \leq r \end{cases} $$

где $c$ — длина гипотезы, $r$ — длина референса.

**Precision n-грамм:**

$$ p_n = \frac{\sum_{g \in \text{Grams}_n(H)} \min(\text{count}_H(g), \text{count}_R(g))}{\sum_{g \in \text{Grams}_n(H)} \text{count}_H(g)} $$

**Вычислительный пайплайн:**
1. Токенизация (обычно на уровне слов или subword)
2. Подсчёт n-грамм для каждого $n$ от 1 до 4
3. Вычисление точности для каждого $n$
4. Применение brevity penalty
5. Вычисление геометрического среднего

**Интерпретация:**
- BLEU = 100: идеальное совпадение (практически недостижимо)
- BLEU > 50: очень хорошее качество машинного перевода
- BLEU 20–40: приемлемое качество
- BLEU < 20: требуется существенное улучшение

**Теоретическое обоснование:** BLEU был представлен в работе Papineni et al. (2002) и стал де-факто стандартом для оценки машинного перевода на протяжении более чем десятилетия.

**Доверие к метрике:** Среднее-низкое для генерации сводных текстов. BLEU имеет существенные ограничения:
- Не учитывает смысл
- Зависит от совпадения на уровне поверхностных форм
- Penalizeет валидные перефразирования
- Особенно ограничен для открытых задач генерации

---

## 6. Tier 3: Сквозное качество (End-to-End)

### 6.1 Теоретическое обоснование

Tier 3 оценивает **полный RAG-пайплайн** в условиях, максимально приближенных к реальной эксплуатации. В отличие от Tier 2, где используется идеальный контекст из датасета, Tier 3 выполняет реальный поиск через production-код (`qa.confluence_retrieving.get_chunk()`), что позволяет оценить систему в целом.

Этот tier является наиболее релевантным для оценки пользовательского опыта, поскольку именно он отражает реальную производительность системы.

### 6.2 Почему мы используем эти метрики

Сквозная оценка позволяет выявить проблемы, которые не обнаруживаются при изолированном тестировании компонентов:
- Взаимодействие между поиском и генерацией
- Влияние качества поиска на качество генерации
- Согласованность production-кода с ожиданиями

### 6.3 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `avg_e2e_score` | float | Средняя E2E оценка качества (0-1) |
| `avg_semantic_similarity` | float | Среднее семантическое сходство (cosine, -1 to 1) |
| `avg_rouge1_f` | float | ROUGE-1 F-measure (0-1) |
| `avg_rouge2_f` | float | ROUGE-2 F-measure (0-1) |
| `avg_rougeL_f` | float | ROUGE-L F-measure (0-1) |
| `avg_bleu` | float | BLEU score (0-100) |

### 6.4 Детальное описание метрик

#### 6.4.1 E2E Score

**Определение:** Комплексная оценка качества ответа, учитывающая вопрос, сгенерированный ответ и эталонный ответ.

**Промпт LLM-судьи (упрощённый):**
```
Оцени общую полезность ответа на основе вопроса и эталонного ответа.
Вопрос: [вопрос пользователя]
Эталонный ответ: [ground_truth_answer]
Сгенерированный ответ: [system_answer]

Оцени по шкале от 0 до 1, где:
1.0 - сгенерированный ответ столь же полезен, как эталонный
0.5 - сгенерированный ответ частично полезен
0.0 - сгенерированный ответ не полезен
```

**Вычислительный пайплайн:**
1. Для каждого вопроса выполняется retrieval (top-1 через production код)
2. Генерируется ответ через Mistral
3. LLM-судья оценивает E2E качество
4. Оценки усредняются

**Интерпретация:** Аналогично Faithfulness и Answer Relevance, но в контексте реального использования.

---

#### 6.4.2 Semantic Similarity (семантическое сходство)

**Определение:** Косинусное сходство между векторными представлениями сгенерированного ответа и эталонного ответа.

$$ \text{semantic\_similarity} = \cos(\vec{e}_{sys}, \vec{e}_{gt}) = \frac{\vec{e}_{sys} \cdot \vec{e}_{gt}}{|\vec{e}_{sys}| \cdot |\vec{e}_{gt}|} $$

где $\vec{e}_{sys}$ — эмбеддинг сгенерированного ответа, $\vec{e}_{gt}$ — эмбеддинг эталонного ответа.

**Вычислительный пайплайн:**
1. Генерируется ответ системы
2. Оба текста (системный и эталонный) преобразуются в эмбеддинги через SentenceTransformer
3. Вычисляется косинусное сходство
4. Результаты усредняются

**Интерпретация:**
- $\to 1.0$: семантически идентичные ответы
- $\to 0.0$: ортогональные ответы
- $\to -1.0$: противоположные по смыслу

**Теоретическое обоснование:** Semantic Similarity преодолевает ограничение лексических метрик (ROUGE, BLEU), поскольку работает на уровне семантики, а не поверхностных форм. Использование предобученных эмбеддингов позволяет уловить синонимию и перефразирование.

**Доверие к метрике:** Высокое для оценки семантической близости. Ограничения:
- Зависит от качества модели эмбеддингов
- Не различает частичную релевантность

---

## 7. Real-Users Benchmark

### 7.1 Теоретическое обоснование

Real-Users Benchmark использует **реальные данные эксплуатации** — вопросы пользователей из продуктивной системы. Это обеспечивает максимальную релевантность оценок, поскольку тестирование проводится на реальных, а не синтетических данных.

Ключевая особенность: в качестве ground truth используется `QuestionAnswer.confluence_url` — URL документа, который был показан пользователю в реальном диалоге. Это позволяет оценить, насколько хорошо система воспроизводит поведение production-системы.

### 7.2 Метрики

Аналогичны Tier 1: HitRate, MRR, NDCG, Recall, Precision.

### 7.3 Ground Truth

**Определение:** `QuestionAnswer.confluence_url` из таблицы `question_answer` базы данных.

**Обоснование:** URL представляет собой прокси-метку релевантности — в production-системе этот документ был признан достаточно релевантным для показа пользователю.

**Ограничения:**
- Не все вопросы имеют URL (возможны NULL значения)
- URL может быть неполным или неактуальным
- Не учитывает полноту ответа

---

## 8. Tier Judge: Качество LLM-судьи

### 8.1 Теоретическое обоснование

Tier Judge оценивает **качество и надёжность самого инструмента оценки** — LLM-судьи. Поскольку значительная часть метрик системы основана на LLM-as-a-Judge, критически важно убедиться в консистентности и точности судьи.

### 8.2 Почему мы используем эти метрики

LLM, выступающие в роли судьи, могут проявлять:
- **Неконсистентность** — разные оценки при повторном вызове на том же примере
- **Предвзятость** — систематическое предпочтение определённых типов ответов
- **Низкую точность** — неспособность корректно оценить качество

Измерение этих аспектов необходимо для обеспечения достоверности оценок.

### 8.3 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `consistency_score` | float | Согласованность при повторной оценке (0-1) |
| `error_rate` | float | Доля ошибок (0-1) |
| `avg_latency_ms` | float | Среднее время оценки (мс) |
| `avg_faithfulness` | float | Средняя оценка фактичности (0-1) |

### 8.4 Детальное описание метрик

#### 8.4.1 Consistency Score

**Определение:** Доля случаев, когда повторная оценка того же примера даёт согласованный результат (разница ≤ 0.5).

$$ \text{Consistency} = \frac{1}{|Q|} \sum_{q \in Q} \mathbb{1}\left[|s_1(q) - s_2(q)| \leq \delta\right] $$

где $s_1(q)$ — первая оценка, $s_2(q)$ — повторная оценка, $\delta = 0.5$ — порог согласованности.

**Вычислительный пайплайн:**
1. Для каждого примера выполняется первичная оценка
2. Выполняется повторная оценка того же примера
3. Вычисляется согласованность
4. Результаты усредняются

**Интерпретация:**
- Consistency = 1.0: полная согласованность
- Consistency > 0.9: высокая надёжность судьи
- Consistency < 0.7: судья нестабилен

**Доверие к метрике:** Высокое. Прямо измеряет критическую характеристику — воспроизводимость оценок.

---

## 9. Tier Judge Pipeline: Качество Production Judge

### 9.1 Теоретическое обоснование

Tier Judge Pipeline оценивает **production-компонент** — судью Mistral, который используется в реальном RAG-пайплайне для решения «показывать ответ пользователю или нет». Это критически важная функция, определяющая будет ли предоставлен ответ или система признает невозможность ответа.

### 9.2 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `accuracy` | float | Общая точность (0-1) |
| `precision` | float | Precision для класса "Yes" (0-1) |
| `recall` | float | Recall для класса "Yes" (0-1) |
| `f1_score` | float | F1-score (0-1) |
| `avg_latency_ms` | float | Среднее время оценки (мс) |

### 9.3 Детальное описание метрик

#### 9.3.1 Accuracy

**Определение:** Доля правильных решений судьи.

$$ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} $$

где:
- $TP$ (True Positive): судья предсказал "Yes", ground truth = "Yes"
- $TN$ (True Negative): судья предсказал "No", ground truth = "No"  
- $FP$ (False Positive): судья предсказал "Yes", ground truth = "No"
- $FN$ (False Negative): судья предсказал "No", ground truth = "Yes"

**Вычислительный пайплайн:**
1. Для каждого примера вызывается production judge с промптом
2. Ответ парсится (извлекается "Yes"/"No")
3. Сравнивается с ground_truth_show
4. Вычисляются TP, TN, FP, FN
5. Вычисляется Accuracy

**Интерпретация:**
- Accuracy > 0.95: отличное качество
- Accuracy 0.85–0.95: приемлемое качество
- Accuracy < 0.85: требует улучшения

---

#### 9.3.2 Precision, Recall, F1

**Определения:**

$$ \text{Precision} = \frac{TP}{TP + FP} $$

$$ \text{Recall} = \frac{TP}{TP + FN} $$

$$ F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} $$

**Интерпретация:**

- **Precision**: доля правильных "Yes" среди всех предсказанных "Yes"
- **Recall**: доля найденных "Yes" среди всех истинных "Yes"
- **F1**: гармоническое среднее Precision и Recall

**Trade-off:** В контексте RAG-системы обычно более важно высокое Recall (не пропустить показ ответа), но высокое Precision также критично (не показывать некачественные ответы). F1 балансирует эти требования.

---

## 10. Tier UX: Пользовательский опыт

### 10.1 Теоретическое обоснование

Tier UX оценивает **аспекты, влияющие на восприятие системы конечным пользователем**, но не связанные напрямую с качеством ответов. Эти метрики важны для понимания общей эффективности системы с точки зрения пользователя.

### 10.2 Метрики

| Метрика | Тип | Описание |
|---------|-----|----------|
| `cache_hit_rate` | float | Попадания в кэш похожих вопросов (0-1) |
| `context_preservation` | float | Сохранение контекста в многотуровых диалогах (0-1) |
| `multi_turn_consistency` | float | Согласованность между вопросами в диалоге (0-1) |

### 10.3 Детальное описание метрик

#### 10.3.1 Cache Hit Rate

**Определение:** Доля вопросов, для которых обнаружен семантически схожий вопрос в кэше (similarity > 0.85).

$$ \text{CacheHitRate} = \frac{1}{|Q|} \sum_{q \in Q} \mathbb{1}\left[\max_{c \in \text{Cache}} \cos(\vec{e}_q, \vec{e}_c) > \tau\right] $$

где $\tau = 0.85$ — порог кэширования.

**Интерпретация:**
- CacheHitRate > 0.5: высокий потенциал для кэширования
- CacheHitRate < 0.1: система не использует преимущества кэша

---

#### 10.3.2 Context Preservation

**Определение:** Среднее косинусное сходство между последовательными вопросами в многотуровом диалоге.

$$ \text{ContextPreservation} = \frac{1}{|S|-1} \sum_{i=1}^{|S|-1} \cos(\vec{e}_{q_i}, \vec{e}_{q_{i+1}}) $$

где $S = [q_1, q_2, \ldots, q_n]$ — последовательность вопросов в сессии.

**Интерпретация:**
- ContextPreservation > 0.7: вопросы связаны, контекст сохраняется
- ContextPreservation < 0.3: вопросы не связаны

---

## 11. Рекомендации по использованию метрик

### 11.1 Выбор метрик в зависимости от цели

| Цель | Рекомендуемые метрики |
|------|----------------------|
| Диагностика поиска | HitRate@1, MRR, NDCG@10 |
| Диагностика генерации | Faithfulness, Answer Relevance |
| Сквозная оценка | E2E Score, Semantic Similarity |
| Оценка production judge | Accuracy, Precision, Recall, F1 |
| Оценка LLM-судьи | Consistency Score |
| Оптимизация кэша | Cache Hit Rate |

### 11.2 Пороговые значения

| Метрика | Отлично | Хорошо | Требует внимания |
|---------|---------|--------|------------------|
| HitRate@1 | > 0.8 | 0.5–0.8 | < 0.5 |
| MRR | > 0.8 | 0.5–0.8 | < 0.5 |
| NDCG@10 | > 0.8 | 0.5–0.8 | < 0.5 |
| Faithfulness | > 0.9 | 0.7–0.9 | < 0.7 |
| E2E Score | > 0.8 | 0.6–0.8 | < 0.6 |
| Accuracy (Judge Pipeline) | > 0.95 | 0.85–0.95 | < 0.85 |
| Consistency | > 0.95 | 0.85–0.95 | < 0.85 |

---

## 12. Заключение

Представленная система метрик обеспечивает комплексную оценку всех аспектов RAG-системы — от качества векторных представлений до пользовательского опыта. Комбинация алгоритмических метрик, LLM-as-a-Judge и специализированных метрик позволяет проводить многоуровневую диагностику и выявлять узкие места системы на различных уровнях абстракции.

Ключевые принципы системы:
1. **Иерархичность** — изоляция компонентов для точной диагностики
2. **Комплементарность** — использование различных подходов к оценке
3. **Воспроизводимость** — формальные метрики обеспечивают повторяемость
4. **Практическая релевантность** — метрики соотносятся с пользовательским опытом

---

## Ссылки

- Cleverdon, C. W. (1960). The Cranfield tests on index language devices. *Aslib Proceedings*.
- Jarvelin, K., & Kekalainen, J. (2000). IR evaluation methods for retrieving highly relevant documents. *SIGIR*.
- Lin, C. Y. (2004). ROUGE: A package for automatic evaluation of summaries. *ACL*.
- Papineni, K., et al. (2002). BLEU: a method for automatic evaluation of machine translation. *ACL*.
- Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. *J. Comput. Appl. Math.*
- Zhang, Y., et al. (2023). A survey on hallucination in large language models. *arXiv*.
- Zheng, L., et al. (2023). Judging LLM-as-a-judge with MT-bench and Chatbot Arena. *NeurIPS*.
